\documentclass[a4paper, 12pt]{report}

%%%%%%%%%%%%
% Packages %
%%%%%%%%%%%%

\usepackage{../Nyx/nyx-packages}
\usepackage{../Nyx/nyx-styles}
\usepackage{../Nyx/nyx-frames}
\usepackage{../Nyx/nyx-title}
\usepackage{../Nyx/nyx-macros}

%%%%%%%%%%%%%%
% Title-page %
%%%%%%%%%%%%%%

\logo{../Nyx/logo.png}

\institute{\curlyquotes{\hspace{0.25mm}Sapienza} Università di Roma}
\faculty{Ingegneria dell'Informazione,\\Informatica e Statistica}
\department{Dipartimento di Informatica}

\title{Progettazione di Algoritmi}
\subtitle{Appunti integrati con il libro "Introduzione agli algoritmi e strutture dati", Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, clifford Stein}

\author{\textit{Author}\\TODO: DECOMMENTARE QUESTA SEZIONE}
% \author{\textit{Author}\\Simone Bianco}
\author{\textit{Author}\\Alessio Bandiera}
% \supervisor{Linus \textsc{Torvalds}}
% \context{Well, I was bored\ldots}

\date{\today}

%%%%%%%%%%%%
% Document %
%%%%%%%%%%%%

\begin{document}
    \maketitle

    % The following style changes are valid only inside this scope 
    {
        \hypersetup{allcolors=black}
        \fancypagestyle{plain}{%
        \fancyhead{}        % clear all header fields
        \fancyfoot{}        % clear all header fields
        \fancyfoot[C]{\thepage}
        \renewcommand{\headrulewidth}{0pt}
        \renewcommand{\footrulewidth}{0pt}}

        \romantableofcontents
    }

    \chapter*{Informazioni e Contatti}      % \chapter* makes this a "fake" chapter
    \markboth{Informazioni e Contatti}{}    % Manually sets \leftmark (current chapter name)
    \addcontentsline{toc}{chapter}{Informazioni e Contatti}     % Manually adds chapter to ToC
    
    \subsubsection{Prerequisiti consigliati:}
    \begin{itemize}
        \item TODO: DA DECIDERE
    \end{itemize}

    \quad

    \subsubsection{Segnalazione errori ed eventuali migliorie:}
    
    Per segnalare eventuali errori e/o migliorie possibili, si prega di utilizzare il \textbf{sistema di Issues fornito da GitHub} all'interno della pagina della repository stessa contenente questi ed altri appunti (link fornito al di sotto), utilizzando uno dei template già forniti compilando direttamente i campi richiesti.

    Gli appunti sono in continuo aggiornamento, pertanto, previa segnalazione, si prega di controllare se l'errore sia ancora presente nella versione più recente.

    \quad

    \subsubsection{Licenza di distribuzione:}
    
    These documents are distributed under the \textbf{\href{https://www.gnu.org/licenses/fdl-1.3.txt}{GNU Free Documentation License}}, a form of copyleft intended to be used on manuals, textbooks or other types of document in order to assure everyone the effective freedom to copy and redistribute it, with or without modifications, either commercially or non-commercially.
    
    \quad

    \subsubsection{Contatti dell'autore e ulteriori link:}
    \begin{itemize}
        % Simone
        % 
        % \item Altri appunti: \textbf{\href{https://github.com/Exyss/university-notes}{https://github.com/Exyss/university-notes}}
        % \item Github: \textbf{\href{https://github.com/Exyss}{https://github.com/Exyss}}
        % \item Email: \textbf{\href{mailto:bianco.simone@outlook.it}{bianco.simone@outlook.it}}
        % \item LinkedIn: \textbf{\href{https://www.linkedin.com/in/simone-bianco}{Simone Bianco}}

        % Alessio

        \item Github: \textbf{\href{https://github.com/ph04}{https://github.com/ph04}}
        \item Email: \textbf{\href{mailto:alessio.bandiera02@gmail.com}{alessio.bandiera02@gmail.com}}
        \item LinkedIn: \textbf{\href{https://www.linkedin.com/in/alessio-bandiera-a53767223/}{Alessio Bandiera}}
    \end{itemize}

    %%%%%%%%%%%%%%%%%%%%%

    \chapter{Elementi di teoria dei grafi}

    \section{Grafi}

    \subsection{Definizioni}

    \begin{frameddefn}{Grafo}
        Un \tit{grafo} è una struttura matematica descritta da vertici, collegati da archi. Un grafo viene descritto formalmente come $G=(V, E)$, dove i $v \in V$ sono i \tbf{vertici} o \tbf{nodi} del grafo, mentre gli $e \in E$ sono gli \tbf{archi} (dall'inglese \tit{edges}). In particolare, $V(G)$ è l'insieme dei vertici di $G$, comunemente indicato con $n$, mentre $E(G)$ è l'insieme degli archi di $G$, comunemente indicato con $m$. Presi due vertici $v_1,v_2 \in V(G)$, allora $(v_1, v_2) \in E(G)$ è l'arco che li collega.
    \end{frameddefn}

    \begin{framedobs}{Relazione tra vertici ed archi}
        Si noti che, per ogni grafo $G$, si verifica che $E(G) \subseteq V(G)^2$.
    \end{framedobs}

    \begin{frameddefn}{Vertici adiacenti}
        Dato un grafo $G$, due suoi vertici $v_1, v_2 \in V(G)$ sono detti \tbf{adiacenti} se $(v_1, v_2) \in E(G)$; in tal caso, si usa la notazione $$v_1 \sim v_2$$
    \end{frameddefn}

    \begin{frameddefn}{Sottografo}
        Dato un grafo $G = (V, E)$, $G'=(V', E')$ è detto \tbf{sottografo di $G$} se è un grafo della forma $$\left \{ \begin{array}{l} V' \subseteq V \\ E' \subseteq E \end{array} \right.$$j Si noti ogni grafo è sottografo di sè stesso.
    \end{frameddefn}

    \begin{frameddefn}{Grafo indiretto}
        Un grafo è detto \tbf{indiretto} se gli archi non hanno direzione, o equivalentemente $$\forall v_1, v_2 \in V(G) \quad (v_1, v_2) = (v_2, v_1) \in E(G)$$
    \end{frameddefn}

    \begin{example}[Grafo indiretto]
        Ad esempio, si consideri questo grafo indiretto:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (6) [right of=1] {6};
                \node[main node] (5) [below right of=6] {5};
                \node[main node] (4) [below left of=5] {4};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (2)
                    (2) edge (3)
                    (2) edge (4)
                    (3) edge (4)
                    (4) edge (1)
                    (5) edge (1)
                    (6) edge (3)
                    (6) edge (5)
                    ;
            \end{tikzpicture}
            \caption{Un grafo indiretto.}
        \end{figure}

        in esso, si hanno $$V(G) = \{1, 2, 3, 4, 5, 6\}$$ $$E(G) = \{(1, 2), (1, 4), (1, 5) (2, 3), (2, 4), (3, 4), (3, 6), (5, 6)\}$$
    \end{example}

    \begin{frameddefn}{Grafo diretto}
        Un grafo è detto \tbf{diretto} se gli archi hanno direzione, o equivalentemente $$\forall v_1, v_2 \in V(G) \quad (v_1, v_2) \neq (v_2, v_1) \in E(G)$$
    \end{frameddefn}

    \begin{example}[Grafo diretto]
        Ad esempio, si consideri questo grafo diretto:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (6) [right of=1] {6};
                \node[main node] (5) [below right of=6] {5};
                \node[main node] (4) [below left of=5] {4};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (2)
                    (2) edge (3)
                    (2) edge (4)
                    (3) edge (4)
                    (4) edge (1)
                    (5) edge (1)
                    (6) edge (3)
                    (6) edge (5)
                    ;
            \end{tikzpicture}
            \caption{Un grafo diretto.}
        \end{figure}

        in esso, si hanno $$V(G) = \{1, 2, 3, 4, 5, 6\}$$ $$E(G) = \{(1, 2), (2, 3), (2, 4), (3, 4), (4, 1), (5, 1), (6, 3), (6, 5)\}$$
    \end{example}

    \begin{frameddefn}{Grado}
        Il \tbf{grado} di un vertice $v \in V(G)$ è il numero di archi incidenti su $v$, indicato con $\deg(v)$; all'interno di questi appunti, nel caso di grafi diretti, con $\deg(v)$ verrà inteso il numero di archi uscenti da $v$, mentre con $\deg^{in}(v)$ il numero di archi entranti in $v$.
    \end{frameddefn}

    \begin{framedlem}[label={Somma dei gradi}]{Somma dei gradi}
        Dato un grafo $G$, la somma dei gradi dei vertici è pari a $2 |E(G)|$.
    \end{framedlem}

    \begin{proof}
        Sia $G$ un grafo; allora, ogni arco $e \in E(G)$ collega due vertici, e dunque necessariamente $\displaystyle \sum_{v \in V(G)}{\deg(v)} = 2 |E(G)|$.
    \end{proof}

    \begin{frameddefn}{Cappio}
        Un arco con estremi coincidenti è detto \tbf{cappio}.
    \end{frameddefn}

    \begin{example}[Grafo con cappio]
        Un esempio di grafo con cappio è il seguente:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (2)
                    (2) edge (3)
                    (3) edge (1)
                    (3) edge [loop below] (3)
                    ;
            \end{tikzpicture}
            \caption{Un grafo diretto con cappio in 3.}
        \end{figure}
    \end{example}

    \begin{frameddefn}{Grafo semplice}
        Un grafo è detto \tbf{semplice} se non contiene cappi, né lati multipli, ovvero più archi per due vertici.

        All'interno di questi appunti, a meno di esplicita specifica, si assume che ogni grafo trattato sia semplice.
    \end{frameddefn}

    \begin{frameddefn}{Multigrafo}
        Un grafo è detto \tbf{multigrafo}, se non è un grafo semplice.
    \end{frameddefn}

    \subsection{Visite}

    \begin{frameddefn}{Passeggiata}
        Una \tbf{passeggiata} è una sequenza di vertici ed archi, della forma $$\{v_0, e_1, v_1, e_2, \ldots , e_{k - 1}, v_{k - 1}, e_k, v_k\}$$ dove $e_i=(v_{i - 1}, v_i)$. È la visita di un grafo più generale, ed è possibile ripercorrere ogni arco ed ogni vertice.
    \end{frameddefn}

    \begin{framedobs}{Lunghezza massima di una passeggiata}
        La lunghezza massima di una passeggiata su un grafo è infinita.
    \end{framedobs}

    \begin{frameddefn}{Passeggiata chiusa}
        Una passeggiata si dice \tbf{chiusa} se è della forma $$\{v_0, e_1, v_1, e_2, \ldots , e_{k - 1}, v_{k - 1}, e_k, v_0\}$$ dunque il primo e l'ultimo vertice coincidono. 
    \end{frameddefn}

    \begin{frameddefn}{Traccia}
        Una \tbf{traccia} è una passeggiata aperta, in cui non è possibile ripercorrere gli archi, ma è possibile ripercorrere i vertici.
    \end{frameddefn}

    \begin{example}[Traccia di un grafo]
        Ad esempio, si consideri questo grafo indiretto:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [below right of=1] {4};
                \node[main node] (5) [above right of=4] {5};
                \node[main node] (6) [below right of=4] {6};

                \path[every node/.style={font=\sffamily\small}]
                    (2) edge (3)
                    (2) edge (4)
                    (3) edge (4)
                    (4) edge (1)
                    (4) edge (5)
                    (4) edge (6)
                    (5) edge (6)
                    ;
            \end{tikzpicture}
            \caption{Un grafo indiretto.}
        \end{figure}

        in esso, si ha la traccia $$\{5, (5,4), 4, (4,3), 3, (3, 2), 2, (2,4), 4, (4, 6), 6\}$$
    \end{example}

    \begin{frameddefn}{Circuito}
        Un \tbf{circuito} è una traccia chiusa.
    \end{frameddefn}

    \begin{frameddefn}{Cammino}
        Un \tbf{cammino} è una traccia aperta, in cui non è possibile ripercorrere i vertici. In simboli, dati $v, v' \in V(G)$, con $v \rightarrow v'$ si indica che è possibile raggiungere $v'$, partendo da $v$, attraverso un cammino; inoltre, è possibile estendere tale sintassi anche agli archi.
    \end{frameddefn}

    \begin{framedobs}{Archi di cammini}
        In una passeggiata in cui non si ripercorrono i vertici, non è possibile ripercorrere gli archi.
    \end{framedobs}

    \begin{framedthm}[label={cammini e passeggiate}]{Cammini e passeggiate}
        Sia $G$ un grafo, e $u, v \in V(G)$ due suoi vertici; allora, in $G$ esiste una passeggiata $u \rightarrow v$, se e solo se esiste un cammino $u \rightarrow v$.
    \end{framedthm}

    \begin{proof}
        \hspace{0.7cm}
        \begin{itemize}
            \item[] \tit{Prima implicazione.} Sia $u \rightarrow v$ una passeggiata da $u$ a $v$; allora, per trovare il cammino $u \rightarrow v$, è sufficiente considerare il sottoinsieme di cardinalità minore, di vertici ed archi, della passeggiata, tale da congiungere $u$ e $v$.
            \item[] \tit{Seconda implicazione.} Per definizione, una passeggiata è un qualsiasi percorso tra due vertici di un grafo, e in particolare un cammino è una passeggiata, e dunque il cammino $u \rightarrow v$ è anche un passeggiata.
        \end{itemize}
    \end{proof}

    \begin{frameddefn}{Ciclo}
        Un \tbf{ciclo} è un cammino chiuso.
    \end{frameddefn}

    \begin{example}[Cicli di un grafo]
        Ad esempio, si consideri questo grafo indiretto:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [below right of=1] {4};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (2)
                    (2) edge (3)
                    (2) edge (4)
                    (3) edge (4)
                    (4) edge (1)
                    ;
            \end{tikzpicture}
            \caption{Un grafo indiretto.}
        \end{figure}

        in esso, si hanno tre cicli: $$\{2, (2, 4), 4, (4,3), 3, (3, 2), 2\}$$ $$\{2, (2, 4), 4, (4, 1), 1, (1, 2), 2\}$$ $$\{1, (1, 2), 2, (2, 3), 3, (3, 4), 4, (4, 1), 1\}$$
    \end{example}

    \begin{frameddefn}{Ordinamento topologico}
        I vertici di un grafo diretto si definiscono \tbf{ordinati topologicamente}, se disposti in modo tale che ogni vertice viene prima di tutti i vertici collegati ai suoi archi uscenti.
    \end{frameddefn}

    \begin{example}[Ordinamento topologico]
        Ad esempio, nel seguente grafo sono presenti vari ordinamenti topologici:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [below right of=1] {4};
                \node[main node] (5) [above right of=4] {5};
                \node[main node] (6) [below right of=5] {6};
                \node[main node] (7) [below left of=6] {7};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (4)
                    (2) edge (1)
                    (2) edge (3)
                    (2) edge (4)
                    (3) edge (4)
                    (4) edge (5)
                    (4) edge (7)
                    (7) edge (6)
                    ;
            \end{tikzpicture}
            \caption{Un grafo diretto con ordinamenti topologici.}
        \end{figure}

        ad esempio, uno di questi è $\{2, 3, 1, 4, 5, 7, 6\}$.
    \end{example}

    \begin{framedthm}[label={Ordinamento topologico}]{Ordinamento topologico}
        Un grafo diretto ha un ordinamento topologico, se e solo se è aciclico.
    \end{framedthm}

    \begin{proof}
        \hspace{0.7cm}
        \begin{itemize}
            \item[] \tit{Prima implicazione.} Per assurdo, sia $G$ un grafo diretto, con un ordinamento topologico, e ciclico, avente dunque almeno un ciclo, e siano $\{v_0, \ldots , v_{k - 1}, v_0\}$ i vertici che costituiscono uno dei cicli di $G$; allora, si ha che $$v_0 \rightarrow v_1 \rightarrow \ldots \rightarrow v_{k - 1} \rightarrow v_0$$ e dunque all'interno dell'ordinamento topologico $v_0$ dovrebbe essere posto contemporaneamente prima e dopo $v_1, \ldots, v_{k - 1} \ \lightning$.
            \item[] \tit{Seconda implicazione.} Sia $G$ un grafo aciclico; allora per definizione, all'interno di esso non esistono cicli, ed è dunque possibile enumerare in sequenza ogni vertice $G$, senza creare dipendenze circloari, per poter trovare un ordinamento topologico del grafo.
        \end{itemize}
    \end{proof}

    \begin{framedcor}[label={vertici particolari}]{Vertici particolari}
        In un grafo diretto aciclico, esiste almeno un vertice senza archi entranti, ed almeno un vertice senza archi uscenti.
    \end{framedcor}

    \begin{proof}
        Per il teorema precedente, è sufficiente considerare un ordinamento topologico del grafo, dove in esso il primo vertice non ha archi entranti, mentre l'ultimo non ha archi uscenti.
    \end{proof}

    \begin{frameddefn}{Arborescenza}
        Sia $G$ un grafo diretto, e $r$ un suo vertice; $G$ è detto \tbf{arborescenza} se e solo se, per ogni vertice $v \in V(G) - \{r\}$, esiste uno ed un solo cammino diretto $r \rightarrow v$; in tal caso, $r$ prende il nome di \tbf{radice}.
    \end{frameddefn}

    \begin{framedobs}{Arborescenza}
        Sia $G$ un grafo diretto, e $v$ un suo vertice; allora, l'insieme degli archi raggiungibili da $v$ formano l'\tbf{arborescenza di $v$} in $G$, e $v$ prende il nome di \tbf{radice}. In simboli $$A_v := \{(v', v'') \in E(G) : v \rightarrow (v', v'')\} \subseteq E(G)$$ è l'arborescenza di $v$ in $G$. Si noti che, spesso, il sottografo generato dall'arborescenza di $v$ viene identificato con l'arborescenza stessa.
    \end{framedobs}

    \begin{example}[Arborescenza]
        Si consideri il seguente grafo diretto:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (2) {2};
                \node[main node] (4) [below left of=2] {4};
                \node[main node] (1) [above left of=4] {1};
                \node[main node] (5) [below right of=2] {5};
                \node[main node] (3) [above right of=5] {3};
                \node[main node] (6) [below left of=4] {6};
                \node[main node] (7) [below right of=4] {7};
                \node[main node] (8) [below right of=5] {8};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (4)
                    (2) edge (4)
                    (2) edge (5)
                    (3) edge (5)
                    (3) edge (8)
                    (4) edge (6)
                    (4) edge (7)
                    (5) edge (7)
                    ;
            \end{tikzpicture}
            \caption{Un grafo diretto.}
        \end{figure}

        in esso, ad esempio il sottografo dell'arborescenza di $3$ è

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (3) {3};
                \node[main node] (5) [below left of=3] {5};
                \node[main node] (7) [below left of=5] {7};
                \node[main node] (8) [below right of=5] {8};

                \path[every node/.style={font=\sffamily\small}]
                    (3) edge (5)
                    (3) edge (8)
                    (5) edge (7)
                    ;
            \end{tikzpicture}
            \caption{Arborescenza di $3$.}
        \end{figure}
    \end{example}

    \begin{frameddefn}{Grafo connesso}
        Un grafo è detto \tbf{connesso} se per ogni $v_1, v_2 \in V(G)$ esiste un cammino che li collega. Nel caso dei grafi indiretti, è sufficiente avere $v_1 \rightarrow v_2$, oppure $v_2 \rightarrow v_1$.
    \end{frameddefn}

    \begin{example}[Grafo non connesso]
        Ad esempio, si consideri questo grafo:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [below right of=1] {4};
                \node[main node] (5) [right of=4] {5};
                \node[main node] (6) [above right of=5] {6};
                \node[main node] (7) [below right of=5] {7};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (2)
                    (2) edge (3)
                    (2) edge (4)
                    (3) edge (4)
                    (4) edge (1)
                    (5) edge (6)
                    (6) edge (7)
                    (7) edge (5)
                    ;
            \end{tikzpicture}
            \caption{Un grafo non connesso.}
        \end{figure}

        Poiché non esiste cammino che possa collegare $4$ e $5$, il grafo non è connesso.
    \end{example}

    \begin{frameddefn}{Albero}
        Un grafo indiretto è detto \tbf{albero} se è connesso ed aciclico.
    \end{frameddefn}

    \begin{example}[Albero]
        Un esempio di albero è il seguente:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [below right of=1] {4};
                \node[main node] (5) [below right of=4] {5};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (2)
                    (3) edge (4)
                    (4) edge (1)
                    (4) edge (5)
                    ;
            \end{tikzpicture}
            \caption{Un albero.}
        \end{figure}
    \end{example}

    \begin{frameddefn}{Grafo fortemente connesso}
        Un grafo diretto è detto \tbf{fortemente connesso} se per ogni $v_1, v_2 \in V(G)$ esistono due cammini diretti, che li collegano in entrambe i versi; allora, è necessario avere $v_1 \rightarrow v_2$ e $v_2 \rightarrow v_1$. Si noti che ogni grafo indiretto connesso è anche fortemente connesso, poiché gli archi non hanno direzione.
    \end{frameddefn}

    \begin{example}[Grafo fortemente connesso]
        Un esempio di grafo fortemente connesso è il seguente:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [below right of=1] {4};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (2)
                    (2) edge (3)
                    (3) edge (4)
                    (4) edge (1)
                    ;
            \end{tikzpicture}
            \caption{Un grafo fortemente connesso.}
        \end{figure}
    \end{example}

    \begin{framedlem}{Grafi fortemente connessi}
        Sia $G$ un grafo diretto, e $u \in V(G)$ un suo vertice; allora, $G$ è fortemente connesso, se e solo se $\forall v \in V(G) \quad v \rightarrow u$ e $u \rightarrow v$.
    \end{framedlem}

    \begin{proof}
        \hspace{0.7cm}
        \begin{itemize}
            \item[] \tit{Prima implicazione.} L'implicazione è vera per definizione di grafo diretto fortemente connesso.
            \item[] \tit{Seconda implicazione.} Siano $x, y \in V(G)$; allora, per ipotesi, esistono cammini $x \rightarrow u$, $u \rightarrow x$, $y \rightarrow u$ e $u \rightarrow y$; inoltre, per definizione, tali cammini sono anche passeggiate. In particolare, poiché le passeggiate non hanno vincoli di attraversamento di vertici ed archi, allora è possibile utilizzare la proprietà transitiva, e dunque esistono passeggiate $x \rightarrow u \rightarrow y \implies x \rightarrow y$, e $y \rightarrow u \rightarrow x \implies y \rightarrow x$; allora, per il \cref{cammini e passeggiate}, esistono anche dei cammini $x \rightarrow y$ e $y \rightarrow x$. Allora, per definizione, per ogni coppia di vertici esistono due cammini in entrambe le direzioni, e dunque $G$ è fortemente connesso.
        \end{itemize}
    \end{proof}

    \begin{frameddefn}{Passeggiata euleriana}
        Una passeggiata si dice \tbf{euleriana} se attraversa ogni arco del grafo, senza ripercorrerne nessuno.
    \end{frameddefn}

    \begin{framedobs}{Passeggiate euleriane}
        Una passeggiata euleriana è una traccia passante per ogni arco del grafo.
    \end{framedobs}

    \begin{example}[Passeggiata euleriana]
        Ad esempio, si consideri il seguente grafo indiretto:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (2)
                    (2) edge (3)
                    ;
            \end{tikzpicture}
            \caption{Un grafo indiretto.}
        \end{figure}

        in esso, l'unica passeggiata euleriana è $$\{1, (1,2), 2, (2,3), 3\}$$
    \end{example}

    \begin{framedthm}{Circuiti euleriani}
        Dato un grafo $G$, esiste un circuito euleriano su $G$ se e solo se $G$ è connesso, ed ogni grado dei vertici di $G$ è pari.
    \end{framedthm}

    \begin{proof}
        \hspace{0.7cm}
        \begin{itemize}
            \item[] \tit{Prima implicazione.} Sia $G$ un grafo avente un circuito euleriano; per assurdo, sia $v \in V(G) : \deg(v)$ non sia pari. Allora, percorrendo $G$ secondo il circuito euleriano, giungendo a $v$ non si potrebbe più lasciare tale vertice senza riattraversare uno degli archi gia visitati $\lightning$. Inoltre, se $G$ non fosse connesso, il circuito non potrebbe essere euleriano poiché non potrebbe attraversare tutti gli archi di $G$.
            \item[] \tit{Seconda implicazione.} Omessa.
        \end{itemize}
    \end{proof}

    \begin{frameddefn}{Passeggiata hamiltoniana}
        Una passeggiata si dice \tbf{hamiltoniana} se attraversa ogni nodo del grafo, senza ripercorrerne nessuno.
    \end{frameddefn}

    \begin{framedobs}{Passeggiate hamiltoniane}
        Una passeggiata hamiltoniana è un cammino.
    \end{framedobs}

    \section{Rappresentazione}

    \subsection{Matrici di adiacenza}

    \begin{frameddefn}{Matrice di adiacenza}
        Sia $G = (V, E)$ un grafo; allora, è possibile rappresentare $G$ attraverso una matrice $M_G \in \textrm{Mat}_{n \times n}(\{0, 1\})$, dove $$\forall m_{i, j} \in M_G \quad m_{i, j} = \left \{ \begin{array}{ll} 1 & i \sim j\\ 0 & i \nsim j \end{array} \right.$$
    \end{frameddefn}

    \begin{framedobs}{Spazio di una matrice}
        Lo spazio utilizzato da una matrice di adiacenza è pari a $O(n^2)$, poiché è necessario rappresentare l'adiacenza di ogni vertice con ogni altro.
    \end{framedobs}

    \begin{framedobs}{Aggiornamento di una matrice}
        Per ogni grafo $G$ indiretto, si ha che $M_G$ è simmetrica; di conseguenza, il costo per aggiornare la corrispondente matrice di adiacenza è $2 O(1)= O(2) = O(1)$, poiché per $v_i, v_j \in V(G)$ non coincidenti, sarà necessario aggiornare $M_G[i, j]$ e $M_G[j, i]$.
    \end{framedobs}

    \begin{framedobs}{Eliminazione di un nodo}
        Per eliminare un nodo da un grafo indiretto, sarà necessario eliminare tutti i suoi collegamenti, e dunque il costo risulta essere $O(n)$ se si pone il valore \texttt{NULL} a l'intera riga e l'intera colonna del nodo da rimuovere, altrimenti è $O(n^2)$ nel caso in cui la matrice è da ricostruire.
    \end{framedobs}

    \begin{framedobs}{Controllo di adiacenza}
        Per controllare che $v_i, v_j \in V(G)$ non coincidenti siano adiacenti, è sufficiente controllare se $M_G[i, j] = M_G[j, i] = 1$, e dunque il costo di un controllo è $O(1)$.
    \end{framedobs}

    \subsection{Liste di adiacenza}

    \begin{frameddefn}{Liste di adiacenza}
        Sia $G = (V, E)$ un grafo; allora, è possibile rappresentare $G$ attraverso liste di adiacenza, salvando dunque una lista per ogni vertice, contenente i vertici ad esso adiacenti; in simboli $$\forall v \in V(G) \quad v:[\hat{v} \in V(G) - \{v\} \mid \hat{v} \sim v]$$
    \end{frameddefn}

    \begin{framedobs}{Spazio delle liste}
        Dato un certo $v \in V(G)$, la lista di adiacenza corrispondente ha lunghezza $\deg(v)$; allora, il numero di elementi nelle liste di adiacenza, per il \cref{Somma dei gradi}, è pari a $\displaystyle O \left(\sum_{v \in V(G)}{\deg(v)}\right) = O\left( 2 |E(G)| \right)=O(2m) = O(m)$. Si noti inoltre che, per un grafo con pochi archi, nonostante si abbiano le liste poco riempite, è comunque necessario salvare i puntatori a tali liste, e dunque è necessario introdurre un $O(n)$ nel costo totale dello spazio, ottenendo allora $O(n) + O(m) = O(n + m)$.
    \end{framedobs}

    \begin{framedobs}{Controllo di adiacenza}
        Per controllare che due nodi $v, v' \in V(G)$ siano adiacenti, è necessario controllare, ad esempio, se $v'$ è contenuto nella lista di $v$, e dunque il costo per tale controllo è $O(\deg(v))$.
        Si noti che, nel caso peggiore, il grafo rappresentato da liste di adiacenza sarà composto da una sola lista per un certo $v \in V(G)$, contenente ogni altro vertice del grafo $\hat{v} \in V(G) - \{v\}$, e la lunghezza della lista di adiacenza di $v$ sarà $n - 1$. Di conseguenza, nel caso peggiore, il costo per controllare se due vertici sono adiacenti è $O(n)$.
    \end{framedobs}

    \begin{framedobs}{Eliminazione di un nodo}
        Per effettuare la rimozione di un nodo da un grafo, è necessario rimuoverlo da ogni lista di adiacenza in cui compare, e nel caso peggiore esso ha archi verso tutti gli altri nodi; allora, il costo di tale operazione è dato dal maggiore tra $n$ ed $m$, e dunque $O(n) + O(m) = O(n + m)$.
    \end{framedobs}

    \begin{framedobs}{Grafo diretto}
        Si noti che per grafi diretti è necessario effettuare una scelta di rappresentazione: all'interno delle liste è possibile salvare i vertici entranti, i vertici uscenti, o entrambi (assegnando due liste ad ogni vertice).
    \end{framedobs}

    \begin{example}[Rappresentazione di un grafo]
        Ad esempio, si consideri il seguente grafo $G$:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [below right of=1] {4};
                \node[main node] (5) [above right of=4] {5};
                \node[main node] (6) [below right of=4] {6};

                \path[every node/.style={font=\sffamily\small}]
                    (2) edge (3)
                    (2) edge (4)
                    (3) edge (4)
                    (4) edge (1)
                    (4) edge (5)
                    (4) edge (6)
                    (5) edge (6)
                    ;
            \end{tikzpicture}
            \caption{Un grafo indiretto.}
        \end{figure}

        allora, la sua corrispondente matrice di adiacenza è
        $$M_G = \left( \begin{array}{llllll}
            0 & 0 & 0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 1 & 0 & 0 \\
            0 & 1 & 0 & 1 & 0 & 0 \\
            1 & 1 & 1 & 0 & 1 & 1 \\
            0 & 0 & 0 & 1 & 0 & 1 \\
            0 & 0 & 0 & 1 & 1 & 0 \\
        \end{array} \right)$$

        mentre le corrispondenti liste di adiacenza sono $$\left \{ \begin{array}{l} 1: \texttt{[}4\texttt{]} \\ 2: \texttt{[}4, 3\texttt{]} \\ 3: \texttt{[}2, 4\texttt{]} \\ 4: \texttt{[}1, 2, 3, 5, 6\texttt{]} \\ 5: \texttt{[}4, 6\texttt{]} \\ 6: \texttt{[}4, 5\texttt{]} \end{array} \right.$$
    \end{example}

    \section{Depth-first Search (DFS)}

    \subsection{Trovare un ciclo}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo indiretto $G$, con ogni vertice avente grado almeno pari a $2$, l'algoritmo restituisce un ciclo di $G$.\\
            \textbf{Input}: $G$ grafo indiretto, tale che $\forall v \in V(G) \quad \deg(v) \ge 2$.\\
            \textbf{Output}: un ciclo di $G$.
        }

        \begin{algorithmic}[1]
            \Function{findCycle}{$G$}
                \State $v \in V(G)$ \Comment{un vertice qualsiasi di $G$}
                \State $\texttt{visited} := \verb|{|v\verb|}|$ \Comment{conterrà i vertici visitati}
                \State $v' \in V(G) : v \sim v'$
                \While{$v' \notin \texttt{visited}$} \Comment{tempo costante perché \texttt{visited} è un set}
                    \State $\texttt{visited.add(}v'\texttt{)}$
                    \State $v' := v'' \in V(G) : \left \{ \begin{array}{l}v' \sim v'' \\ v'' \neq \texttt{visited[visited.length()} - 2\texttt{]} \end{array} \right.$
                \EndWhile
                \State \textbf{return} $\texttt{visited[visited.indexOf(}v'\texttt{)} \texttt{:} \texttt{visited.length()]}$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia scegliendo un qualsiasi vertice di $G$, denotato alla riga $2$ con $v$; successivamente, alla riga $3$ viene inizializzato un insieme \texttt{visited} che contterà tutti i vertici visitati attraverso l'algoritmo; inotre, alla riga $4$ viene scelto un altro vertice $v'$, che sia adiacente al $v$ di partenza.

        All'interno del ciclo \texttt{while}, alla riga $6$ l'algoritmo salva $v'$ all'interno dell'insieme di vertici visitati, mentre alla riga $7$ viene rimpiazzato $v'$, scegliendo un nuovo vertice, adiacente a $v'$, che sia diverso dal penultimo vertice inserito all'interno di \texttt{visited}. Il motivo per cui quest'ultimo controllo è necessario, è che il penultimo vertice inserito sarà il vertice dal quale $v'$ proveniva, di conseguenza si rischierebbe di ripercorrere uno stesso vertice più di una volta, e dunque non si formerebbe un ciclo. Si noti che è necessaria l'ipotesi per cui $G$ abbia ogni vertice di grado almeno pari a $2$, altrimenti non sarebbe possibile trovare un vertice differente dal penultimo di \texttt{visited}. Il ciclo termina nel momento in cui viene scelto un $v'$ già presente all'interno di \texttt{visited}, in quanto, poiché non è possibile ripercorrere i propri passi, l'unica possibilità in cui si è giunti ad un vertice già visitato è se si è concluso un ciclo.
        
        L'algoritmo termina restituendo uno slice dell'insieme, partendo dal primo indice di $v'$ disponibile (si noti che alla fine dell'algoritmo anche l'ultimo elemento di \texttt{visited} sarà $v'$), fino alla fine.

        Si noti che, nella maggior parte dei linguaggi di programmazione, gli insiemi non hanno ordine, dunque non sarebbe possibile restituire uno slice di \texttt{visited}; allora, per semplicità, all'interno dello pseudocodice presentato, si assume si stia utilizzando una struttura dotata di hashing per l'univocità degli elementi, e di ordine per restituirne uno slice, ad esempio un \texttt{IndexSet}.
    \end{proof}

    \begin{framedobs}{Porzione di \ttt{visited}}
        Si noti che \texttt{visited} contiene tutti i nodi visitati, dunque restituire interamente l'insieme potrebbe non fornire un ciclo, come nel seguente grafo
        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [below right of=1] {4};

                \path[every node/.style={font=\sffamily\small}]
                    (2) edge (3)
                    (2) edge (4)
                    (3) edge (4)
                    (4) edge (1)
                    ;
            \end{tikzpicture}
            \caption{Un grafo diretto contenente un ciclo.}
        \end{figure}

        in cui, ad esempio partendo da $v= 4$, l'unico ciclo è $$\{4, (4, 3), 3, (3, 2), 2, (2, 4), 4\}$$ nonostante al termine dell'algoritmo si avrebbe $\texttt{visited}=[1, 4, 3, 2, 4]$, che non costituisce un ciclo.
    \end{framedobs}

    \begin{proof}[Costo dell'algoritmo.]
        Il costo di questo algoritmo dipende dalla struttura dati utilizzata per rappresentare il grafo in input: nel caso in cui $G$ è rappresentato attraverso una matrice di adiacenza, il costo del ciclo \texttt{while} è pari a $O(n)$, poiché la riga $7$ richiede di trovare un $v'' \in V(G) : v' \sim v''$, il ché potrebbe portare a dover scorrere tutta la riga/colonna di $v''$, dunque nel caso peggiore $O(n)$; differentemente, rappresentando $G$ attraverso liste di adiacenza, basta scegliere il primo vertice contenuto nella lista di $v''$, e se questo dovesse coincidere con il penultimo elemento di \texttt{visited}, sarà sufficiente scegliere il secondo elemento della lista (sicuramente presente per come $G$ è scelto in ipotesi), dunque si ha $O(2) = O(1)$.

        Infine, si noti che il ciclo \texttt{while} ha costo $O(n)$, poiché nel caso peggiore si ha un ciclo che percorre tutto il grafo.

        Allora, attraverso una rappresentazione matriciale, l'algoritmo ha costo $O(n) \cdot O(n) = O(n^2)$, mentre attraverso la rappresentazione con liste di adiacenza, si ha $O(1) \cdot O(n) = O(n)$.
    \end{proof}

    \subsection{Visita in DFS}

    \begin{frameddefn}{DFS}
        Con DFS si indica un criterio di visita di un grafo; in particolare, DFS sta per \tit{Depth-first Search}, dunque la visita del grafo avviene procendendo sempre più in profondità, retrocedendo esclusivamente se non è più possibile avanzare.
    \end{frameddefn}

    \begin{algorithm}[H]
        \caption{
            Prima versione dell'algoritmo; dato un grafo $G$, diretto o indiretto, e un suo vertice $v$, l'algoritmo restituisce tutti i vertici, raggiungibili attraverso cammini, partendo da $v$.\\
            \textbf{Input}: $G$ un grafo; $v$ un vertice di $G$.\\
            \textbf{Output}: i vertici raggiungibili da $v$.
        }

        \begin{algorithmic}[1]
            \Function{findReachableNodes$_1$}{$G$, $v$}
                \State $\texttt{visited}:=\texttt{[}0\texttt{]} * n$ \Comment{array di $n$ zeri}
                \State $\texttt{visited[}v\texttt{]} = 1$
                \State $\texttt{Stack S}:=\texttt{[}v\texttt{]}$
                \While{$!\texttt{S.isEmpty()}$}
                    \State $v_{top}:=\texttt{S.top()}$
                    \If{$\exists z \in V(G) : \left \{ \begin{array}{l}z \sim v_{top} \\ \texttt{visited[}z\texttt{]}=0 \end{array} \right.$}
                        \State $\texttt{S.push(}z\texttt{)}$
                        \State $\texttt{visited[}z\texttt{]} = 1$
                    \Else
                        \State $\texttt{S.pop()}$
                    \EndIf
                \EndWhile
                \State \textbf{return} \texttt{visited}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}
        Per assurdo, sia $\hat v \in V(G)$, raggiungibile da $v$ attraverso un cammino, che non sia stato raggiunto dall'algoritmo; allora, per definizione esiste un cammino $\{v, e_1, v_1, \ldots, v_{n - 1}, v_n, \hat v\}$; inoltre, sia $v_i$ il vertice con indice maggiore all'interno del cammino, raggiunto dall'algoritmo, e dunque avendo che $\left \{ \begin{array}{l}v_i \sim v_{i + 1} \\ v_{i + 1} \notin \texttt{visited}  \end{array} \right.$. Allora, per costruzione dell'algoritmo, $v_i$ sarebbe stato rimosso dallo stack, alla riga $11$, prima che $v_{i + 1}$ potesse essere visitato; ma poiché $v_i \sim v_{i +1}$, allora l'algoritmo dovrebbe aver sbagliato esecuzione, poiché $v_{i +1}$ sarebbe stato raggiunto alla riga $7$ inevitabilmente $\lightning$.
    \end{proof}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia definendo un array \texttt{visited}, alla riga 2, contenente inizialmente il valore sentinella 0 per ogni vertice, il quale marcherà la presenza di nodi non ancora visitati dall'algoritmo; successivamente, viene segnato $v$, il vertice in input, come visitato all'interno di \texttt{visited}, alla riga 3, e viene posto all'interno dello stack \texttt{S}, alla riga 4.

        Alla riga 5, viene inizializzato un ciclo \texttt{while}, all'interno del quale, fintanto che lo stack \texttt{S} non è vuoto, alla riga 6 viene definito $v_{top}$, pari al primo elemento dello stack, e se esiste un vertice $z \in V(G)$, non ancora visitato dall'algoritmo ($\texttt{visited[}z\texttt{]}=0$), tale che sia adiacente a $v_{top}$, viene inserito all'interno dello stack, alla riga 8, e viene marcato come visitato, alla riga 9. Si noti che queste due ultime operazioni garantiscono che la visita del grafo sia in DFS, poiché $v_{top}$ alla prossima iterazione del \texttt{while} sarà proprio il vertice $z$ appena inserito, e dunque l'algoritmo sta procedendo più in profondità che si possa; se invece un tale vertice $z$ non esiste, può voler dire esclusivamente che ogni singolo vertice adiacente a $v_{top}$ corrente è già stato visitato dall'algoritmo, e dunque è possibile retrocedere di profondità nella visita del grafo, andando dunque a rimuovere dallo stack l'attuale primo vertice, alla riga 11.

        Per terminare, è sufficiente dunque ritornare l'array \texttt{visited}, che conterrà 1 su tutti e soli i vertici che sono stati raggiunti dall'algoritmo, e sono dunque raggiungibili dal vertice di partenza $v$; infatti, gli unici vertici che non sono stati raggiunti dall'algoritmo, sono vertici che non sono raggiungibili, partendo da $v$, attraverso cammini. Si noti che, nel caso di un $G$ indiretto, è sempre possibile raggiungere qualsiasi vertice, partendo da qualsiasi nodo, a meno di grafi non connessi.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Si consideri $G$ rappresentato attraverso matrice di adiacenza; allora, il costo della riga $7$, nel caso peggiore, è $O(n)$, poiché è necessario controllare tutta la riga/colonna di $v_{top}$ per trovare un vertice $z$ tale che $\texttt{visited[}z\texttt{]} = 0$, dunque non sia stato ancora visitato. Per ragione analoga, rappresentando $G$ attraverso liste di adiacenza, nel caso peggiore si ha una sola lista corrispondente ad un singolo vertice di $G$, e sarà dunque necessario effettuare $O(n - 1) = O(n)$ controlli.

        Inoltre, si noti che il caso peggiore dell'algoritmo si ha quando $v$ può raggiungere ogni altro nodo di $G$, e dunque il ciclo \texttt{while} sarà ripetuto $O(2n - 1) = O(2n) = O(n)$ volte, poiché ogni vertice verrà inserito e rimosso dallo stack, eccetto il primo, inserito alla riga $4$.

        Allora, il costo complessivo dell'algoritmo, indipendentemente dalla rappresentazione di $G$, è pari a $O(n) \cdot O(n) = O(n^2)$.
    \end{proof}

    \begin{framedobs}[label={Sottografo di un grafo indiretto}]{Albero}
        Sia $G$ un grafo indiretto; considerando l'insieme di archi attraversati dall'algoritmo per trovare ogni vertice raggiungibile partendo da $v$, al termine della procedura si ottiene un sottografo indiretto di $G$ connesso ed aciclico: connesso, poiché l'algoritmo procede per adiacenza di vertici, ed aciclico, poiché l'algoritmo non visita lo stesso vertice più di una volta. Allora, per definizione, tale sottografo è un albero.
    \end{framedobs}

    \begin{framedobs}{Arborescenza}
        Sia $G$ un grafo diretto; considerando l'insieme di archi attraversati dall'algoritmo per trovare ogni vertice raggiungibile partendo da $v$, al termine della procedura si ottiene un sottografo diretto di $G$ connesso ed aciclico, per gli stessi motivi dell'\cref{Sottografo di un grafo indiretto}; tale sottografo è un arborescenza di $v$.
    \end{framedobs}

    \begin{algorithm}[H]
        \caption{
            Seconda versione dell'algoritmo; dato un grafo $G$, diretto o indiretto, rappresentato attraverso liste di adiacenza, e un suo vertice $v$, l'algoritmo restituisce tutti i vertici, raggiungibili attraverso cammini, partendo da $v$.\\
            \textbf{Input}: $G$ un grafo, rappresentato attraverso liste di adiacenza; $v$ un vertice di $G$.\\
            \textbf{Output}: i vertici raggiungibili da $v$.
        }

        \begin{algorithmic}[1]
            \label{findReachableNodes2}
            \Function{findReachableNodes$_2$}{$G$, $v$}
                \State $\texttt{visited} := \verb|{|v\verb|}|$
                \State $\texttt{Stack S} := \texttt{[}v\texttt{]}$
                \While{$!\texttt{S.isEmpty()}$}
                    \State $v_{top}:=\texttt{S.top()}$
                    \While{$!v_{top}\texttt{.adjacent().isEmpty()}$}
                        \State $z := v_{top}\texttt{.adjacent()[}0\texttt{]}$
                        \State $v_{top}\texttt{.adjacent().remove(}0\texttt{)}$ \Comment{fa la differenza}
                        \If{$z \notin \texttt{visited}$}
                            \State $\texttt{visited.add(}z\texttt{)}$
                            \State $\texttt{S.push(}z\texttt{)}$
                            \State \textbf{break}
                        \EndIf
                    \EndWhile
                    \If{$v_{top}\texttt{ == S.top()}$}
                        \State \texttt{S.pop()}
                    \EndIf
                \EndWhile
                \State \textbf{return} \texttt{visited}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        Questa seconda versione dell'algoritmo presenta una miglioria sostanziale alla riga $8$: infatti, attraverso questa riga si rimuovono di volta in volta i vertici adiacenti appena visitati; di conseguenza, i vertici adiacenti da controllare saranno progressivamente sempre meno. Infatti, si noti che senza la riga $8$, l'algoritmo si comporterebbe come la prima versione.

        Il \texttt{break} alla riga $12$ interrompe il ciclo \texttt{while} della riga $6$, facendo sì che $v_{top}$ della riga $5$, all'iterazione successiva del \texttt{while} della riga $4$, sia pari a $z$, dunque cambiando il vertice correntemente in esame. Di conseguenza, alla riga $15$ il controllo sarà valutato a \texttt{True} esclusivamente se non è mai stata eseguita la riga $11$ per tutta l'iterazione del ciclo \texttt{while} della riga $6$, ovvero quando tutti i vertici adiacenti a $v_{top}$ sono già stati visitati.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        \label{costo FRN2}
        Poiché i nodi visitati vengono eliminati, il costo del ciclo \texttt{while} dipende da operazioni eseguite in tempo costante $O(1)$, e da quanti nodi vengono controllati per ogni iterazione del ciclo, ma poiché non si possono ricontrollare più volte gli stessi nodi, allora il costo del ciclo dipende solamente dalla dimensione delle liste di adiacenza, e dunque si ha $\displaystyle O\left( \sum_{v \in V(G)}{1 + \deg(v)}\right)=O\left(\sum_{v \in V(G)}{1}\right) + O\left(\sum_{v \in V(G)}{\deg(v)}\right) = O(n) + O(m) = O(n+ m)$, per il \cref{Somma dei gradi}.
    \end{proof}

    \begin{framedobs}{Grafo diretto}
        Per estendere questo algoritmo a grafi diretti, è necessario fornire in input un grafo rappresentato attraverso liste di adiacenza, le quali devono contenere esclusivamente i vertici uscenti, poichè sono gli unici archi percorribili.
    \end{framedobs}

    \subsection{Trovare un ordinamento topologico}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo diretto aciclico $G$, l'algoritmo restituisce un suo ordinamento topologico.\\
            \textbf{Input}: $G$ grafo diretto aciclico.\\
            \textbf{Output}: un ordinamento topologico di $G$.
        }

        \begin{algorithmic}[1]
            \Function{findTopologicalSorting}{$G$}
                \State $\texttt{order} := \texttt{[}\texttt{]}$
                \While{$V(G) \neq 0$}
                    \State $v \in V(G) : v.\texttt{incoming\_adjacent().length()} = 0$
                    \State $\texttt{order.append(}v\texttt{)}$
                    \State $V(G)\texttt{.remove(}v\texttt{)}$
                \EndWhile
                \State \textbf{return} \texttt{order}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia definendo una lista vuota \texttt{order}, all'interno della quale verrà salvato l'ordinamento topologico; successivamente, alla riga $3$, viene inizializzato un ciclo \texttt{while} che, in ogni iterazione, trova un vertice $v$ il cui numero di vertici adiacenti entranti è $0$, e lo inserisce in \texttt{order}; questo garantisce che ogni vertice inserito venga necessariamente inserito prima di ogni suo arco uscente, e ne esiste sempre almeno uno grazie al \cref{vertici particolari}. Si noti inoltre che, poiché $G$ è aciclico, è garantito che rimuovendo un vertice senza archi entranti, il grafo risultante sarà ancora aciclico, ed è possibile dunque ripetere il ragionamento induttivamente per poter dimostrare la correttezza dell'algoritmo.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Il ciclo \texttt{while} della riga $3$, indipendentemente dalla struttura di rappresentazione del grafo $G$, deve essere eseguito $n$ volte, e dunque ha costo $O(n)$, poiché l'ordinamento topologico deve coinvolgere ogni nodo del grafo, e alla riga $6$ i nodi controllati vengono progressivamente rimossi.

        Rappresentando $G$ attraverso matrice di adiacenza, indicando con $1$ i vertici adiacenti entranti, il costo della riga $4$ è pari a $O(n^2)$, poiché per trovare un vertice $v$ che non abbia archi entranti, è necessario controllare tutta la sua riga/colonna, e dunque nel caso peggiore, per trovarlo sarà necessario controllare l'intera matrice; inoltre, per effettuare la rimozione di $v$ alla riga $6$, il costo è $O(n)$, Allora, il costo complessivo dell'algoritmo risulta essere $O(n) \cdot [O(n^2) + O(n)]= O(n) \cdot O(n^2) = O(n^3)$.

        Differentemente, rappresentando $G$ attraverso liste di adiacenza, salvando solamente i vertici adiacenti entranti per ogni nodo, alla riga $4$ per trovare un nodo senza archi entranti è sufficiente controllare il numero di elementi della lista di ogni vertice, operazione a costo $O(1)$, e dunque il costo, nel caso peggiore, è $O(n - 1) = O(n)$; inoltre, per rimuovere $v$ alla riga $6$, il costo è pari a $O(n + m)$. Allora, il costo complessivo dell'algoritmo risulta essere $O(n) \cdot [O(n) + O(n + m)] = O(n) \cdot [O(2n + m)] = O(n) \cdot O(n + m) = O(n \cdot (n +m))$.
    \end{proof}

    \section{Tempi di visita e di chiusura}

    \subsection{Definizioni}

    \begin{frameddefn}{Tempo di visita e di chiusura}
        All'interno degli algoritmi che visitano grafi secondo DFS, è possibile introdurre un \texttt{counter} inizializzato ad $1$, ed incrementato ogni volta che viene attraversato un \tit{nuovo} vertice.

        Allora, per ogni vertice $v$ del grafo diretto in input, si definiscono $t(v)$, detto \tbf{tempo di visita di $v$}, pari al valore del \texttt{counter} la prima volta che $v$ viene visitato, e $T(v)$, detto \tbf{tempo di chiusura di $v$}, pari al valore del \texttt{counter} nel momento in cui $v$ viene rimosso dallo stack.

        Inoltre, si definisce $\mathrm{Int}(v) := [t(v), T(v)]$.
    \end{frameddefn}

    \begin{framedobs}{Intervalli delle foglie}
        Si noti che per ogni foglia $v$ del grafo, ovvero i vertici per i quali non è più possibile scendere di profondità, si ha $t(v) = T(v)$, per definizione stessa dei tempi.
    \end{framedobs}

    \begin{framedlem}[label={Intervalli diretto}]{Proprietà degli intervalli}
        Sia $G$ un grafo diretto, e $u, v \in V(G)$ adiacenti; allora solo una delle seguenti proposizioni è vera:
        \begin{enumerate}[label=\roman*), font=\itshape]
            \item $\mathrm{Int}(u) \subseteq \mathrm{Int}(v)$
            \item $\mathrm{Int}(v) \subseteq \mathrm{Int}(u)$
            \item $\mathrm{Int}(u) \cap \mathrm{Int}(v) = \varnothing$
        \end{enumerate}
        
        Dunque, gli intervalli o sono l'uno interamente contenuto nell'altro, o non si intersecano.
    \end{framedlem}

    \begin{proof}
        La tesi equivale a dimostrare che non può verificarsi il caso in cui c'è intersezione \tbf{propria} non vuota tra i due intervalli, e dunque non è possibile che $\mathrm{Int}(u) \cap \mathrm{Int}(v) \neq \varnothing$, ovvero $t(u) < t(v) < T(u) < T(v)$, allora:

        \begin{itemize}
            \item $t(u) < t(v) \implies u$ inserito nello stack prima di $v$
            \item $t(v) < T(u) \implies u$ viene rimosso dallo stack dopo aver visitato $v$, ma poiché $u$ era sotto a $v$ all'interno dello stack, necessariamente $v$ deve essere stato rimosso dallo stack prima di $u$, e allora non è possibile che $T(u) < T(v) \ \lightning$
        \end{itemize}
    \end{proof}

    \begin{framedobs}{Intervalli disgiunti}
        Si noti che, avendo un $G$ grafo diretto, e un arco $(u, v) \in E(G)$, dunque con $u$ incidente su $v$, si ha che $$\mathrm{Int}(u) \cap \mathrm{Int}(v) = \varnothing \implies t(v) < T(v) < t(u) \le T(u)$$ e non $t(u) \le T(u) < t(v) < T(v)$, poiché $T(u) < t(v)$ implicherebbe che la visita in DFS avrebbe sbagliato a rimuovere $u$ dallo stack prima che $v$ potesse essere visitato.
    \end{framedobs}

    \begin{framedlem}{Proprietà degli intervalli}
        Sia $G$ un grafo indiretto, e $u, v \in V(G)$ adiacenti; allora si verifica una sola tipologia di inclusione, in cui $\mathrm{Int}(u) \subseteq \mathrm{Int}(v)$, oppure $\mathrm{Int}(v) \subseteq \mathrm{Int}(u)$, e poiché gli archi non sono orientati perde di significato la distinzione tra i due casi.
    \end{framedlem}

    \begin{proof}
        La tesi equivale a dimostrare che non può verificarsi il caso in cui c'è intersezione vuota tra i due intervalli, e dunque non è possibile che $\mathrm{Int}(u) \cap \mathrm{Int}(v) = \varnothing$, ovvero $t(u) \le T(u) < t(v) \le T(v)$, poiché $T(u) < t(v)$ implicherebbe che $u$ verrebbe rimosso dallo stack prima che $v$ possa essere inserito, e questo non è possibile per costruzione della visita DFS, poiché $u \sim v$.
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo $G$, rappresentato attraverso liste di adiacenza (nel caso di $G$ diretto, l'adiacenza è dei nodi uscenti), e un suo vertice $r$, l'algoritmo restituisce i tempi di visita e di chiusura dei nodi di $G$, relativi alla visita dell'albero, o dell'arborescenza, di $r$.\\
            \textbf{Input}: $G$ grafo, rappresentato attraverso liste di adiacenza; $r$ un vertice di $G$.\\
            \textbf{Output}: tempi di visita e di chiusura dei $v \in V(G)$, relativi all'albero, o all'arborescenza, di $r$.
        }

        \begin{algorithmic}[1]
            \Function{DFS}{$G$, $v$, \texttt{visited}, \texttt{c}, \texttt{t}, \texttt{T}}
                \For{$u \in V(G) : (v, u) \in E(G)$} \Comment{$u$ deve essere uscente da $v$}
                    \If{$u \notin \texttt{visited}$}
                        \State $\texttt{c.increment()}$
                        \State $\texttt{t[}u\texttt{]}=\texttt{c}$
                        \State $\texttt{visited.add(}u\texttt{)}$
                        \State $\texttt{DFS(}G, u, \texttt{visited}, \texttt{c}, \texttt{t}, \texttt{T)}$
                    \EndIf
                \EndFor
                \State $\texttt{T[}v\texttt{]} = \texttt{c}$
            \EndFunction
            \\
            \Function{findTimes}{$G$, $r$}
                \State $\texttt{visited} := \verb|{|r\verb|}|$
                \State $\texttt{t} := \texttt{[}0\texttt{]} * n$
                \State $\texttt{T} := \texttt{[}0\texttt{]} * n$
                \State $\texttt{t[}r\texttt{]}=1$
                \State $\texttt{Counter c} := 1$ \Comment{questo contatore \underline{deve essere un oggetto}}
                \State $\texttt{DFS(}G, r, \texttt{visited}, \texttt{c}, \texttt{t}, \texttt{T)}$
                \State \textbf{return} \texttt{t}, \texttt{T}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia salvando la radice $r$ all'interno di un insieme \texttt{visited}, ed inizializzando gli array \texttt{t} e \texttt{T} con $0$; inoltre, il tempo di visita di $r$ viene inizializzato a $1$, alla riga $17$; infine, viene istanziato un contatore, partendo da $1$ (poiché la radice è già stata inizializzata).

        All'interno della funzione ricorsiva, per ogni livello della ricorsione, viene esplorato ogni vertice adiacente a $v$ in ingresso; in particolare, se gia non visitato, viene scelto un $u$ tale che $u \sim v$. Successivamente, viene aggiornato il contatore, e viene salvato il tempo di visita di $u$; infine, viene aggiunto il vertice a \texttt{visited}. Alla riga $7$, la funzione ricorsiva viene eseguita nuovamente, utilizzando come nuovo nodo di partenza $u$. Infine, per ogni livello di ricorsione, dopo aver terminato i vertici adiacenti, il ciclo \texttt{for} della riga $2$ termina, e alla riga $10$ viene aggiornato il tempo di chiusura del vertice $v$.

        Allora, il codice è in grado di visitare il grafo intermaente, senza ripercorrere vertici già visitati, utilizzando una visita in DFS, poiché vengono esplorati tutti i vertici adiacenti ricorsivamente, prima di tornare al vertice precedente.

        Si noti che l'algoritmo funziona correttamente, solamente se \texttt{c} è un oggetto e non una variabile; infatti, il contatore si deve comportare come se fosse globale per ogni livello di ricorsione, altrimenti i tempi sarebbero tutti errati; in particolare, senza trattare il contatore come oggetto, il contatore, ritornando indietro con i livelli ricorsivi, decrementerebbe.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Si noti che l'algoritmo controlla ogni singolo vertice una ed una sola volta, e la vista avviene in DFS; inoltre, poiché si ha un ciclo \texttt{for} all'interno di ogni livello della ricorsione, per costruzione stessa, l'algoritmo si comporta esattamente come l'\cref{findReachableNodes2}. Allora, poiché il grafo è rappresentato tramite liste di adiacenza, per ragionamento analogo all'\cref{costo FRN2}, il costo dell'algoritmo è pari a $O(n + m)$.
    \end{proof}

    \subsection{Categorie di archi}

    \begin{frameddefn}{Categorie di archi}
        Sia $G= (V, E)$ un grafo diretto, $\hat v \in V(G)$, e sia $A_{\hat v}$ la sua arborescenza; allora, è possibile classificare ogni arco $(u, v) \in E(G) - E(A_{\hat v})$, mediante $\mathrm{Int}(u)$ e $\mathrm{Int}(v)$:
        \begin{itemize}
            \item $\mathrm{Int}(u) \subseteq \mathrm{Int}(v)$, allora l'arco $(u, v)$ è un \tbf{backward edge}, ovvero all'indietro: sono gli archi che congiungono due nodi dello stesso ramo di $A_{\hat v}$, nel caso in cui $u$ è più in profondità di $v$ nella visita DFS 
            \item $\mathrm{Int}(v) \subseteq \mathrm{Int}(u)$, allora l'arco $(u, v)$ è un \tbf{forward edge}, ovvero in avanti: sono gli archi che congiungono due nodi dello stesso ramo di $A_{\hat v}$, nel caso in cui $v$ è più in profondità di $u$ nella visita DFS
            \item $\mathrm{Int}(v) \cap \mathrm{Int}(u) = \varnothing$, allora l'arco $(u, v)$ è un \tbf{cross edge}, detto \tbf{arco di attraversamento}: sono gli archi che congiungono due nodi di rami differenti dell'arborescenza $A_{\hat v}$
        \end{itemize}
    \end{frameddefn}

    \begin{frameddefn}{Categorie di archi}
        Sia $G=(V, E)$ un grafo indiretto, $\hat v \in V(G)$, e sia $T_{\hat v}$ il suo albero; allora, ogni arco $(u, v) \in E(G) - E(T_{\hat v})$ viene classificato come \tbf{backward edge}.
    \end{frameddefn}

    \begin{example}
        Ad esempio, si consideri il seguente multigrafo diretto $G$:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [above right of=1] {2};
                \node[main node] (3) [right of=2] {3};
                \node[main node] (4) [right of=1] {4};
                \node[main node] (5) [below of=1] {5};
                \node[main node] (6) [right of=4] {6};
                \node[main node] (7) [right of=5] {7};
                \node[main node] (8) [below right of=6] {8};
                \node[main node] (9) [below of=7] {9};
                \node[main node] (10) [below left of=8] {10};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge[bend left] (2)
                    (1) edge (5)
                    (1) edge[bend right] (7)
                    (2) edge (1)
                    (2) edge (3)
                    (2) edge (4)
                    (3) edge (4)
                    (3) edge (6)
                    (4) edge (1)
                    (5) edge[bend right] (7)
                    (7) edge[bend right] (1)
                    (7) edge (6)
                    (7) edge (8)
                    (7) edge (9)
                    (7) edge (10)
                    (8) edge (4)
                    (8) edge (10)
                    (9) edge[bend left] (5)
                    ;
            \end{tikzpicture}
            \caption{Un multigrafo diretto.}
        \end{figure}

        sia $A_1$ la seguente arborescenza di visita in DFS di $G$, radicata in 1:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [above right of=1] {2};
                \node[main node] (3) [right of=2] {3};
                \node[main node] (4) [right of=1] {4};
                \node[main node] (5) [below of=1] {5};
                \node[main node] (6) [right of=4] {6};
                \node[main node] (7) [right of=5] {7};
                \node[main node] (8) [below right of=6] {8};
                \node[main node] (9) [below of=7] {9};
                \node[main node] (10) [below left of=8] {10};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge[bend left] (2)
                    (1) edge[bend right] (7)
                    (2) edge (3)
                    (3) edge (4)
                    (3) edge (6)
                    (7) edge (8)
                    (7) edge (9)
                    (8) edge (10)
                    (9) edge[bend left] (5)
                    ;
            \end{tikzpicture}
            \caption{Un'arborescenza, radicata in 1, di un multigrafo diretto.}
        \end{figure}

        e siano inoltre i seguenti i tempi di visita e di chiusura di ogni vertice di $G$, relativi ad $A_1$

        \begin{center}
            \begin{tabular}{c|c c} 
                 \hline
                 $v$ & $t(v)$ & $T(v)$ \\
                 \hline\hline
                 1 & 1 & 10 \\ 
                 \hline
                 2 & 2 & 5 \\
                 \hline
                 3 & 3 & 5 \\
                 \hline
                 4 & 5 & 5 \\
                 \hline
                 5 & 10 & 10 \\
                 \hline
                 6 & 4 & 4 \\
                 \hline
                 7 & 6 & 10 \\
                 \hline
                 8 & 7 & 8 \\
                 \hline
                 9 & 9 & 10 \\
                 \hline
                 10 & 8 & 8 \\
                 \hline
            \end{tabular}
        \end{center}

        allora, è possibile classificare gli archi in $E(G) - E(A_1)$, attraverso i loro tempi di visita e di chiusura come segue 

        \begin{center}
            \begin{tabular}{c||c c} 
                 \hline
                 \tbf{backward} & $\{(2, 1), (4, 1), (5, 7), (7, 1)\}$ \\
                 \hline
                 \tbf{forward} & $\{(1, 5), (2, 4), (7, 10)\}$\\ 
                 \hline
                 \tbf{cross} & $\{(7, 6), (8, 4)\}$\\
                 \hline
            \end{tabular}
        \end{center}
    \end{example}

    \begin{framedthm}{Presenza di cicli}
        Sia $G$ un grafo indiretto connesso; allora $G$ ha un ciclo se e solo se in esso esiste un backward edge in ogni albero.
    \end{framedthm}

    \begin{proof}
        \hspace{0.7cm}
        \begin{itemize}
            \item[] \tit{Prima implicazione.} Per assurdo, sia $G$ un grafo indiretto, in cui è presente almeno un ciclo, e non sono presenti backward edge; inoltre, sia $\hat v \in V(G)$, e sia $T_{\hat v}$ il suo albero. Allora, poiché $G$ non ha backward edge, necessariamente gli unici suoi archi sono quelli che compongono $T_{\hat v}$, e dunque si ha che $E(G) = E(T_{\hat v}) \implies G = T_{\hat v} \implies G$ è un albero, e di conseguenza $G$ non ha cicli $\lightning$.
            \item[] \tit{Seconda implicazione.} Sia $G$ un grafo indiretto connesso, sia $\hat v \in V(G)$, sia $T_{\hat v}$ il suo albero, e sia $(u, v) \in E(G) - E(T_{\hat v})$ un backward edge. Allora, poiché $u, v \in V(T_{\hat v})$, è sufficiente considerare il cammino tale che $u \rightarrow v$, che esiste poiché $T_{\hat v}$ è un albero, e dunque $\{u \rightarrow v\} \cup (u, v)$ è un ciclo di $G$.
        \end{itemize}
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo diretto $G$, rappresentato attraverso liste di adiacenza (per ogni vertice sono salvate due liste, dei vertici entranti e dei vertici uscenti), e un suo vertice $v$, l'algoritmo restituisce gli archi non facenti parti dell'arborescenza di $v$, categorizzati in base ai loro intervalli di apertura e chiusura.\\
            \textbf{Input}: $G$ grafo diretto, rappresentato attraverso liste di adiacenza; $v$ un vertice di $G$.\\
            \textbf{Output}: archi non dell'arborescenza, categorizzati per intervalli.
        }

        \begin{algorithmic}[1]
            \Function{categorizeEdges}{$G$, $v$}
                \State $\texttt{visited} := \texttt{[}0\texttt{]} * n$
                \State $\texttt{visited[}v\texttt{]}=1$
                \State $\texttt{Stack S}:=\texttt{[}v\texttt{]}$
                \State $\texttt{c}:=1$
                \State $\texttt{t} := \texttt{[}0\texttt{]} * n$ \Comment{tempi di visita}
                \State $\texttt{T} := \texttt{[}0\texttt{]} * n$ \Comment{tempi di chiusura}
                \State $\texttt{t[}v\texttt{]}=\texttt{c}$
                \State $\texttt{parents} := \texttt{[}0\texttt{]} * n$
                \State $\texttt{parents[}v\texttt{]}=v$ \Comment{per riconoscere la radice}
                \While{$!\texttt{S.isEmpty()}$}
                    \State $v_{top} := \texttt{S.top()}$
                    \While{$!v_{top}\texttt{.outgoing\_adjacent().isEmpty()}$}
                        \State $z:=v_{top}\texttt{.outgoing\_adjacent()[}0\texttt{]}$
                        \State $v_{top}\texttt{.outgoing\_adjacent().remove(}0\texttt{)}$
                        \If{$\texttt{visited[}z\texttt{]}==0$}
                            \State $\texttt{visited[}z\texttt{]}=1$
                            \State $\texttt{S.push(}z\texttt{)}$
                            \State $\texttt{parents[}z\texttt{]}=v_{top}$
                            \State $\texttt{c} \ += 1$
                            \State $\texttt{t[}z\texttt{]}=\texttt{c}$
                            \State \textbf{break}
                        \EndIf
                    \EndWhile
                    \If{$v_{top}\texttt{ == S.top()}$}
                        \State \texttt{S.pop()}
                        \State $\texttt{T[}v_{top}\texttt{]}=\texttt{c}$
                    \EndIf
                \EndWhile
                \algstore{bkbreak}
        \end{algorithmic}
    \end{algorithm}

    \begin{nocaptionalg}
        \begin{algorithmic}[1]
                \algrestore{bkbreak}
                \State $\texttt{forward} := \texttt{[]}$
                \State $\texttt{backward} := \texttt{[]}$
                \State $\texttt{cross} := \texttt{[]}$

                \For{$x \in V(G)$}
                    \For{$u \in x\texttt{.incoming\_adjacent()}$}
                        \If{$\texttt{parents[}x\texttt{]}==u$}
                            \State \textbf{continue} \Comment{faceva parte dell'arborescenza di $v$}
                        \ElsIf{$\texttt{T[}u\texttt{]} < \texttt{t[}x\texttt{]} \ \texttt{or} \ \texttt{T[}x\texttt{]} < \texttt{t[}u\texttt{]}$}
                            \State $\texttt{cross.append(}(u, x)\texttt{)}$
                        \ElsIf{$\texttt{T[}u\texttt{]} \le \texttt{T[}x\texttt{]}$}
                            \State $\texttt{backward.append(}(u, x)\texttt{)}$
                        \Else
                            \State $\texttt{forward.append(}(u, x)\texttt{)}$
                        \EndIf
                    \EndFor
                \EndFor

                \State \textbf{return} \texttt{forward}, \texttt{backward}, \texttt{cross}
            \EndFunction
        \end{algorithmic}
    \end{nocaptionalg}

    \begin{proof}[Correttezza dell'algoritmo.]
        Si noti che, dalla riga 2 alla riga 29, l'algoritmo seguente non presenta alcuna differenza dall'\cref{findReachableNodes2}, se non per alcuni accorgimenti al fine di collezionare i tempi di visita e di chiusura dei vari vertici:

        \begin{itemize}
            \item alle righe 5, 6 e 7 viene istanziato un contatore \texttt{c}, che servirà per determinare i tempi di visita e di chiusura dei vari vertici, i quali verranno salvati all'interno dei due array \texttt{t} e \texttt{T} appena definiti;
            \item alla riga 8 viene aggiornato il tempo di visita di $v$ di partenza, pari a \texttt{c}, poiché quest'ultimo è stato definito partendo da 1;
            \item alla riga 9 e 10 viene creato un array \texttt{parents}, che servirà per la seconda parte dell'algoritmo, ed in esso viene salvato il padre di $v$, pari a $v$ stesso, poiché così facendo sarà possibile identificare la radice successivamente;
            \item alle righe 13, 14 e 15 è necessario controllare esclusivamente gli archi uscenti, poiché l'algoritmo deve progredire visitando i vertici adiacenti uscenti, e gli entranti sono quelli dai quali si proviene;
            \item alla riga 19, è necessario salvare il nodo padre di $z$, che sarà naturalmente $v_{top}$;
            \item alle righe 20 e 21 viene incrementato il contatore dei tempi \texttt{c}, e viene posto il suo valore, appena incrementato, come tempo di visita di $z$;
            \item alla riga 27 viene aggiornato il tempo di chiusura di $v_{top}$, che sarà pari al valore di \texttt{c} corrente, poiché il contatore deve aumentare esclusivamente quando si incontrano vertici non ancora visitati.
        \end{itemize}

        Alle righe 30, 31 e 32 vengono definite 3 liste, che conterranno tutti gli archi del grafo, non facenti parte della visita di $G$ in DFS appena realizzata dall'algoritmo, categorizzati opportunamente grazie ai cicli \texttt{for} delle righe 33 e 34. In particolare, per ogni vertice $x \in V(G)$ del grafo, vengono esaminati tutti i vertici $u$, che siano \tbf{adiacenti entranti} ad $x$, controllando di fatto l'arco $(u, x)$:

        \begin{itemize}
            \item se il nodo padre di $x$ è proprio $u$ (riga 35), allora necessariamente l'arco è stato visitato dall'algoritmo nella prima fase, poiché si trova all'interno di \texttt{parents} stesso, e dunque fa parte della visita del grafo, ed è possibile ignorarlo;
            \item se il tempo di chiusura di uno dei due, tra $x$ e $u$, è minore del tempo di visita dell'altro (riga 37), allora uno dei due è stato rimosso dallo stack, prima che l'altro potesse essere visitato, e l'unico caso in cui tale condizione può verificarsi, è se l'arco $(u, x)$ è un cross edge;
            \item se il tempo di chiusura di $u$, il quale è entrante in $x$, è inferiore (o uguale) al tempo di chiusura di quest'ultimo (riga 39), allora $x$ è stato rimosso dallo stack dopo $u$, nonostante $u$ fosse adiacente entrante ad $x$, e dunque necessariamente l'arco $(u, x)$ è un backward edge;
            \item infine, l'ultimo caso possibile restante (riga 41) è un forward edge.
        \end{itemize}

        L'algoritmo conclude restituendo le liste contenenti i vari archi del grafo, non facenti parte dell'arborescenza di visita di $G$, radicata in $v$, categorizzati sfruttando i loro intervalli.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Si noti che il ciclo \texttt{while} della riga $11$, e termina alla riga $29$, ha lo stesso costo computazionale dell'\cref{findReachableNodes2}, e dunque il suo costo è pari a $O(n + m)$.

        Si noti inoltre che, il ciclo \texttt{for} della riga $33$, effettua un'iterazione per ogni singolo vertice del grafo, ma all'interno di esso è presente un ulteriore ciclo \texttt{for}, alla riga $34$, che itera sui rispettivi vertici adiacenti entranti; allora, il costo di questi due cicli equivale al solo spazio di rappresentazione delle liste di adiacenza di $G$, ovvero $O(n + m)$.

        Allora, il costo computazionale è pari a $O(n + m) + O(n + m) = O(n + m)$.
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Dato un'array di padri \texttt{parents}, che rappresenta un'arborescenza di visita in DFS di un grafo diretto, e un arco $(x, y)$ del grafo, l'algoritmo restituisce il tipo di arco.\\
            \textbf{Input}: \texttt{parents} array di padri di un'arborescenza di visita in DFS di un grafo diretto; $(x, y)$ un arco del grafo.\\
            \textbf{Output}: la categoria di $(x, y)$.
        }

        \begin{algorithmic}[1]
            \Function{categorizeEdge}{\texttt{parents}, $(x, y)$}
                \If{$\texttt{parents[}y\texttt{]}==x$}
                    \State \textbf{return} \texttt{NodeType::Arborescence}
                \EndIf
                \State $z := y$
                \While{$\texttt{parents[}z\texttt{]} \neq z$}
                    \State $z = \texttt{parents[}z\texttt{]}$
                    \If{$z == x$}
                        \State \textbf{return} \texttt{NodeType::Forward}
                    \EndIf
                \EndWhile
                \State $z := x$
                \While{$\texttt{parents[}z\texttt{]} \neq z$}
                    \State $z = \texttt{parents[}z\texttt{]}$
                    \If{$z == y$}
                        \State \textbf{return} \texttt{NodeType::Backward}
                    \EndIf
                \EndWhile
                \State \textbf{return} \texttt{NodeType::Cross}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        Sia \texttt{NodeType} un \texttt{enum}, che può assumere le 4 seguenti varianti:

        \begin{itemize}
            \item \texttt{Arborescence}
            \item \texttt{Forward}
            \item \texttt{Backward}
            \item \texttt{Cross}
        \end{itemize}

        L'algoritmo inizia controllando, alla riga 2, se il vertice $x$ è proprio il nodo padre di $y$: in tal caso, vorrebbe dire che il vertice appartiene a \texttt{parents}, e dunque appartiene all'arborescenza di visita del grafo; allora, viene restituita la variante \texttt{NodeType::Arborescence}, alla riga 3.

        Successivamente, alla riga 5 viene definito $z$, posto inizialmente ad $y$, ed all'interno del ciclo \texttt{while} della riga seguente, fintanto che non viene trovata la radice (condizione di uscita, alla riga 6), viene risalita la catena di nodi padri, e se durante il percorso viene trovato proprio $x$, allora vuol dire che $x$ si trova prima di $y$ all'interno della visita in DFS del grafo, e poiché l'arco $(x, y)$ è diretto incidente su $x$, allora questo deve essere un forward edge, ed è dunque sufficiente restituire \texttt{NodeType::Forward}, alla riga 9.

        Simmetricamente, viene eseguito lo stesso ciclo \texttt{while}, ma partendo da $x$, per cercare $y$, e se quest'ultimo viene trovato, allora vuol dire che $y$ viene prima di $x$ nell'albero di visita, ma poiché l'arco $(x, y)$ è diretto incidente su $y$, l'unica possibilità è che l'arco sia un backward edge, e dunque l'algoritmo restituisce \texttt{NodeType::Backward} in tal caso, alla riga 16.

        Infine, se l'arco $(x, y)$ in input non rispetta nessuna delle condizioni precedenti, segue che questo è un cross edge, e viene dunque restituito \texttt{NodeType::Cross} alla riga 18.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        I cicli \texttt{while} delle righe 6 e 12, poiché identici, hanno lo stesso costo, ovvero $O(n)$, in quanto semplicemente risalgono ogni nodo della catena di padri in \texttt{parents}, il quale ha esattamente $n$ nodi, e dunque nel caso peggiore verranno visitati tutti i nodi fino alla radice.

        Allora, il costo dell'algoritmo è pari a $O(n) + O(n) = O(n)$.
    \end{proof}

    \subsection{Trovare un ordinamento topologico}

    \begin{framedthm}[label={Cicli diretto}]{Presenza di cicli}
        Sia $G$ un grafo diretto connesso; allora $G$ ha un ciclo se e solo se in esso esiste un backward edge in almeno un'arborescenza.
    \end{framedthm}

    \begin{proof}
        \hspace{0.7cm}
        \begin{itemize}
            \item[] \tit{Prima implicazione.} Omessa.
            \item[] \tit{Seconda implicazione.} Sia $G$ un grafo diretto fortemente connesso, e sia $\hat v \in V(G)$ tale che la sua arborescenza $A_{\hat v}$ contenga un backward edge $(u, v) \in E(G) - E(A_{\hat v})$. Allora, poiché $u, v \in V(A_{\hat v})$, e $(u, v)$ è un backward edge, è sufficiente considerare il cammino tale che $u \rightarrow v$, che esiste poiché $A_{\hat v}$ è un arborescenza, e dunque $\{u \rightarrow v\} \cup (u, v)$ è un ciclo di $G$. Si noti che il fatto che $G$ sia fortemente connesso garantisce di poter considerare un $\hat v \in V(G)$ qualsiasi.
        \end{itemize}
    \end{proof}

    \begin{framedcor}{Relazioni tra tempi}
        Sia $G$ un grafo diretto aciclico connesso, sia $\hat v \in V(G)$ un suo vertice, sia $A_{\hat v}$ la relativa arborescenza, e sia $(u, v) \in E(G)$ un arco; allora $t(v) \le T(u)$.
    \end{framedcor}

    \begin{proof}
        Si noti che ogni arco $(u, v) \in A_{\hat v}$ è un forward edge per costruzione della visita DFS; allora si consideri il caso in cui $(u, v) \in E(G) - A_{\hat v}$. Allora per il teorema precedente, $(u, v)$ non è un backward edge, e dunque per il \cref{Intervalli diretto} si può verificare solo una delle seguenti:
        \begin{itemize}
            \item $\mathrm{Int}(v) \subseteq \mathrm{Int}(u) \implies t(u) < t(v) \le T(v) < T(u)$, e in particolare $t(v) \le T(u)$
            \item $\mathrm{Int}(u) \cap \mathrm{Int}(v) = \varnothing \implies t(v) < T(v) < t(u) < T(u)$, e in particolare $t(v) \le T(u)$.
        \end{itemize}
    \end{proof}

    \begin{framedcor}{Relazioni tra tempi}
        Sia $G$ un grafo diretto aciclico connesso, sia $\hat v \in V(G)$ un suo vertice, sia $A_{\hat v}$ la relativa arborescenza, e sia $(u, v) \in E(G)$ un arco; allora $T(v) \le T(u)$.
    \end{framedcor}

    \begin{proof}
        Per il corollario precedente, per ogni arco di un grafo indiretto aciclico connesso $t(v) \le T(u)$, allora:
        \begin{itemize}
            \item $t(v) < t(u)$: se $T(u) \le T(v)$, allora $(u, v)$ sarebbe un backward edge, che non è possibile avere per il \cref{Cicli diretto}; allora necessariamente $T(v) < T(u)$; ma se $t(u) < T(v)$ allora si avrebbe intersezione non vuota tra gli intervalli, impossibile per il \cref{Intervalli diretto}; allora necessariamente $t(v) < T(v) < t(u) < T(u)$, e dunque $(u, v)$ è un cross edge
            \item $t(u) < t(v)$: se $T(u) < T(v)$, allora gli intervalli avrebbero intersezione non vuota, e ciò non si può verificare per il \cref{Intervalli diretto}; allora necessariamente $t(u) < t(v) \le T(v) \le T(u)$, e dunque $(u, v)$ è un forward edge.
        \end{itemize}

        In particolare, si ha che $T(v) \le T(u)$ in entrambe i casi.
    \end{proof}

    \begin{framedthm}{Ordinamento topologico attraverso i tempi}
        Sia $G$ un grafo diretto aciclico connesso, sia $\hat v \in V(G)$ un suo vertice, e sia $A_{\hat v}$ la sua arborescenza; allora, ordinando i vertici attraverso i loro tempi di chiusura $T$ in ordine decrescente, si ottiene un ordinamento topologico del grafo.
    \end{framedthm}

    \begin{proof}
        Per definizione, un'ordinamento è detto topologico se ogni vertice è posto prima dei suoi archi uscenti; inoltre, per il corollario precedente, per ogni $(u, v) \in E(G)$ si ha $T(v) \le T(u)$, e dunque se si ordinassero i vertici di $G$ utilizzando i tempi di chiusura $T$, in ordine crescente, come criterio, allora $v$ verrebbe prima di $u$, e $(u, v)$ è un arco diretto in cui $u$ è incidente su $v$; allora, segue che ordinando i vertici in ordine decrescente di $T$, si ha un ordinamento topologico di $G$.
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo diretto aciclico connesso $G$, rappresentato attraverso liste di adiacenza in cui vengono salvati gli archi adiacenti uscenti, l'algoritmo restituisce un ordinamento topologico di $G$.\\
            \textbf{Input}: $G$ grafo diretto, rappresentato attraverso liste di adiacenza.\\
            \textbf{Output}: un ordinamento topologico di $G$.
        }

        \begin{algorithmic}[1]
            \Function{DFS}{$G$, $v$, \texttt{visited}, \texttt{order}}
                \State $\texttt{visited.add(}v\texttt{)}$
                \For{$u \in V(G) : (v, u) \in E(G)$} \Comment{$u$ deve essere uscente da $v$}
                    \If{$u \notin \texttt{visited}$}
                        \State $\texttt{DFS(}G, u, \texttt{visited},\texttt{order)}$
                    \EndIf
                \EndFor
                \State $\texttt{order.append(}v\texttt{)}$ \Comment{l'ordinamento risulterà invertito}
            \EndFunction
            \\
            \Function{findTopologicalSortingDFS}{$G$}
                \State $\texttt{order} := \texttt{[]}$
                \State $\texttt{visited} := \verb|{|\verb|}|$
                \For{$v \in V(G)$}
                    \If{$v \notin \texttt{visited}$}
                        \State $\texttt{DFS(}G, v, \texttt{visited},\texttt{order)}$
                    \EndIf
                \EndFor
                \State $\texttt{order.reverse()}$ \Comment{viene invertita la lista}
                \State \textbf{return} \texttt{order}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        Si noti che l'algoritmo non salva i tempi di chiusura dei vari vertici per ordinarli, ma non è necessario grazie alla ricorsione: infatti, inserendo il vertice alla riga $8$, dunque dopo il loop \texttt{for}, l'inserimento avviene in post-order rispetto alla visita del grafo, e dunque è equivalente a rispettare l'ordinamento crescente dei tempi di chiusura di ogni nodo. Dunque, è sufficiente invertire la lista ottenuta, alla riga $19$, per ottenere un ordinamento topologico.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Si noti che l'algoritmo ha costo $O(n + m)$, poiché attua una sola visita in DFS del grafo, rappresentato attraverso liste di adiacenza, ricorsivamente.
    \end{proof}

    \subsection{Trovare un pozzo universale}

    \begin{frameddefn}{Pozzo universale}
        Sia $G$ un grafo diretto; $v \in V(G)$ è detto \tbf{pozzo universale} se ha $n - 1$ archi entranti, e nessun arco uscente.
    \end{frameddefn}

    \begin{example}[Pozzo universale]
        Ad esempio, il seguente grafo diretto presenta un pozzo universale in $3$:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [above right of=3] {4};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (2)
                    (2) edge (3)
                    (4) edge (3)
                    (4) edge (1)
                    (1) edge (3)
                    ;
            \end{tikzpicture}
            \caption{Un grafo con pozzo universale in $3$.}
        \end{figure}
    \end{example}

    \begin{framedthm}{Unicità del pozzo universale}
        Sia $G$ un grafo diretto, e $p \in V(G)$ un suo pozzo universale; allora, tale pozzo universale $p$ è unico in $G$.
    \end{framedthm}

    \begin{proof}
        Per assurdo, sia $p' \in V(G)$ un secondo pozzo universale in $G$; allora, per definizione, sia $p$ che $p'$ avrebbero $n - 1$ archi entranti, e nessun arco uscente, ma questo non è possibile poiché l'unico modo per avere entrambe le condizioni verificate sarebbe attraverso un arco bidirezionale tra $p$ e $p' \ \lightning$.
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo diretto $G$, rappresentato attraverso matrice di adiacenza, l'algoritmo restituisce, se presente, il pozzo universale di $G$.\\
            \textbf{Input}: $G$ grafo diretto, rappresentato attraverso matrice di adiacenza.\\
            \textbf{Output}: il pozzo universale di $G$, se presente.
        }

        \begin{algorithmic}[1]
            \Function{findUniversalSink}{$M_G$}
                \State $p \in V(G)$ \Comment{un vertice qualsiasi, possibile pozzo universale}
                \For{$v \in V(G)$}
                    \If{$M_G[p, v] = = 1$}
                        \State $p = v$
                    \EndIf
                \EndFor
                \For{$v \in V(G) - \{p\}$}
                    \If{$M_G[p, v] == 1$}
                        \State \textbf{return} \texttt{None}
                    \EndIf
                    \If{$M_G[v, p] == 0$}
                        \State \textbf{return} \texttt{None}
                    \EndIf
                \EndFor
                \State \textbf{return} $p$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        Per definizione stessa di pozzo universale, indipendentemente dalla scelta del vertice di partenza del grafo in input, percorrendo una qualsiasi sequenza di archi, inevitabilmente, si deve giungere al pozzo universale, poiché esso ha esattamente $n - 1$ archi entranti. Allora, alla riga $2$ viene arbitrariamente scelto un possibile pozzo universale, e il ciclo \texttt{for} della riga $3$ percorre la catena di archi possibili, e ha costo $O(n)$. Si noti però che il vertice a cui si è giunti potrebbe non essere un pozzo universale, ad esempio si consideri questo grafo:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [below right of=1] {4};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (4)
                    (2) edge (1)
                    (2) edge (4)
                    (3) edge (2)
                    ;
            \end{tikzpicture}
            \caption{Un grafo che contiene un possibile pozzo universale.}
        \end{figure}

        si noti che ndipendentemente dalla scelta iniziale di $p \in V(G)$, al termine del ciclo \texttt{for} si avrà $p = 4$, pur non essendo $4$ un pozzo universale, poiché $(3, 4) \notin E(G)$. Risulta dunque necessario accertarsi che $p$ sia realmente un pozzo universale, andando dunque a controllare, all'interno del ciclo \texttt{for} della riga $8$, se $p$ non ha archi uscenti, ed ogni altro arco è incidente su $p$. Tali controlli vengono effettuati rispettivamente alla riga $9$, in cui il ciclo termina restituendo \texttt{None} se esiste un vertice uscente da $p$, e alla riga $12$, in cui il ciclo termina analogamente se esiste un vertice non adiacente entrante a $p$.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Poiché i due cicli \texttt{for} percorrono ogni vertice di $G$, una ed una sola volta, il costo dell'algoritmo è pari a $O(n) + O(n) = O(n)$.
    \end{proof}

    \subsection{Trovare i ponti}

    \begin{frameddefn}{Ponte}
        Sia $G$ un grafo, e sia $(u, v) \in E(G)$ un suo arco; allora, $(u, v)$ è detto \tbf{ponte} se e solo se non è contenuto in nessun ciclo di $G$.
    \end{frameddefn}

    \begin{example}[Ponte]
        Un esempio di grafo che presenta un ponte è il seguente:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [below right of=1] {4};
                \node[main node] (5) [right of=4] {5};
                \node[main node] (6) [above right of=5] {6};
                \node[main node] (7) [below right of=5] {7};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (2)
                    (2) edge (3)
                    (3) edge (4)
                    (4) edge (1)
                    (4) edge (5)
                    (5) edge (6)
                    (6) edge (7)
                    (7) edge (5)
                    ;
            \end{tikzpicture}
            \caption{Un grafo con un ponte.}
        \end{figure}

        Infatti, $(4, 5)$ è un ponte poiché gli unici due cicli di $G$ sono $$\{1, (1, 2), 2, (2, 3), 3, (3, 4), 4, (4, 1), 1\}$$ $$\{5, (5, 6), 6, (6, 7), 7, (7, 5), 5\}$$
    \end{example}

    \begin{framedthm}[label={teorema dei ponti}]{Presenza di ponti}
        Sia $G$ un grafo indiretto connesso, $r \in V(G)$, e sia $T_r$ l'albero di visita in DFS di $r$ in $G$; sia $x \in V(T_r)$, e sia $T_x \subseteq T_r$ il suo sottoalbero in $T_r$; sia inoltre $y \in V(T_x)$ tale che $(x, y) \in E(G)$, e sia $T_y \subseteq T_x$ il suo sottoalbero in $T_x$; allora, esiste un arco $(u, v) \in E(G)$, tale che $u \in V(T_y)$ e $v \in V(T - T_x)$, se e solo se esiste un ciclo in $G$ contenente $(x, y)$.
    \end{framedthm}

    \begin{proof}
        \hspace{0.7cm}
        \begin{itemize}
            \item[] \tit{Prima implicazione.} Si noti che, poiché $G$ è indiretto e connesso, per qualsiasi $r \in V(G)$, $V(T_r) = V(G)$, dunque la visita in DFS è sempre in grado di raggiungere ogni nodo del grafo $G$.
                Per definizione $T_y$ è connesso ed aciclico, e dunque esiste uno ed un solo cammino $y \rightarrow u$; inoltre, per ragionamento analogo, esiste uno ed un solo cammino $v \rightarrow x$. Allora, se esiste un arco $(u, v) \in E(G)$, viene creato un ciclo in $G$ della forma $v \rightarrow x \rightarrow y \rightarrow u \rightarrow v$, poiché esiste l'arco $(x, y) \in E(G)$ in ipotesi, che sarà dunque necessariamente contenuto in tale ciclo.
            \item[] \tit{Seconda implicazione.}  Se $(x, y)$ è incluso in almeno un ciclo di $G$, deve necessariamente esistere un cammino $x \rightarrow y$ non passante per $(x, y)$, il quale deve contenere un arco $(u, v)$, in cui $u \in V(T_y)$, e $v \in V(T - T_x)$, poiché $(x, y) \in E(G)$ è un arco.

        \end{itemize}
    \end{proof}

    \begin{framedlem}{Alberi con ponti}
        Sia $G$ un grafo indiretto, e $(u, v) \in E(G)$ un suo ponte; allora, per ogni possibile $T_u$, albero di visita di $G$ in DFS, radicato in $u$, si ha che $(u, v) \in E(T_u)$.
    \end{framedlem}

    \begin{proof}
        Poiché $(u, v)$ è un ponte, per definizione non appartiene a nessun ciclo di $G$, allora preso un qualsiasi $T_u$, albero di visita di $G$ in DFS, radicato in $u$, l'unico modo per raggiungere $v$ è attraverso $(u, v)$ stesso, in quanto $G$ è indiretto, e dunque necessariamente $(u, v) \in E(T_u)$
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo indiretto $G$, rappresentato attraverso liste di adiacenza, l'algoritmo restituisce i ponti di $G$.\\
            \textbf{Input}: $G$ grafo indiretto, rappresentato attraverso liste di adiacenza.\\
            \textbf{Output}: i ponti di $G$.
        }

        \begin{algorithmic}[1]
            \Function{DFS}{$G$, $y$, \texttt{c}, \texttt{back}, \texttt{t}, \texttt{parents}}
                \State $\texttt{c.increment()}$
                \State $\texttt{t[}y\texttt{]}= \texttt{c}$
                \State $\texttt{back[}y\texttt{]}= \texttt{t[}y\texttt{]}$
                \For{$z \in V(G) : z \sim y$}
                    \If{$\texttt{t[}z\texttt{]}==0$} \Comment{$z$ non deve essere già stato visitato}
                        \State $\texttt{parents[}z\texttt{]}=y$
                        \State $\texttt{DFS(}G, z, \texttt{c}, \texttt{back}, \texttt{t}, \texttt{parents)}$
                        \If{$\texttt{back[}z\texttt{]} < \texttt{back[}y\texttt{]}$}
                            \State $\texttt{back[}y\texttt{]} = \texttt{back[}z\texttt{]}$
                        \EndIf
                    \ElsIf{$\left \{ \begin{array}{l} z \neq \texttt{parents[}y\texttt{]} \\ \texttt{t[}z\texttt{]} < \texttt{back[}y\texttt{]} \end{array}\right.$}
                        \State $\texttt{back[}y\texttt{]} = \texttt{back[}z\texttt{]}$
                    \EndIf
                \EndFor
            \EndFunction
            \\
            \Function{findBridges}{$G$}
                \State $v \in V(G)$ \Comment{un vertice qualsiasi di $G$}
                \State $\texttt{t} := \texttt{[}0\texttt{]} * n$
                \State $\texttt{parents} := \texttt{[}0\texttt{]} * n$
                \State $\texttt{parents[}v\texttt{]}=v$
                \State $\texttt{back} := \texttt{[}0\texttt{]} * n$
                \State $\texttt{Counter c} := 0$ \Comment{è un oggetto}
                \State $\texttt{DFS(}G, v, \texttt{c}, \texttt{back}, \texttt{t}, \texttt{parents)}$
                \State $\texttt{bridges} := \verb|{|\verb|}|$
                \For{$u \in V(G)$}
                    \If{$\left \{ \begin{array}{l}\texttt{back[}u\texttt{]}= \texttt{t[}u\texttt{]} \\ u \neq \texttt{parents[}u\texttt{]} \end{array} \right.$}
                        \State $\texttt{bridges.add(}(\texttt{parents[}u\texttt{]}, u)\texttt{)}$
                    \EndIf
                \EndFor
                \State \textbf{return} \texttt{bridges}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia scegliendo un vertice $v$ casuale del grafo $G$ in input; successivamente, vengono definiti gli array \texttt{t}, all'interno del quale verranno salvati i tempi di visita dei vertici, \texttt{parents}, utilizzato per salvare i nodi padri di ogni vertice visitato, e \texttt{back}.

        Sia $T$ l'albero di visita in DFS della visita correntemente in esecuzione dall'algoritmo; allora, per un certo vertice $y \in V(T)$, all'interno di $\texttt{back[}y\texttt{]}$, verrà salvato il tempo di visita del vertice $v \in V(T - T_y)$ più lontano, raggiungibile da un discendente $z$ di $y$ (incluso $y$ stesso), attraverso un cammino passante per un arco $(z, v) \notin E(T)$. L'array \texttt{back} verrà sfruttato congiuntamente al \cref{teorema dei ponti}, in quanto l'arco $(\texttt{parents[}y\texttt{]}, y)$ è un ponte, se e solo se non esiste un tale arco $(z, v)$.

        Infine, prima di iniziare una visita in DFS ricorsiva dell'albero, viene istanziato un oggetto contatore \texttt{c}, utilizzato per salvare i tempi di visita in \texttt{t}.

        L'esecuzione procede all'interno della funzione ricorsiva \texttt{DFS}, che ha lo scopo di popolare i valori degli array precedentemente definiti. La funzione inizia aggiornando il contatore, e inserendo il relativo valore del tempo di visita del vertice corrente; inoltre, di quest'ultimo viene inizializzato il valore corrisopndente in \texttt{back}, pari al suo stesso tempo di visita. Tale valore servirà come \tbf{sentinel value}, e verrà utilizzato successivamente dall'algoritmo per stabilire quali archi del grafo sono ponti.

        Successivamente, viene istanziato un ciclo \texttt{for}, alla riga $5$, in cui per ogni vertice $z$, adiacente al vertice $y$ corrente:

        \begin{itemize}
            \item se tale vertice $z$ ha tempo di visita pari a $0$, e dunque non è ancora stato visitato, ne viene inizialmente aggiornato il valore del padre, alla riga $7$, che sarà proprio $y$, e viene poi effettuata una chiamata ricorsiva, radicata in $z$; al termine di quest'ultima, viene aggiornato il valore di $\texttt{back[}y\texttt{]} = \min(\texttt{back[}y\texttt{]}, \texttt{back[}z\texttt{]})$, e dunque sarà sempre il minore tra i suoi discendenti;
            \item se invece tale vertice $z$ è già stato visitato, va aggiornato il valore di $\texttt{back[}y\texttt{]}$ con $\min(\texttt{t[}z\texttt{]}, \texttt{back[}y\texttt{]})$, esclusivamente se $z$ non è il padre di $y$.
        \end{itemize}

        Al termine della visita in DFS ricorsiva, l'algoritmo conclude cercando tutti i ponti del grafo, a partire dalle informazioni all'interno di \texttt{back}: infatti, alla riga $27$, per ogni vertice $u \in V(G)$ del grafo, viene inserito all'interno dell'insieme \texttt{bridges} l'arco tra $u$ e il suo vertice padre, se e solo se $u \neq \texttt{parents[}u\texttt{]}$ (ovvero, $u$ non è la radice), e $\texttt{back[}u\texttt{]} = \texttt{t[}u\texttt{]}$, dunque il valore di $\texttt{back[}u\texttt{]}$ è rimasto invariato dalla riga $4$. L'algoritmo termina restituendo \texttt{bridges}, che conterrà l'insieme dei ponti di $G$.

        Ad esempio, si consideri il seguente grafo:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [above right of=1] {2};
                \node[main node] (3) [right of=1] {3};
                \node[main node] (4) [below right of=1] {4};
                \node[main node] (5) [left of=1] {5};
                \node[main node] (6) [left of=5] {6};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (2)
                    (1) edge (3)
                    (1) edge (4)
                    (1) edge (5)
                    (4) edge[bend left] (6)
                    (5) edge (6)
                    ;
            \end{tikzpicture}
            \caption{Un grafo indiretto.}
        \end{figure}

        inoltre, sia $$T_6 = (\{6, 5, 1, 2, 3, 4\}, \{(6, 5), (5, 1), (1, 2), (1, 3), (1, 4)\})$$ l'albero di visita in DFS del grafo, partendo dal vertice $6$; allora, si avranno $$\texttt{t} = \texttt{[}3, 4, 5, 6, 2, 1\texttt{]}$$ $$\texttt{parents} = \texttt{[}5, 1, 1, 1, 6, 6\texttt{]}$$ $$\texttt{back} = \texttt{[}1, 4, 5, 1, 1, 1\texttt{]}$$

        allora, al termine del ciclo \texttt{for} della riga $27$, si avrà $$\texttt{bridges} = \verb|{| (\texttt{parents[}2\texttt{]}, 2), (\texttt{parents[}3\texttt{]}, 3)\verb|}|$$ ovvero $$\texttt{bridges} = \verb|{| (1, 2), (1, 3)\verb|}|$$ poiché $\texttt{back[}2\texttt{]} = \texttt{t[}2\texttt{]} = 4$ e $\texttt{back[}3\texttt{]} = \texttt{t[}3\texttt{]} = 5$; infine, si noti che $\texttt{back[}6\texttt{]} = \texttt{back[}6\texttt{]} = 1$, ma $6 \notin \texttt{bridges}$ poiché $\texttt{parents[}6\texttt{]} = 6$, ed infatti $T_6$ è proprio radicato in $6$.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Il costo dell'algoritmo è $O(n + m)$, poiché è costituito semplicemente da una visita in DFS ricorsiva del grafo in input, e da un ciclo \texttt{for}, alla riga $27$, su tutti i nodi del grafo, e dunque si ha $O(n + m) + O(n) = O(n + m)$.
    \end{proof}

    \subsection{Trovare le componenti}

    \begin{frameddefn}{Componenti indirette}
        Sia $G$ un grafo indiretto, non necessariamente connesso; si definisce \tbf{componente} un sottografo di $G$, connesso, non ulteriormente estendibile. Più rigorosamente, sia $H \subseteq G$ un sottografo di $G$; esso è una componente di $G$, se e solo se non esiste $H' \subseteq G$, sottografo connesso di $G$, tale che $H \subsetneq H'$. Le componenti sono anche definite come sottografi \tbf{massimalmente connessi}.

        In simboli, dato un vertice $v \in V(G)$, $\mathrm{comp}(v)$ è la componente contenente $v$.
    \end{frameddefn}

    \begin{example}[Componenti indirette]
        Ad esempio, si consideri questo grafo:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [below right of=1] {4};
                \node[main node] (5) [right of=4] {5};
                \node[main node] (6) [above right of=5] {6};
                \node[main node] (7) [below right of=5] {7};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (2)
                    (2) edge (3)
                    (3) edge (4)
                    (4) edge (1)
                    (5) edge (6)
                    (6) edge (7)
                    (7) edge (5)
                    ;
            \end{tikzpicture}
            \caption{Un grafo indiretto.}
        \end{figure}

        esso, presenta $2$ componenti: $$H_1 := (\{1, 2, 3, 4\}, \{(1, 2), (2, 3), (3, 4), (4, 1)\})$$ $$H_2 := (\{5, 6, 7\}, \{(5, 6), (6, 7), (7, 5)\})$$
    \end{example}

    \begin{frameddefn}{Componenti dirette}
        Sia $G$ un grafo diretto, non necessariamente connesso; allora, si definisce \tbf{componente} un sottografo di $G$, fortemente e massimalmente connesso.
    \end{frameddefn}

    \begin{example}[Componenti dirette]
        Ad esempio, si consideri questo grafo:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [below right of=1] {4};
                \node[main node] (5) [above right of=4] {5};
                \node[main node] (6) [below right of=5] {6};
                \node[main node] (7) [above right of=6] {7};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (3)
                    (1) edge (5)
                    (2) edge (1)
                    (3) edge (2)
                    (3) edge (4)
                    (4) edge (5)
                    (4) edge (6)
                    (5) edge (7)
                    (6) edge (3)
                    (6) edge (5)
                    (6) edge (7)
                    ;
            \end{tikzpicture}
            \caption{Un grafo diretto.}
        \end{figure}

        esso, presenta $3$ componenti: $$H_1 := (\{1, 2, 3, 4, 6\}, \{(1, 3), (3, 4), (4, 6), (6, 3), (3, 2), (2, 1)\})$$ $$H_2 := (\{5\}, \varnothing)$$ $$H_3 := (\{7\}, \varnothing)$$
    \end{example}

    \begin{framedlem}[label={disjoint comps}]{Digiunzione delle componenti}
        Sia $G$ un grafo; allora, le sue componenti sono disgiunte.
    \end{framedlem}

    \begin{proof}
        Per assurdo, sia $G$ un grafo, diretto o indiretto, e $H_1, H_2 \subseteq G$ due sue componenti tali che $H_1 \cap H_2 \neq \varnothing$; se tali componenti esistessero, allora esisterebbe almeno un vertice nella loro intersezione, raggiungibile da entrambe le componenti (nel caso di $G$ diretto, i cammini sarebbero in entrambe le direzioni, poiché tale vertice sarebbe parte sia di $H_1$ che di $H_2$, entrambe fortemente connesse per definizione); allora, $H_1$ e $H_2$ non sarebbero massimalmente connesse $\lightning$.
    \end{proof}

    \begin{frameddefn}{Contrazione}
        Sia $G$ un grafo, e $H \subseteq G$ un suo sottografo; si definisce \tbf{contrazione di $H$ in $G$}, l'operazione che rimuove vertici (ed archi) di $H$ da $G$, e al suo posto inserisce un vertice, generalmente denotato con $v_H$, che viene connesso con $G - H$ dagli archi che precedentemente connettevano $H$ con $G$. In simboli, il grafo $G$, contratto su $H$, verrà indicato con $\mathrm{contr}(G, H)$.
    \end{frameddefn}

    \begin{example}[Contrazione di un grafo diretto]
        Si consideri il seguente grafo diretto:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [right of=3] {4};
                \node[main node] (6) [right of=1] {6};
                \node[main node] (5) [below right of=6] {5};
                \node[main node] (7) [right of=5] {7};
                \node[main node] (8) [below right of=7] {8};
                \node[main node] (9) [above right of=7] {9};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (3)
                    (1) edge (6)
                    (2) edge (1)
                    (3) edge (2)
                    (3) edge (4)
                    (5) edge (4)
                    (4) edge (6)
                    (6) edge (5)
                    (5) edge (7)
                    (7) edge (9)
                    (8) edge (7)
                    (9) edge (8)
                    ;
            \end{tikzpicture}
        \end{figure}
            
        contraendo la componente $H_1 = (\{1, 2, 3\}, \{(1, 3), (3, 2), (2, 1)\})$ in $v_{H_1}$, si ottiene:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (6) {6};
                \node[main node] (1) [below left of=6] {$v_{H_1}$};
                \node[main node] (4) [below right of=1] {4};
                \node[main node] (5) [below right of=6] {5};
                \node[main node] (7) [right of=5] {7};
                \node[main node] (8) [below right of=7] {8};
                \node[main node] (9) [above right of=7] {9};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (6)
                    (1) edge (4)
                    (5) edge (4)
                    (4) edge (6)
                    (6) edge (5)
                    (5) edge (7)
                    (7) edge (9)
                    (8) edge (7)
                    (9) edge (8)
                    ;
            \end{tikzpicture}
        \end{figure}

        contraendo la componente $H_2 = (\{4, 5, 6\}, \{(4, 6), (6, 5), (5, 4)\})$ in $v_{H_2}$, si ottiene:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {$v_{H_1}$};
                \node[main node] (5) [right of=1] {$v_{H_2}$};
                \node[main node] (7) [right of=5] {7};
                \node[main node] (8) [below right of=7] {8};
                \node[main node] (9) [above right of=7] {9};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (5)
                    (5) edge (7)
                    (7) edge (9)
                    (8) edge (7)
                    (9) edge (8)
                    ;
            \end{tikzpicture}
        \end{figure}

        infine, contraendo la componente $H_3 = (\{7, 8, 9\}, \{(7, 9), (9, 8), (8, 7)\})$ in $v_{H_3}$, si ottiene il seguente grafo massimalmente contratto:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.8,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {$v_{H_1}$};
                \node[main node] (5) [right of=1] {$v_{H_2}$};
                \node[main node] (7) [right of=5] {$v_{H_3}$};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (5)
                    (5) edge (7)
                    ;
            \end{tikzpicture}
            \caption{Il grafo iniziale massimalmente contratto.}
        \end{figure}
    \end{example}

    \begin{framedthm}[label={contrazioni fortemente connesse}]{Contrazioni fortemente connesse}
        Sia $G$ un grafo diretto fortemente connesso, e $H \subseteq G$ un suo sottografo fortemente connesso; allora, $\mathrm{contr}(G, H)$ è ancora fortemente connesso.
    \end{framedthm}

    \begin{proof}
        Sia $v_H$ il vertice del grafo contratto $\mathrm{contr}(G, H)$, e $y \in V(G - H)$; poiché $G$ è fortemente connesso, devono necessariamente esistere due vertici $v', v'' \in V(H)$ tali che $y \rightarrow v'$ e $v'' \rightarrow y$; siano i vertici $v'$ e $v''$ per i quali tali cammini siano di minor lunghezza possibile. Allora, effettuando la contrazione di $H$, in $\mathrm{contr}(G, H)$ sarà necessario rimpiazzare gli archi che permettevano tali cammini, e dunque verranno inseriti due nuovi archi, uno entrante uno uscente, verso $v_H$. Allora, necessariamente $\mathrm{contr}(G, H)$ è ancora fortemente connesso.
    \end{proof}

    \begin{framedthm}[label={presenza di cicli fort conn}]{Presenza di cicli}
        Sia $G$ un grafo diretto fortemente connesso, non composto da un singolo vertice; allora, esso presenta almeno un ciclo.
    \end{framedthm}

    \begin{proof}
        Per definizione $\forall u, v \in V(G) \quad u \rightarrow v$ e $v \rightarrow u$; allora, presi due vertici $u, v \in V(G)$, in $G$ deve necessariamente esistere un ciclo della forma $u \rightarrow v \rightarrow u$.
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo diretto $G$, rappresentato attraverso liste di adiacenza, con liste di archi sia entranti che uscenti per ogni vertice, l'algoritmo restituisce le componenti di $G$.\\
            \textbf{Input}: $G$ grafo diretto, rappresentato attraverso liste di adiacenza, con liste di archi sia entranti che uscenti per ogni vertice.\\
            \textbf{Output}: le componenti di $G$.
        }

        \begin{algorithmic}[1]
            \Function{findComponents$_1$}{$G$}
                \State $C := \texttt{findCycle}(G)$
                \If{$C == \texttt{None}$}
                    \State \textbf{return} $\verb|{|\{v\} : v \in V(G)\verb|}|$ \Comment{le componenti sono i singoli vertici}
                \Else
                    \State $v_C, G = \mathrm{contr}(G, C)$ \Comment{$v_C$ è il vertice della contrazione}
                    \State $\verb|{|H_1, \ldots, H_k\verb|}| = \texttt{findComponents}(G)$
                    \State $\texttt{new\_components} := \verb|{|\verb|}|$ \Comment{conterrà le nuove componenti}
                    \For{$i \in [1, k]$}
                        \If{$v_C \notin H_i$}
                            \State $\texttt{new\_components.add(}H_i\texttt{)}$
                        \Else
                        \State $\texttt{new\_components.add(}(H_i - \{v_C\}) \cup V(C)\texttt{)}$
                        \EndIf
                    \EndFor
                \EndIf
                \State \textbf{return} \texttt{new\_components}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        Sia $G$ un grafo diretto, e $H \subseteq G$ una sua componente; allora, se $H$ non è composto da un singolo vertice, poiché è fortemente connesso per definizione, per il \cref{presenza di cicli fort conn}, in $H$ è presente un ciclo $C$; allora, contraendo tale ciclo, si otterrà un sottografo $\mathrm{contr}(H, C)$, ancora fortemente connesso, per il \cref{contrazioni fortemente connesse}. Allora, questo garantisce di poter trovare tutte le componenti di un grafo, contraendone ricorsivamente i cicli.

        L'algoritmo inizia cercando un ciclo $C$ all'interno del grafo $G$; se non ne viene trovato alcuno, viene raggiunto il caso base della ricorsione, e l'algoritmo restituisce un insieme di insiemi dei singoli vertici di $G$; si noti che tale caso si presenta esclusivamente quando il grafo è massimalmente contratto.

        Differentemente, se è presente un ciclo $C$ all'interno del grafo, l'algoritmo procede contraendo $C$ in $v_C$, e rimpiazzando $G$, alla riga $10$; successivamente, viene effettuata una chiamata ricorsiva sul grafo contratto, al termine della quale viene restituito l'insieme di componenti di $G$ corrente.

        Dopo aver contratto ricorsivamente ogni possibile ciclo, l'algoritmo istanzia un loop \texttt{for} alla riga $13$, all'interno del quale viene trovata la componente $H_i$ in cui $v_C$ faceva parte: le componenti che non contengono $v_C$ vengono inserite all'interno di \texttt{new\_components}, senza subire modifiche; al contrario, dall'unica componente $H_i$ che contiene $v_C$, viene rimosso $v_C$ stesso, e ad essa verrà aggiunto tutto il ciclo $C$ che era stato contratto, ripristinando dunque il grafo di partenza di ricorsione in ricorsione.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        La funzione \texttt{findCycle} della riga 2 ha costo $O(n + m)$, ed utilizza la funzione dell'\cref{findCycleDir}. Allora, poiché la riga 4 ha costo pari a $O(n)$, dovendo scorrere tutti i vertici dell'attuale $G$ (che nel caso peggiore, contiene ogni vertice del grafo di partenza), il caso base ha costo $O(n+ m)$.

        L'operazione di contrazione, alla riga 6, ha costo $O(n+m)$, in quanto contrarre un grafo, nel caso peggiore, implica contrarne ogni vertice, e dunque modificarne ogni arco e nodo presente; inoltre, il ciclo \texttt{for} della riga 9 esegue un loop sulle componenti trovate alla riga 7 dalla chiamata ricorsiva, ed il suo costo dipende dunque dal numero di componenti che nel caso peggiore possono essere trovate, ovvero $n - 1$, dunque $O(n)$, poiché un ciclo è costituito da minimo 2 nodi.

        Allora, il costo dell'algoritmo è $O(n \cdot (n + m))$, poiché nel caso peggiore si hanno $n - 1$ contrazioni (chiamate ricorsive), ovvero $O(n)$, ed in ogni chiamata ricorsiva si effettuano operazioni in $O(n + m)$.

        Si noti che questa stima è particolarmente approssimativa, e sarebbe necessario risolvere l'equazione di ricorrenza associata all'algoritmo per giustificarne completamente il costo dell'algoritmo, ma è omesso tale calcolo per semplicità.
    \end{proof}

    \subsection{Algoritmo di Tarjan}

    \begin{frameddefn}{C-radici}
        Sia $G$ un grafo diretto, sia $\hat v \in V(G)$ un suo vertice, e sia $A_{\hat v}$ l'arborescenza di $\hat v$ in $G$; $v \in A_{\hat v}$ è detto \tbf{c-radice} di $\mathrm{comp}(v)$ in $A_{\hat v}$, se e solo se è il primo vertice visitato in $\mathrm{comp}(v)$.
    \end{frameddefn}

    \begin{example}[C-radici]
        Si consideri il seguente $G$ grafo diretto:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [below right of=1] {4};
                \node[main node] (5) [below right of=4] {5};
                \node[main node] (7) [above right of=4] {7};
                \node[main node] (6) [below right of=7] {6};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (3)
                    (1) edge (4)
                    (2) edge (1)
                    (3) edge (2)
                    (3) edge (4)
                    (4) edge (7)
                    (5) edge (4)
                    (5) edge (6)
                    (7) edge (5)
                    (7) edge (6)
                    ;
            \end{tikzpicture}
            \caption{Un grafo diretto.}
        \end{figure}

        esso presenta $3$ componenti: $$H_1 = (\{1, 2, 3\}, \{(1, 3), (3, 2), (2, 1)\})$$ $$H_2 = (\{4, 5, 7\}, \{(4, 7), (7, 5), (5, 4)\})$$ $$H_3=(\{6\}, \varnothing)$$

        sia $\hat v = 2$, dunque la sua arborescenza $A_{\hat v}$ è la seguente:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (4) [below right of=1] {4};
                \node[main node] (5) [below right of=4] {5};
                \node[main node] (7) [above right of=4] {7};
                \node[main node] (6) [below right of=7] {6};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (3)
                    (1) edge (4)
                    (2) edge (1)
                    (4) edge (7)
                    (5) edge (6)
                    (7) edge (5)
                    ;
            \end{tikzpicture}
            \caption{Arborescenza di $2$.}
        \end{figure}

        allora, in $H_1$ il primo vertice visitato da $A_{\hat v}$ è $2$, in $H_2$ è $4$, e in $H_3$ è $6$; allora le c-radici di $A_{\hat v}$ sono $\{2, 4, 6\}$.
    \end{example}

    \begin{framedthm}[label={cradiciteorema}]{C-radici}
        Sia $G$ un grafo diretto, sia $\hat v \in V(G)$ un suo vertice, e sia $A_{\hat v}$ l'arborescenza di $\hat v$ in $G$; sia $u$ la c-radice di $\mathrm{comp}(u)$ in $A_{\hat v}$; allora si verificano le seguenti proposizioni:

        \begin{enumerate}[label=\roman*), font=\itshape]
            \item sia $A_u \subseteq A_{\hat v}$ è l'arborescenza radicata in $u$; allora $$V(\mathrm{comp}(u)) \subseteq V(A_u)$$
            \item siano $u_1, \ldots, u_k$ le c-radici di $\mathrm{comp}(u_1), \ldots, \mathrm{comp}(u_k)$ in $A_u \subseteq A_{\hat v}$; allora $$V(\mathrm{comp}(u)) \cup \displaystyle \bigcup_{i = 1}^k {V(\mathrm{comp}(u_i))} = V(A_u)$$
        \end{enumerate}
    \end{framedthm}

    \begin{proof}
        \hspace{0.7cm}
        \begin{enumerate}[label=\roman*), font=\itshape]
            \item Per definizione, $\mathrm{comp}(u)$ è fortemente connesso, e dunque per ogni $v \in \mathrm{comp}(u)$ esistono due cammini $u \rightarrow v$ e $v \rightarrow u$; allora, partendo da $u$, necessariamente $v \in V(A_{\hat v})$, poiché la visita in DFS di $\hat v$ deve aver raggiunto $v$. Infine, $v$ non potrebbe essere stato visitato prima di $u$, poiché $u$ è stato scelto come c-radice di $\mathrm{comp}(u)$; allora necessariamente $v \in V(A_u)$, e dunque $V(\mathrm{comp}(u)) \subseteq V(A_u)$.
            \item Si noti che $u_1, \ldots, u_k \in V(A_u)$, e dunque per definizione $$\forall i \in [1, k] \quad V(A_{u_i}) \subseteq V(A_u)$$
                Inoltre, per la proposizione precedente, si ha che $$\forall i \in [1, k] \quad V(\mathrm{comp}(u_i)) \subseteq V(A_{u_i})$$ e anche che $$V(\mathrm{comp}(u)) \subseteq V(A_u)$$
                Allora, necessariamente $$V(\mathrm{comp}(u)) \cup \displaystyle \bigcup_{i = 1}^k {V(\mathrm{comp}(u_i))} \subseteq V(A_u)$$
                Sia $w \in V(A_u)$, e dunque esiste un cammino $u \rightarrow w$:
                
                \begin{itemize}
                    \item se esiste un cammino $w \rightarrow u$, poiché $\mathrm{comp}(u)$ è massimalmente fortemente connesso per definizione, necessariamente $w \in \mathrm{comp}(u)$; allora $V(A_u) \subseteq \mathrm{comp}(u)$, e dunque segue la tesi;
                    \item allora, si supponga non esista un tale cammino $w \rightarrow u$, e dunque $\mathrm{comp}(u) \neq \mathrm{comp}(w)$, per il \cref{disjoint comps}; allora, sia $z \in V(\mathrm{comp}(w))$ la c-radice di $\mathrm{comp}(w) = \mathrm{comp}(z)$; si noti che $z \in \mathrm{comp}(w)$, implica che è presente in $G$ un cammino $w \rightarrow z$, e dunque si ha un cammino $u \rightarrow w \rightarrow z$ in $G$; allora, la visita in DFS di $\hat v$ deve aver necessariamente raggiunto $z$, e dunque $z \in V(A_{\hat v})$;
                    \item per assurdo, sia $z \notin V(A_u)$; poiché in $G$ si ha un cammino $u \rightarrow w \rightarrow z$ per osservazione precedente, allora segue che $t(z) < t(u)$, e dunque $z$ deve essere stato visitato prima di $u$, altrimenti sarebbe stato raggiunto a partire da $u$; allora:
                    \begin{itemize}
                        \item sia $\mathrm{Int}(z) \cap \mathrm{Int}(u) = \varnothing$: si noti che $z \in \mathrm{comp}(w)$ è c-radice di $\mathrm{comp}(w)$, e dunque si ha il cammino $z \rightarrow w$; allora, poiché $\mathrm{Int}(w) \subseteq \mathrm{Int}(u)$, la visita in DFS avrebbe sbagliato a non controllare $w$ prima di rimuovere $z$ dallo stack $\lightning$
                        \item sia $\mathrm{Int}(z) \supset \mathrm{Int}(u) \supseteq \mathrm{Int}(w)$: allora si avrebbe che $A_u \subset A_z$, e dunque esiste un cammino $z \rightarrow u$, ma per il cammino $u \rightarrow w \rightarrow z$, allora si avrebbe $\mathrm{comp}(z) = \mathrm{comp}(u)$, contraddicendo l'ipotesi per cui $\mathrm{comp}(u) \neq \mathrm{comp}(w) = \mathrm{comp}(z) \ \lightning$
                    \end{itemize}
                \item allora necessariamente $z \in V(A_u)$, e poiché è stato scelto come c-radice di una delle componenti di $G$ in $A_u$, allora deve verificarsi che $z \in \{u_1, \ldots, u_k\}$;
                \item dunque $\exists i \in [1, k] \mid z= u_i$, e poiché $\forall i \in [1, k] \quad u_i \in V(\mathrm{comp}(u_i))$, allora $$z \in V(A_u) \implies \exists i \in [1, k] \mid z = u_i \in V(\mathrm{comp}(u_i))$$ dunque $V(A_u) \subseteq V(\mathrm{comp}(u_i))$ per qualche $i \in [1, k]$, e quindi segue la tesi.
                \end{itemize}
        \end{enumerate}
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo diretto $G$, rappresentato attraverso liste di adiacenza, l'algoritmo restituisce le componenti di $G$.\\
            \textbf{Input}: $G$ grafo diretto, rappresentato attraverso liste di adiacenza.\\
            \textbf{Output}: le componenti di $G$.
        }

        \begin{algorithmic}[1]
            \Function{DFS}{$G, u, \texttt{components}, \texttt{S}, \texttt{c}, \texttt{cc}$}
                \State $\texttt{c.increment()}$
                \State $\texttt{components[}u\texttt{]} = - \texttt{c}$ \Comment{i valori negativi indicano i tempi di visita}
                \State $\texttt{S.push(}u\texttt{)}$
                \State $b := \texttt{c}$
                \For{$v \in V(G) : (u, v) \in E(G)$}
                    \If{$\texttt{components[}v\texttt{]}= = 0$}
                        \State $b' := \texttt{DFS(}G, v, \texttt{components}, \texttt{S}, \texttt{c}, \texttt{cc)}$
                        \State $b = \min(b, b')$
                    \ElsIf{$\texttt{components[}v\texttt{]} < 0$}
                        \State $b = \min(b, - \texttt{components[}v\texttt{]})$
                    \EndIf
                \EndFor
                \If{$b == - \texttt{components[}u\texttt{]}$} \Comment{$u$ è c-radice}
                    \State $\texttt{cc.increment()}$
                    \Do
                        \State $w := \texttt{S.pop()}$
                        \State $\texttt{components[}w\texttt{]} = \texttt{cc}$
                    \doWhile{$u \neq w$}
                \EndIf
                \State \textbf{return} $b$
            \EndFunction
            \\
            \Function{findComponents$_2$}{$G$}
                \State $\texttt{components} := \texttt{[}0\texttt{]} * n$
                \State $\texttt{Counter cc} := 0$
                \State $\texttt{Counter c} := 0$
                \State $\texttt{Stack S} := \texttt{[}\texttt{]}$
                \For{$u \in V(G)$}
                    \If{$\texttt{components[}u\texttt{]}==0$}
                        \State $\texttt{DFS(}G, u, \texttt{components}, \texttt{S}, \texttt{c}, \texttt{cc)}$
                    \EndIf
                \EndFor
                \State \textbf{return} \texttt{components}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia alla riga 25, definendo l'array \texttt{components}, attraverso il quale verranno trovate le varie componenti del grafo $G$ in input; in particolare, alla riga 26 viene definito un contatore \texttt{cc}, affinché l'array \texttt{components} contenga, per ogni vertice $v \in V(G)$, un valore $\texttt{components[}v\texttt{]}$ pari a \texttt{cc}, se $v$ fa parte della \texttt{cc}-esima componente trovata dall'algoritmo; infine, alla riga 27 viene definito il contatore \texttt{c}, utilizzato per i tempi di visita dei vertici, ed alla riga 28 si ha uno stack \texttt{S}, che verrà utilizzato per trovare i vertici facenti parte della \texttt{cc}-esima componente.
        
        Alla riga 29, l'algoritmo inizializza un ciclo \texttt{for}, che per ognuno dei vertici $u \in V(G)$, effettuerà una chiamata ricorsiva alla funzione \texttt{DFS} (riga 31), solo se $u$ non era ancora stato visitato (riga 30).

        Alla riga 2, l'algoritmo incrementa il contatore dei tempi di visita, per poterne utilizzare il valore correttamente (si noti che alla riga 27 viene inizializzato a 0, e poiché canonicamente il primo vertice ha tempo di visita pari ad 1, il contatore viene incrementato prima di essere utilizzato). Alla riga 3, l'opposto del valore di tale contatore viene inserito all'interno di $\texttt{compontents[}u\texttt{]}$: questa tecnica permette di ridurre spazio, utilizzando solamente un array per salvare sia i tempi di visita dell'algoritmo, sia le componenti da restituire in output; infatti per ogni $u \in V(G)$, se $\texttt{components[}u\texttt{]}$ è negativo, allora si sta salvando $-t(u)$, se il valore è nullo, allora $u$ non è ancora stato visitato, e se è positivo, allora $u$ appartiene alla $\texttt{components[}u\texttt{]}$-esima componente; infine, $u$ viene inserito all'interno dello stack, nella riga 4, e viene definito $b$, alla riga 5, che rappresenta il nodo più all'indietro che è possibile raggiungere da $u$ stesso, e da uno qualsiasi dei suoi discendenti (cfr. il ciclo \texttt{for} seguente).

        Alla riga 6, la funzione ricorsiva inizializza un ciclo \texttt{for}, che per ogni vertice $v$, adiacente uscente da $u$, effettua una chiamata ricorsiva, radicata su di lui, se quest'ultimo non era ancora stato visitato dall'algoritmo (riga 7), salvando il valore trovato dalla ricorsione in $b'$, ed aggiornando opportunamente il valore di $b$ alla riga 9; infatti, poichè $b$ deve rappresentare il nodo più all'indietro che è possibile raggiungere a partire da $u$, e da un suo qualsiasi discendente, allora $b$ deve necessariamente essere il minimo tra i $b'$ di tutte le chiamate ricorsive effettuate sui figli (incluso il valore iniziale); alla riga 10, se invece il valore salvato all'interno dell'array \texttt{components}, per $v$, era negativo, allora ne era noto il tempo di vista ma non la componente a cui faceva parte, e dunque viene solamente aggiornato $b$, utilizzando proprio il tempo $t(v)$, pari a $-\texttt{components[}v\texttt{]}$.

        Nella riga 14, è presente una condizione che viene valutata vera, esclusivamente se $u$ è una c-radice della componente corrente: infatti, se $b$ è pari a $-\texttt{components[}u\texttt{]} = t(u)$, allora il nodo più all'indietro che è possibile raggiungere attraverso un discendente qualsiasi di $u$, è $u$ stesso, e dunque per il \cref{cradiciteorema} e per definizione di c-radice stessa, $u$ deve necessariamente essere una c-radice. Allora, se $u$ è c-radice, è sufficiente incrementare il contatore delle componenti, alla riga 15, ed aggiornare il valore di \texttt{components} di tutti i vertici accumulati all'interno dello stack durante le chiamate ricorsive effettuate più in profondità della chiamata corrente, con il valore di \texttt{cc}: infatti, lo stack viene utilizzato dall'algoritmo per salvare i vertici nello stesso ordine in cui sono visitati dalle chiamate ricorsive, e facendo \texttt{S.pop()} (riga 17), si è certi di preservare l'ordine di attraversamento del grafo $G$; si noti che tale operazione termina non appena viene eseguita la rimozione di $u$ stessa dallo stack, poiché sarà esso l'ultimo vertice della \texttt{cc}-esima componente.

        La funzione ricorsiva termina alla riga 21, restituendo al chiamante il valore $b$ della chiamata ricorsiva corrente. Infine, l'algoritmo termina alla riga 34, restituendo tutte le componenti di $G$.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Poiché l'algoritmo effettua esclusivamente una visita in DFS del grafo $G$ in input, e lo stack \texttt{S} conterrà $n$ nodi nel caso peggiore (all'interno della chiamata ricorsiva più profonda della visita in DFS), l'algoritmo ha costo $O(n + m)$.
    \end{proof}

    \subsection{Trovare un ciclo}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo indiretto $G$, rappresentato attraverso liste di adiacenza, l'algoritmo restituisce un suo ciclo, se presente.\\
            \textbf{Input}: $G$ grafo indiretto, rappresentato attraverso liste di adiacenza.\\
            \textbf{Output}: un ciclo di $G$, se presente.
        }

        \begin{algorithmic}[1]
            \label{findCycleNonDir}
            \Function{DFS}{$G, u, \texttt{parents}, \texttt{visited}$}
                \State $\texttt{visited[}u\texttt{]} = 1$
                \For{$v \in V(G): v \sim u$}
                    \If{$\texttt{visited[}v\texttt{]} == 0$}
                        \State $\texttt{visited[}v\texttt{]} = 1$
                        \State $\texttt{parents[}v\texttt{]} = u$
                        \State $\texttt{output} = \texttt{DFS(}G, v, \texttt{parents}, \texttt{visited)}$
                        \If{$\texttt{output} \neq \texttt{None}$}
                            \State \textbf{return} \texttt{output}
                        \EndIf
                    \ElsIf{$\left \{ \begin{array}{l} \texttt{parents[}u\texttt{]} \neq v \\ \texttt{parents[}v\texttt{]} \neq u \end{array} \right.$} \Comment{l'array \texttt{parents} è \tbf{diretto}}
                        \State $\texttt{output} := \verb|{| \verb|}|$
                        \While{$u \neq v$}
                            \State $\texttt{output.add(}u\texttt{)}$
                            \State $u = \texttt{parents[}u\texttt{]}$
                        \EndWhile
                        \State \textbf{return} \texttt{output}
                    \EndIf
                \EndFor
                \State \textbf{return} \texttt{None}
            \EndFunction
            \algstore{bkbreak}
        \end{algorithmic}
    \end{algorithm}

    \begin{nocaptionalg}
        \begin{algorithmic}[1]
            \algrestore{bkbreak}
            \Function{findCycle}{$G$}
                \State $\texttt{visited} := \texttt{[}0\texttt{]} * n$
                \State $\texttt{parents} := \texttt{[}0\texttt{]} * n$
                \For{$v \in V(G)$}
                    \If{$\texttt{visited[}v\texttt{]} = = 0$}
                        \State $\texttt{parents[}v\texttt{]} = v$ \Comment{la radice corrente}
                        \State $\texttt{output} = \texttt{DFS(}G, v, \texttt{parents}, \texttt{visited)}$
                        \If{$\texttt{output} \neq \texttt{None}$}
                            \State \textbf{return} \texttt{output}
                        \EndIf
                    \EndIf
                \EndFor
                \State \textbf{return} \texttt{None}
            \EndFunction
        \end{algorithmic}
    \end{nocaptionalg}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia effettuando un ciclo \texttt{for}, alla riga $25$, su ogni vertice $v \in V(G)$; in particolare, se $v$ non è stato ancora visitato (riga $26$), viene effettuata una visita ricorsiva in DFS di $G$, partendo da $v$. Si noti che il ciclo \texttt{for} permette di gestire il caso in cui $G$ non sia connesso; inoltre, alla riga 27 viene posto $\texttt{parents[}v\texttt{]} = v$, di fatto definendo più alberi di visita sullo stesso grafo, poiché se $G$ non fosse connesso, si avrebbero necessariamente molteplici visite in DFS, e di conseguenza molteplici alberi di visita.

        La funzione che effettua la visita ricorsiva in DFS, inizia con un ciclo \texttt{for}, alla riga $3$, all'interno del quale, per ogni vertice $v \in V(G)$ adiacente a $u$ in input non ancora visitato, viene marcato come tale (riga $5$), ne viene aggiornato il nodo padre in \texttt{parents} (riga $6$), e viene effettuata una chiamata ricorsiva radicata su $v$ stesso, per far si che la visita sia in DFS.

        Al termine della chiamata ricorsiva della riga 7, viene esaminato il valore restituito dalla funzione \texttt{DFS}, che può essere un ciclo di $G$, oppure \texttt{None}; se ci si trova nel primo caso, e dunque si è trovato un output richiesto dall'algoritmo, sarà sufficiente restituire il risultato di ricorsione in ricorsione a ritroso, ritornando \texttt{output} stesso, alla riga $9$.

        Se invece il vertice $v$ era già stato visitato, è necessario eseguire un controllo, poiché ci si potrebbe trovare all'interno di un ciclo; si noti che il ciclo \texttt{for} della riga 3, trova tutti i vertici adiacenti ad $u$, compreso il nodo padre di $u$ stesso, ma naturalmente non deve essere di interesse per la ricerca di un ciclo in $G$. Allora, se $u$ non è il nodo padre di $v$, né $v$ è il nodo padre di $u$ (si noti che l'array \texttt{parents} è diretto, anche se $G$ non lo è, dunque non è sufficiente controllare solo uno dei due casi), l'unico caso in cui può capitare di rivisitare un nodo è se ci si trova all'interno di un ciclo; di conseguenza, in tale circostanza, l'algoritmo istanzia un insieme \texttt{output}, alla riga 12, all'interno del quale vengono inseriti tutti i vertici che vengono trovati, risalendo la catena di padri, costruitasi in \texttt{parents}, partendo da $u$ stesso, fino a ritornare proprio su $v$; infine, viene restituito tale insieme, alla riga 17, che verrà propagato fino alla riga 28.

        Nel controllo della riga 29, ci si accerta che non sia ancora stato trovato alcun ciclo, e non appena ne viene trovato uno, viene restituito dall'algoritmo, troncando il loop \texttt{for}.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Poiché l'algoritmo effettua una visita ricorsiva in DFS del grafo $G$ in input, il costo dovrebbe essere $O(n + m)$, ma l'unico caso in cui viene effettuata una visita completa del grafo è nel caso peggiore, ovvero quando il grafo è aciclico. Si noti però che, se $G$ è aciclico, si verifica che $m := |E(G)| = n - 1$, e dunque $O(n + m) = O(n + n - 1) = O(2n - 1) = O(n)$.

        Allora, il costo dell'algoritmo è $O(n)$.
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo diretto $G$, rappresentato attraverso liste di adiacenza, l'algoritmo restituisce un suo ciclo, se presente.\\
            \textbf{Input}: $G$ grafo diretto, rappresentato attraverso liste di adiacenza.\\
            \textbf{Output}: un ciclo di $G$, se presente.
        }

        \begin{algorithmic}[1]
            \label{findCycleDir}
            \Function{DFS}{$G, x, \texttt{t}, \texttt{T}, \texttt{S}, \texttt{c}$}
                \For{$y \in V(G) : (x, y) \in E(G)$}
                    \If{$\texttt{t[}y\texttt{]} == 0$}
                        \State $\texttt{t[}y\texttt{]} = \texttt{c}$
                        \State \texttt{c.increment()}
                        \State $\texttt{S.push(}y\texttt{)}$
                        \State $\texttt{output} = \texttt{DFS(}G, y, \texttt{t}, \texttt{T}, \texttt{S}, \texttt{c)}$
                        \If{$\texttt{output} \neq \texttt{None}$}
                            \State \textbf{return} \texttt{output}
                        \EndIf
                    \ElsIf{$\texttt{T[}y\texttt{]} == 0$}
                        \State $\texttt{output} := \verb|{| y \verb|}|$ \Comment{altrimenti $y$ non verrebbe preso}
                        \While{$x \neq y$}
                            \State $\texttt{output.add(}x\texttt{)}$
                            \State $x = \texttt{S.pop()}$
                        \EndWhile
                        \State \textbf{return} \texttt{output}
                    \EndIf
                \EndFor
                \State $\texttt{T[}x\texttt{]} = \texttt{c}$
                \State $\texttt{S.pop()}$
                \State \textbf{return} \texttt{None}
            \EndFunction
            \algstore{bkbreak}
        \end{algorithmic}
    \end{algorithm}

    \begin{nocaptionalg}
        \begin{algorithmic}[1]
            \algrestore{bkbreak}
            \Function{findCycle}{$G$}
                \State $\texttt{t} := \texttt{[}0\texttt{]} * n$
                \State $\texttt{T} := \texttt{[}0\texttt{]} * n$
                \State $\texttt{Counter c} := 1$
                \State $\texttt{Stack s} := \texttt{[]}$
                \For{$x \in V(G)$}
                    \If{$\texttt{t[}x\texttt{]} = = 0$}
                        \State $\texttt{output} = \texttt{DFS(}G, v, \texttt{t}, \texttt{T}, \texttt{S}, \texttt{c)}$
                        \If{$\texttt{output} \neq \texttt{None}$}
                            \State \textbf{return} \texttt{output}
                        \EndIf
                    \EndIf
                \EndFor
                \State \textbf{return} \texttt{None}
            \EndFunction
        \end{algorithmic}
    \end{nocaptionalg}

    \begin{proof}[Correttezza dell'algoritmo.]
        Omessa.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Poiché l'algoritmo effettua esclusivamente una visita in DFS del grafo $G$ in input, il suo costo è pari a $O(n + m)$; si noti che, nel caso peggiore, l'intero grafo descrive un ciclo, e dunque il ciclo \texttt{while} della riga 13 ha costo $O(n)$.
    \end{proof}

    \section{Breadth-first Search (BFS)}

    \subsection{Distanza}

    \begin{frameddefn}[label={distanza}]{Distanza}
        Sia $G$ un grafo, e siano $x, y \in V(G)$ due suoi vertici; si definisce \tbf{distanza tra $x$ e $y$}, il numero minimo di archi che costituiscono un cammino $x \rightarrow y$. In simboli, sia $\mathcal{C}$ l'insieme dei cammini $x \rightarrow y$ in $G$; allora $$\mathrm{dist}(x, y) := \min_{c \in \mathcal{C}}{\left |E(c) \right|}$$
    \end{frameddefn}

    \begin{algorithm}[H]
        \caption{
            Dato un array di padri di un grafo (nel caso questo fosse diretto, l'array rappresenterebbe una visita in DFS di tale grafo), e due suoi vertici $x$ ed $y$, l'algoritmo restituisce la loro distanza.\\
            \textbf{Input}: \texttt{parents} un array di padri di un grafo; $x, y$ due vertici del grafo.\\
            \textbf{Output}: $\mathrm{dist}(x, y)$.
        }

        \begin{algorithmic}[1]
            \Function{findCommonAncestor}{\texttt{parents}, $x$, $y$}
                \State $\texttt{ancestorsX} := \texttt{[}0\texttt{]} * n$
                \State $v := x$
                \While{$\texttt{parents[}v\texttt{]} \neq v$}
                    \State $\texttt{ancestorsX[}v\texttt{]} = 1$
                    \State $v = \texttt{parents[}v\texttt{]}$
                \EndWhile
                \State $\texttt{ancestorsX[}v\texttt{]} = 1$
                \State $v := y$
                \While{$\texttt{ancestorsX[}v\texttt{]} == 0$}
                    \State $v = \texttt{parents[}v\texttt{]}$
                \EndWhile
                \State \textbf{return} $v$
            \EndFunction
            \\
            \Function{distanceRoot}{\texttt{parents}, $v$}
                \If{$\texttt{parents[}v\texttt{]} == v$}
                    \State \textbf{return} $0$
                \Else
                \State \textbf{return} $\texttt{distanceRoot(parents}, \texttt{parents[}v\texttt{])} + 1$
                \EndIf
            \EndFunction
            \\
            \Function{findDistance}{\texttt{parents}, $x$, $y$}
                \State $a := \texttt{findCommonAncestor(parents}, x, y\texttt{)}$
                \State $d_x := \texttt{distanceRoot(parents}, x\texttt{)}$
                \State $d_y := \texttt{distanceRoot(parents}, y\texttt{)}$
                \State $d_a := \texttt{distanceRoot(parents}, a\texttt{)}$
                \State \textbf{return} $d_x + d_y - 2 \cdot d_a$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia, alla riga 25, effettuando una chiamata alla funzione \texttt{findCommonAncestor}, che restituisce l'antenato, comune ad $x$ ed $y$ in input, a loro più vicino.

        In particolare, viene inizialmente istanziato un array \texttt{ancestorsX} a 0, alla riga 2, e successivamente viene inizializzato un ciclo \texttt{while}, il quale è utilizzato per marcare gli antenati di $x$, risalendo fino alla radice (condizione di uscita del ciclo, alla riga 4); si noti che la riga 8 è necessaria, in quanto $v$, in quel momento, sarà proprio la radice, la quale non potrebbe essere marcata all'interno del ciclo appena terminato, poiché l'incontro della radice è proprio la condizione di uscita di quest'ultimo; inoltre, è importante notare che l'antenato comune più vicino ad $x$ e $y$ potrebbe proprio essere la radice stessa come caso limite, ed è dunque fondamentale segnare anche quest'ultima come antenato di $x$.

        Nelle righe 9, 10 e 11 viene poi eseguito un ulteriore ciclo \texttt{while}, grazie al quale viene effettivamente trovato l'antenato comune ad $x$ ed $y$, risalendo la catena di padri di $y$ (riga 9), fintanto che gli attuali vertici $v$, padri di $y$, non siano ancora essi stessi antenati di $x$ (condizione di uscita del ciclo, alla riga 10); infatti, si noti che non appena viene trovato un $v$ tale che $\texttt{ancestorsX[}v\texttt{]}$ sia pari ad 1, allora tale $v$ è proprio il primo antenato in comune tra $x$ ed $y$ cercato, che va dunque ritornato, alla riga 13.

        Dopo aver salvato l'antenato, comune ad $x$ ed $y$, a loro più vicino, all'interno di $a$, alla riga 25, l'algoritmo procede definendo 3 variabili, $d_x$, $d_y$ e $d_a$, rispettivamente le distanze, dalla radice di \texttt{parents}, da $x$, $y$ e $a$; tali distanzi vengono determinate utilizzando la funzione \texttt{distanceRoot}, definita alla riga 16, che ricorsivamente risale i nodi padri di $v$ in input, fino a trovare la radice (caso base della ricorsione, riga 17), restituendo 0 in tal caso, ed accumulando $+1$ (riga 20) ogni volta che si decrementa la profondità delle chiamate ricorsive.

        Allora, per restituire la distanza tra $x$ ed $y$, sarà sufficiente calcolare $$d_x + d_y - 2 \cdot d_a$$ come descritto nella riga 29, poiché $d_x + d_y$ è una somma di distanze che conterrà 2 volte la distanza tra la radice, e l'antenato, comune ad $x$ ed $y$, a loro più vicino (di fatto, è l'intersezione dei cammini tra radice ed $x$, e radice ed $y$). Si noti che tale formula risulta essere sempre corretta, poiché \texttt{parents} è un array di padri, il quale rappresenta sempre un albero/arborescenza del grafo di partenza, e dunque per definizione la struttura che si sta utilizzando è aciclica, garantendo che l'unico modo per trovare la distanza tra due vertici è proprio attraverso la formula utilizzata dall'algoritmo.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        La funzione \texttt{findCommonAncestor} esegue 2 cicli \texttt{while}, i quali entrambi visitano ogni nodo di \texttt{parents}, avente esattamente $n$ nodi, una ed una sola volta; allora, il costo della funzione risulta essere $O(n) + O(n) = O(n)$.

        La funzione \texttt{distanceRoot} risale ricorsivamente l'albero \texttt{parents}, e nel caso peggiore dovranno essere ripercorsi tutti i nodi padri del grafo; allora, il suo costo è $O(n)$.

        Infine, \texttt{findDistance} effettua una chiamata a \texttt{findCommonAncestor}, e successivamente 3 chiamate a \texttt{distanceRoot}, e dunque il costo totale dell'algoritmo risulta essere $O(n) + 3 \cdot O(n) = 4 \cdot O(n) = O(n)$.
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo $G$, rappresentato attraverso liste di adiacenza, e un vettore di padri di un albero/arborescenza di visita in DFS di $G$, l'algoritmo restituisce la distanza di ogni vertice dalla radice dell'albero/arborescenza.\\
            \textbf{Input}: $G$ grafo diretto, rappresentato attraverso liste di adiacenza; \texttt{parents} un array di padri di un albero/arborescenza radicato in un certo $r \in V(G)$.\\
            \textbf{Output}: $\forall v \in V(G) \quad \mathrm{dist}(r, v)$.
        }

        \begin{algorithmic}[1]
            \Function{updateDistances}{$v$, \texttt{parents}, \texttt{distances}}
                \If{$\texttt{distances[parents[}v\texttt{]]} == -1$} \Comment{se non ancora visitato}
                    \State $\texttt{updateDistances(parents[}v\texttt{]}, \texttt{parents}, \texttt{distances)}$
                \EndIf
                \State $\texttt{distances[}v\texttt{]} = \texttt{distances[parents[}v\texttt{]]} + 1$
            \EndFunction
            \\
            \Function{findRoot}{$G$, \texttt{parents}}
                \For{$v \in V(G)$}
                    \If{$\texttt{parents[}v\texttt{]} = = v$}
                        \State \textbf{return} $v$
                    \EndIf
                \EndFor
            \EndFunction
            \\
            \Function{findDistances}{$G$, \texttt{parents}}
                \State $r := \texttt{findRoot}(G, \texttt{parents})$
                \State $\texttt{distances}:=\texttt{[}-1\texttt{]} * n$
                \State $\texttt{distances[}r\texttt{]} = 0$
                \For{$v \in V(G)$}
                    \If{$\texttt{distances[}v\texttt{]} == -1$} \Comment{se non ancora visitato}
                        \State $\texttt{updateDistances(}v, \texttt{parents}, \texttt{distances)}$
                    \EndIf
                \EndFor
                \State \textbf{return} \texttt{distances}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia istanziando un array \texttt{distances}, posto a $-1$ per ogni elemento, valore sentinella che verrà sfruttato dall'algoritmo per determinare se un dato vertice è stato visitato o meno.

        Come primo passo, alla riga 17 viene trovata la radice $r$ dell'array \texttt{parents}, il quale rappresenta un albero/arborescenza di visita in DFS del grafo $G$ in input, grazie alla funzione \texttt{findRoot}; quest'ultima, per trovare la radice, esegue un ciclo \texttt{for} sull'array \texttt{parents}, restituendo l'unico vertice $v \in V(G) : \texttt{parents[}v\texttt{]} = v$. Infine, viene salvata la distanza di $r$ dalla radice in \texttt{distances}, pari naturalmente a 0, alla riga 19.

        Successivamente, viene istanziato un ciclo \texttt{for}, alla riga 20, che per ogni vertice $v \in V(G)$ non ancora visitato dall'algoritmo (contrassegnato con $\texttt{distances[}v\texttt{]} = -1$), effettua una chiamata ricorsiva alla funzione \texttt{updateDistances}.

        All'interno di $\texttt{updateDistances}$, alla riga 2, viene controllato se la distanza del padre del vertice $v$ corrente, dalla radice di \texttt{parents}, non sia nota, e in tal caso viene effettuata una chiamata ricorsiva radicata proprio nel padre; questo, di fatto, ha l'effetto di "risalire" l'albero rappresentato da \texttt{parents}, di padre in padre, fintanto che non viene trovato un vertice la cui distanza dalla radice è nota; si noti che, inizialmente, l'unico vertice la cui distanza è nota è la radice stessa, e dunque la riga 19 funge da \tbf{caso base} della ricorsione. Infine, una volta risalito l'albero fino alla radice, vengono rimosse le chiamate ricorsive dallo stack-call, aggiornando i valori delle distanze, ora note, dalla radice, secondo la legge $$\texttt{distances[}v\texttt{]} = \texttt{distances[parents[}v\texttt{]]} + 1$$ alla riga 5, poiché naturalmente ogni figlio $v$ sarà ad una distanza, dalla radice $r$, pari alla distanza del proprio padre + 1, quest'ultimo dato dall'arco $(\texttt{parents[}v\texttt{]}, v)$ stesso.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        L'algoritmo inizialmente chiama la funzione \texttt{findRoot}, la quale effettua un singolo ciclo \texttt{for}, che nel caso peggiore ha costo $O(n)$.

        Successivamente, viene effettuato un ciclo \texttt{for} per ogni vertice del grafo $G$, il quale, per ogni nodo non ancora visitato, chiama \texttt{updateDistances}; all'interno di quest'ultima, si noti che la ricorsione viene effettuata esclusivamente se il padre del vertice corrente non è stato ancora visitato (riga 2), e dunque l'intero ciclo \texttt{for} della riga 20 ha costo $O(n)$, poiché è garantito che ogni vertice $v \in V(G)$ viene visitato una ed una sola volta.

        Allora, il costo dell'algoritmo è pari a $O(n) + O(n) = O(n)$.
    \end{proof}

    \subsection{Visita in BFS}

    \begin{frameddefn}{BFS}
        Con BFS si indica un criterio di visita di un grafo; in particolare, BFS sta per \tit{Breadth-first Search}, dunque la visita del grafo avviene controllando, prima di procedere al prossimo livello, tutti i vertici adiacenti al nodo del livello corrente.
    \end{frameddefn}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo $G$, rappresentato attraverso liste di adiacenza (nel caso di grafo diretto, è sufficiente memorizzare gli archi uscenti per ogni vertice), ed un suo vertice $u \in V(G)$, l'algoritmo restituisce le distanze dei vertici di $G$ da $u$.\\
            \textbf{Input}: $G$ grafo, rappresentato attraverso liste di adiacenza; $u \in V(G)$ un vertice di $G$.\\
            \textbf{Output}: $\forall v \in V(G) \quad \mathrm{dist}(u, v)$.
        }

        \begin{algorithmic}[1]
            \label{bfs}

            \Function{findDistances}{$G, u$}
                \State $\texttt{parents}:=\texttt{[}0\texttt{]} * n$
                \State $\texttt{parents[}u\texttt{]} = u$
                \State $\texttt{distances}:=\texttt{[}0\texttt{]} * n$
                \State $\texttt{Queue Q} := \texttt{[}u\texttt{]}$
                \While{!$\texttt{Q.isEmpty()}$}
                    \State $v := \texttt{Q.dequeue()}$
                    \For{$x \in V(G) : x \sim v$}
                    \If{$\left \{ \begin{array}{l} \texttt{distances[}x\texttt{]}= = 0 \\ \texttt{parents[}x\texttt{]} \neq x\end{array} \right .$}
                        \State $\texttt{parents[}x\texttt{]} = v$
                        \State $\texttt{distances[}x\texttt{]}=\texttt{distances[}v\texttt{]} + 1$
                        \State $\texttt{Q.enqueue(}x\texttt{)}$
                    \EndIf
                    \EndFor
                \EndWhile
                \State \textbf{return} \texttt{distances}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}
        Per dimostrare la correttezza dell'algoritmo, è necessario dimostrare che l'array \texttt{distances} in output sia corretto, e dunque che $$\forall v \in V(G) \quad \texttt{distances[}v\texttt{]} = \mathrm{dist}(u, v)$$

        dove $u$ è la radice in input. La dimostrazione procede dunque per induzione sull'array \texttt{distances}, come segue:

        \begin{itemize}
            \item \tit{caso base}
                \begin{itemize}
                    \item $\texttt{distances[}u\texttt{]} = 0$ e questo è garantito dalla riga 4, all'interno della quale vengono inizializzate tutte le distanze dei vertici del grafo, dalla radice $u$, a 0; si noti che l'algoritmo si assicura di non modificare la radice ulteriormente, grazie al controllo della riga 9, che procede solo se il vertice correntemente in esame non è la radice
                \end{itemize}
            \item \tit{ipotesi induttiva forte}
                \begin{itemize}
                    \item $\forall v \in V(G) \mid \mathrm{dist}(u, v) =: k \quad \texttt{distances[}v\texttt{]} = k$
                \end{itemize}
            \item \tit{passo induttivo}
                \begin{itemize}
                    \item è necessario dimostrare che $$\forall v \in V(G) \mid \mathrm{dist}(u, v) =: k + 1 \quad \texttt{distances[}v\texttt{]} = k + 1$$
                    \item per la \cref{distanza}, $\forall x, y \in V(G) \quad \mathrm{dist}(x, y)$ è il numero minimo di archi che costituiscono un cammino $x \rightarrow y$; allora, affinché $v$ sia a distanza $k + 1$ dalla radice $u$, deve necessariamente esistere un cammino $u \rightarrow v$ contenente esattamente $k + 1$ archi, per definizione
                    \item sia $\hat v$ il penultimo vertice del cammino $u \rightarrow v$ (l'ultimo è proprio $v$): esso avrà esattamente $(k + 1) - 1 = k$ vertici, e poiché il cammino preso in considerazione aveva un numero di archi minimo, rimuovendo l'arco $(\hat v, v)$ da quest'ultimo, si otterrà un cammino ancora minimo, che definisce di conseguenza la distanza $\mathrm{dist}(u, \hat v) = k$
                    \item allora, su $\hat v$ è possibile applicare l'ipotesi induttiva, per la quale si ha che $$\texttt{distances[}\hat v\texttt{]} = k$$ e poiché $\hat v \sim v$, allora l'arco $(\hat v, v)$ verrà percorso all'interno della visita in BFS (grazie alla riga 8), e dunque alla riga 11 si otterrà che $$\texttt{distances[}v\texttt{]} = \texttt{distances[}\hat v\texttt{]} + 1 = k + 1$$ per ipotesi induttiva.
                \end{itemize}
        \end{itemize}
    \end{proof}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia definendo un array di nodi padri \texttt{parents}, un array di distanze \texttt{distances} (che conterrà le distanze dei vertici di $G$ dalla radice $u$ di partenza), e una coda \texttt{Q}, contenente $u$ inizialmente (riga $5$).

        Successivamente, fintanto che la coda \texttt{Q} non è vuota, viene eseguito il ciclo \texttt{while} della riga $6$, all'interno del quale viene rimosso il primo nodo $v$ della coda \texttt{Q}, alla riga $7$; inoltre, per ognuno dei suoi nodi adiacenti $x \sim v$, se non ancora visitato (la sua distanza da $u$ è $0$, ma non è la radice), ne viene aggiornato il padre, alla riga $10$, e viene aggiornata la sua distanza da $u$, sommando $1$ alla distanza del padre, ossia $v$, dalla radice; infine, $x$ viene posizionato in coda. In particolare, quest'ultima riga permette all'algoritmo di effettuare una visita in BFS del grafo, poiché ogni vertice adiacente a $v$ trovato viene inserito all'interno della coda non appena viene visitato.

        Ad esempio, si consideri il seguente grafo:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (3) {3};
                \node[main node] (4) [above right of=3] {4};
                \node[main node] (2) [below right of=3] {2};
                \node[main node] (5) [above right of=4] {5};
                \node[main node] (6) [below right of=4] {6};
                \node[main node] (1) [below right of=2] {1};

                \path[every node/.style={font=\sffamily\small}]
                    (3) edge (4)
                    (3) edge (2)
                    (4) edge (5)
                    (4) edge (6)
                    (2) edge (6)
                    (2) edge (1)
                    ;
            \end{tikzpicture}
            \caption{Un grafo indiretto.}
        \end{figure}

        partendo dal vertice $u = 3$, l'ordine di visita in BFS dei vertici è il seguente: $$\{3, 4, 2, 5, 6, 1\}$$

    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Il ciclo \texttt{for} della riga $8$ viene eseguito per ognuno dei vertici adiacenti a $v$, e dunque ha costo $O(\deg(v))$; allora, per ragionamento analogo all'\cref{costo FRN2}, il costo dell'algoritmo è pari a $O(n +m)$.
    \end{proof}

    \begin{framedobs}{Archi non della visita}
        Sia $G$ un grafo, e sia $H$ un albero/arborescenza di visita in BFS di $G$, radicato in un certo $r \in V(G)$; allora $$\forall (x, y) \in E(G - H) \quad \mathrm{dist}(r, x) = \left \{ \begin{array}{l} \mathrm{dist}(r, y) \pm 1 \\ \mathrm{dist}(r, y) \end{array} \right.$$
    \end{framedobs}

    \begin{example}[Archi non della visita]
        Ad esempio, si consideri il seguente grafo indiretto $G$:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (3) {3};
                \node[main node] (4) [below right of=3] {4};
                \node[main node] (2) [below left of=3] {2};
                \node[main node] (6) [below of=4] {6};
                \node[main node] (5) [right of=6] {5};
                \node[main node] (7) [below of=2] {7};
                \node[main node] (1) [left of=7] {1};

                \path[every node/.style={font=\sffamily\small}]
                    (3) edge (4)
                    (3) edge (2)
                    (4) edge (5)
                    (4) edge (6)
                    (4) edge (7)
                    (2) edge (1)
                    (2) edge (7)
                    (6) edge (7)
                    ;
            \end{tikzpicture}
            \caption{Un grafo indiretto.}
        \end{figure}

        eseguendo una visita in BFS su di esso, radicata in 3, si potrebbe ottenere il seguente albero $T$:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (3) {3};
                \node[main node] (4) [below right of=3] {4};
                \node[main node] (2) [below left of=3] {2};
                \node[main node] (6) [below of=4] {6};
                \node[main node] (5) [right of=6] {5};
                \node[main node] (7) [below of=2] {7};
                \node[main node] (1) [left of=7] {1};

                \path[every node/.style={font=\sffamily\small}]
                    (3) edge (4)
                    (3) edge (2)
                    (4) edge (5)
                    (4) edge (6)
                    (2) edge (1)
                    (2) edge (7)
                    ;
            \end{tikzpicture}
            \caption{Un grafo indiretto.}
        \end{figure}

        allora, si noti che $$E(G - T) = \{(4, 7), (6, 7)\}$$ ed infatti si ha che $$\mathrm{dist}(3, 7) = 2 = 1 + 1 = \mathrm{dist}(3, 4) + 1$$ $$\mathrm{dist}(3, 6) = 2 = \mathrm{dist}(3, 7)$$
    \end{example}

    \subsection{Trovare il numero di cammini minimi}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo $G$, rappresentato attraverso liste di adiacenza, ed un suo vertice $u$, per ogni $v \in V(G)$, l'algoritmo restituisce il numero di cammini minimi della forma $u \rightarrow v$.\\
            \textbf{Input}: $G$ grafo, rappresentato attraverso liste di adiacenza; $u \in V(G)$ un vertice di $G$.\\
            \textbf{Output}: per ogni vertice $v \in V(G)$, il numero di cammini minimi della forma $u \rightarrow v$.
        }

        \begin{algorithmic}[1]
            \Function{countShortestPaths}{$G, u$}
                \State $\texttt{parents}:=\texttt{[}-1\texttt{]} * n$
                \State $\texttt{parents[}u\texttt{]} = u$
                \State $\texttt{count}:=\texttt{[}0\texttt{]} * n$
                \State $\texttt{count[}u\texttt{]}= 1$ \Comment{solo un modo per giungere ad $u$ stesso}
                \State $\texttt{distances}:=\texttt{[}0\texttt{]} * n$
                \State $\texttt{Queue Q} := \texttt{[}u\texttt{]}$
                \While{!$\texttt{Q.isEmpty()}$}
                    \State $v := \texttt{Q.dequeue()}$
                    \For{$y \in V(G) : y \sim v$}
                        \If{$\texttt{parents[}y\texttt{]}== -1$} \Comment{se non è stato ancora visitato}
                            \State $\texttt{parents[}y\texttt{]} = v$
                            \State $\texttt{distances[}y\texttt{]}=\texttt{distances[}v\texttt{]} + 1$
                            \State $\texttt{count[}y\texttt{]}=\texttt{count[}v\texttt{]}$
                            \State $\texttt{Q.enqueue(}y\texttt{)}$
                        \ElsIf{$\texttt{distances[}y\texttt{]}==\texttt{distances[}v\texttt{]} + 1$}
                            \State $\texttt{count[}y\texttt{]} \ += \texttt{count[}v\texttt{]}$
                        \EndIf
                    \EndFor
                \EndWhile
                \State \textbf{return} \texttt{count}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo presenta una semplice modifica dell'\cref{bfs}, inserendo le righe $14$, $16$ e $17$:

        \begin{itemize}
            \item alla riga $14$, viene posto il numero di cammini minimi per raggiungere $y$ partendo da $u$, pari al numero di cammini minimi di $v$, ovvero il suo nodo padre; ciò, in quanto alla riga $11$ viene controllato che il nodo non sia stato ancora visitato, e dunque il suo valore in \texttt{count} sarà ancora pari a $0$, e con la riga $14$ viene dunque semplicemente rimpiazzato; inoltre, il numero di cammini minimi per raggiungere un figlio, sara sicuramente almeno pari a quello del padre;
            \item alla riga $16$, viene controllato che il valore $\texttt{distances[}y\texttt{]}$ sia pari a $\texttt{distances[}v\texttt{]} + 1$, poiché in tal caso il cammino corrente è un cammino minimo verso $y$, e dunque alla riga $17$ viene accumulato al conteggio di $y$, il conteggio di $v$, ovvero il suo nodo padre.
        \end{itemize}

        Infatti, si noti che avendo un vertice $y \in V(G)$, figlio di $v_1, \ldots, v_k \in V(G)$, è sempre vero che $$\texttt{count[}y\texttt{]} = \texttt{count[}v_1\texttt{]} + \ldots + \texttt{count[}v_k\texttt{]}$$
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        L'algoritmo effettua esclusivamente una visita in BFS del grafo $G$ in input, e dunque il suo costo è pari a $O(n+m)$.
    \end{proof}

    \subsection{Distanza tra insiemi di vertici}

    \begin{frameddefn}{Distanza tra insiemi di vertici}
        Sia $G$ un grafo, e $X, Y \subseteq V(G)$ due sottoinsiemi di vertici di $G$; allora, si definisce \tbf{distanza tra $X$ e $Y$} la distanza minima tra due vertici di $X$ e $Y$; in simboli $$\mathrm{dist}(X, Y) := \min_{x \in X, y \in Y}{\mathrm{dist}(x, y)}$$
    \end{frameddefn}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo $G$, rappresentato attraverso liste di adiacenza, e due suoi sottoinsiemi di vertici $X, Y \subseteq V(G)$, l'algoritmo restituisce $\mathrm{dist}(X, Y)$.\\
            \textbf{Input}: $G$ grafo, rappresentato attraverso liste di adiacenza; $X, Y \subseteq V(G)$ sottoinsiemi di vertici di $G$.\\
            \textbf{Output}: $\mathrm{dist}(X, Y)$.
        }

        \begin{algorithmic}[1]
            \Function{setDistance}{$G$, $X$, $Y$}
                \State $\texttt{distances}:=\texttt{[}-1\texttt{]} * n$
                \State $\texttt{Queue Q} := \texttt{[}\texttt{]}$
                \For{$x \in X$}
                    \State $\texttt{Q.enqueue(}x\texttt{)}$
                    \State $\texttt{distances[}x\texttt{]} = 0$ \Comment{i vertici $x \in X$ sono a distanza 0 da $X$}
                \EndFor
                \While{!$\texttt{Q.isEmpty()}$}
                    \State $v := \texttt{Q.dequeue()}$
                    \For{$u \in V(G) : u \sim v$}
                        \If{$\texttt{distances[}u\texttt{]}== -1$} \Comment{se non è stato ancora visitato}
                            \State $\texttt{distances[}u\texttt{]}=\texttt{distances[}v\texttt{]} + 1$
                            \State $\texttt{Q.enqueue(}u\texttt{)}$
                        \EndIf
                    \EndFor
                \EndWhile
                \State $d := + \infty$
                \For{$y \in Y$}
                    \If{$\texttt{distances[}y\texttt{]} < d$}
                        \State $d = \texttt{distances[}y\texttt{]}$
                    \EndIf
                \EndFor
                \State \textbf{return} $d$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia inserendo ogni vertice $x \in X$ in \texttt{Q}, all'interno del ciclo \texttt{for} della riga 4; successivamente, viene effettuata una visita in BFS di $G$, salvando le distanze trovate all'interno dell'array \texttt{distances}.

        Si noti che, per dimostrazione precedente, l'algoritmo di visita in BFS trova la distranza tra vertici, naturalmente relativa al vertice di partenza; tale caratteristica può essere sfruttata, ad esempio in questo algoritmo, per calcolare tutte le distanze tra i vertici di $X$ e di $Y$, semplicemente ponendo ogni vertice di $X$ all'interno della coda \texttt{Q} all'inizio dell'algoritmo: così facendo, la visita in BFS calcolerà le distanze a partire dai vertici $x \in X$, che avranno $\texttt{distances[}x\texttt{]} = 0$, e per trovare $\mathrm{dist}(X, Y)$ cercata, è sufficiente un ciclo \texttt{for}, alla riga 18, con il quale viene semplicemente trovata $$\displaystyle d := \min_{y \in Y}{\texttt{distances[}y\texttt{]}} = \min_{x \in X, y \in Y}{\mathrm{dist}(x, y)} =: \mathrm{dist}(X, Y)$$
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        L'algoritmo è costituito da una visita in BFS del grafo $G$ in input, partendo dai vertici in $X$, che ha costo $O(n + m)$ per osservazioni precedenti, seguita da un ciclo \texttt{for} su ogni vertice $y \in Y$, che ha dunque costo $O(|Y|)$; di conseguenza, il costo dell'algoritmo è pari a $O(n + m) + O(|Y|)$, ma si noti che $Y \subseteq V(G) \implies |Y| \le |V(G)| =: n \implies O(n + m) + O(|Y|) = O(n + m)$.
    \end{proof}

    \chapter{Algoritmi greedy}

    \section{Algoritmi greedy}

    \subsection{Definizioni}

    \begin{frameddefn}{Algoritmi greedy}
        Un algoritmo è detto \tbf{greedy}, se cerca una soluzione effettuando delle scelte di passo in passo, optando sempre per il passo più "appetibile" momentaneamente.
    \end{frameddefn}

    \section{Distanza pesata}

    \subsection{Archi pesati}

    \begin{frameddefn}{Archi pesati}
        Sia $G$ un grafo; su esso, è possibile definire una funzione $$w: E(G) \rightarrow \mathbb{R}^+$$ che, ad ogni arco, associa un valore reale positivo detto \tbf{peso}.
    \end{frameddefn}

    \begin{example}[Grafo indiretto pesato]
        Ad esempio, si consideri il seguente grafo indiretto:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [above of=1] {2};
                \node[main node] (3) [right of=2] {3};
                \node[main node] (4) [below of=3] {4};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge node {1} (2)
                    (2) edge node {2} (3)
                    (3) edge node {5} (4)
                    (4) edge node {10} (1)
                    (1) edge node {6} (3)
                    ;
            \end{tikzpicture}
            \caption{Un grafo indiretto pesato.}
        \end{figure}

        su di esso, sono stati inseriti dei valori sugli archi, che ne indicano il peso, secondo la funzione $w$ che associa $$w(1, 2) = 1; \ w(2,3)=2; \ w(3, 4)=5; \ w(4, 1)=10; \ w(1,3)=6$$
    \end{example}

    \begin{frameddefn}{Peso di un cammino}
        Sia $G$ un grafo, e $c$ un cammino in $G$; si definisce \tbf{peso di $c$} la somma dei pesi degli archi che lo compongono. In simboli, sia $\mathcal {C}$ l'insieme dei cammini su $G$; allora, è possibile definire la funzione seguente $$w_p : \mathcal{C} \rightarrow \mathbb{R}^+: c \rightarrow \displaystyle \sum_{e \in E(c)}{w(e)}$$ che associa un cammino $c$ al suo peso $w_p(c)$.
    \end{frameddefn}

    \begin{frameddefn}{Distanza pesata}
        Sia $G$ un grafo pesato, e $x, y \in V(G)$ due suoi vertici; si definisce \tbf{distanza pesata tra $x$ e $y$} il peso del cammino $x \rightarrow y$ di peso minimo. In simboli, sia $\mathcal{C}$ l'insieme dei cammini $x \rightarrow y$ in $G$; allora $$\mathrm{dist}_w(x, y) := \min_{c \in \mathcal{C}}{w_p(c)}$$
    \end{frameddefn}

    \begin{example}[Grafo indiretto pesato]
        Ad esempio, si consideri il seguente grafo indiretto pesato:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [above of=1] {2};
                \node[main node] (3) [right of=2] {3};
                \node[main node] (4) [below of=3] {4};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge node {1} (2)
                    (2) edge node {2} (3)
                    (3) edge node {5} (4)
                    (4) edge node {10} (1)
                    (1) edge node {6} (3)
                    ;
            \end{tikzpicture}
            \caption{Un grafo indiretto pesato.}
        \end{figure}

        si noti che, in esso, si ha che $$\mathrm{dist}(1, 4) = 10$$ attraversando l'arco $(1, 4)$, mentre $$\mathrm{dist}_w(1, 4)=w(1, 2) + w(2, 3) + w(3, 4)=1 + 2 + 5 = 8$$ attraversando gli archi $(1, 2), (2, 3)$ e $(3, 4)$, poiché $$w(1, 4) = 10; \ w(1, 3) + w(3, 4) = 6 + 5 = 11$$ entrambi maggiori di $8$.
    \end{example}

    \begin{framedlem}[label={caratteristiche distanze pesate}]{Caratteristiche delle distanze pesate}
        Sia $G$ un grafo; le seguenti proposizioni sono vere:

        \begin{enumerate}[label=\roman*), font=\itshape]
            \item $\forall x \in V(G) \quad \mathrm{dist}_w(x, x) = 0$
            \item $\forall x, y \in V(G) \quad \mathrm{dist}_w(x, y) \ge 0$
            \item $\forall x, y, z \in V(G) \quad \mathrm{dist}_w(x, y) \le \mathrm{dist}_w(x, z) + \mathrm{dist}_w(z, y)$, detta \textup{disuguaglianza triangolare}.
        \end{enumerate}
    \end{framedlem}

    \begin{proof}
        \hspace{0.7cm}
        \begin{enumerate}[label=\roman*), font=\itshape]
            \item La distanza da un nodo in sé stesso deve necessariamente essere nulla, poiché composta da 0 archi.
            \item Per definizione di $w$, la somma di numeri in $\mathbb{R}^+$ deve ancora essere in $\mathbb{R}^+$.
            \item Siano $Q_1$ e $Q_2$ cammini della forma $x \rightarrow z$ e $z \rightarrow y$ rispettivamente; si noti che il peso della passeggiata $Q_1 \cup Q_2$, della forma $x \rightarrow y$, è pari a $$w_p(Q_1 \cup Q_2) = w(Q_1) + w(Q_2)$$ e inoltre, per il \cref{cammini e passeggiate}, è possibile trovare un cammino all'interno di tale passeggiata che, al più, attraversa lo stesso numero di archi di $Q_1 \cup Q_2$; allora segue che tale cammino deve avere peso minore o uguale al peso della passeggiata che lo contiene, e dunque esiste un cammino tale che $$\mathrm{dist}_w(x, y) \le \mathrm{dist}_w(x, z) + \mathrm{dist}_w(z, y)$$
        \end{enumerate}
    \end{proof}

    \begin{framedobs}{Distanze pesate di un grafo diretto}
        Sia $G$ un grafo diretto, e $x, y \in V(G)$ due suoi vertici; si noti che, in un grafo diretto, $\mathrm{dist}_w$ non è necessariamente simmetrica, e potrebbe verificarsi che $$\mathrm{dist}_w(x, y) \neq \mathrm{dist}_w(y, x)$$
    \end{framedobs}

    \begin{framedlem}[label={vicini pesati}]{Distanze pesate dei vicini}
        Sia $G$ un grafo indiretto pesato attraverso $w$ e $v \in V(G)$ un suo vertice; sia $N(v)$ l'insieme dei vertici adiacenti a $v$, e sia $$u \in \argmin_{x \in N(v)}{w(v, x)}$$ allora necessariamente $$\mathrm{dist}_w(v, u) = w(v, u)$$
    \end{framedlem}

    \begin{proof}
        Per ipotesi, $u$ è un vertice adiacente a $v$, attraverso un arco $(v, u) \in E(G)$, avente peso minimo tra gli adiacenti a $v$; allora, considerando qualsiasi altro cammino $c'$ della forma $v \rightarrow u$, non passante per l'arco $(v, u)$, si deve necessariamente avere $$w_p(c') > w_p(\{v, (v, u), u\})$$ poiché il primo arco di $c'$ avrà peso maggiore di $w(v, u)$, per come $u$ è stato scelto in ipotesi.
    \end{proof}

    \begin{framedthm}[label={dijkstra theorem}]{Estensioni pesate di insiemi di vertici}
        Sia $G$ un grafo indiretto, e sia $R \subseteq V(G)$ un sottoinsieme dei suoi vertici; sia $v\in R$ un vertice in $R$, e sia $(x, u) \in E(G)$ un arco con $x \in R$ e $u \in V(G) - R$, tale che $$(x, u) \in \argmin_{\substack{(a, b) \in E(G) \\ a \in R \\ b \in V(G) - R}}{\left(\mathrm{dist}_w(v, x) + w(a, b)\right)}$$ allora necessariamente $$\mathrm{dist}_w(v, u) = \mathrm{dist}_w(v, x) + w(x, u)$$
    \end{framedthm}

    \begin{proof}
        Per ipotesi, $(x, u)$ è un arco tale da minimizzare $\mathrm{dist}_w(v, x) + w(x, u)$; si noti che, affinché sia verificata la tesi, è necessario dimostrare che esista un cammino $v \rightarrow u$, passante per $(x, u)$, tale da minimizzare $\mathrm{dist}_w(v, x) + w(x, u)$, e che non esistano cammini $v \rightarrow u$, non passanati per $(x, u)$, di peso inferiore:

        \begin{itemize}
            \item per la prima parte, è sufficiente considerare il cammino $v \rightarrow x$ che definisce $\mathrm{dist}_w(v, x)$, al quale è possibile aggiungere l'arco $(x, u)$ stesso, per ottenere il cammino che minimizzi $\mathrm{dist}_w(v, x) + w(x, u)$, grazie al \cref{vicini pesati};
            \item per la seconda parte, sia $Q$ un cammino $v \rightarrow u$, non passante per $(x, u)$; allora $Q$ deve necessariamente passare per un altro arco $(x', u')$, con $x' \in R$ e $u' \in V(G) - R$; ma poiché $(x, u)$ è stato scelto tale da minimizzare la somma $\mathrm{dist}_w(v, x) + w(x, u)$, si ha che la porzione di cammino $v \rightarrow (x', u')$ deve essere pari o superiore a $w_p\left(v \rightarrow (x, u)\right)$, e dunque $w_p(Q) \ge w_p(v \rightarrow (x, u))$; allora, il cammino cercato deve necessariamente passare per $(x, u)$.
        \end{itemize}
    \end{proof}

    \subsection{Algoritmo di Dijkstra}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo pesato $G$ attraverso $w$, e $u \in V(G)$ un suo vertice, l'algoritmo restituisce le distanze pesate dei vertici di $G$ da $u$; l'algoritmo assume che ogni vertice di $G$ sia raggiungibile da $u$, e che ogni peso definito da $w$ sia positivo.\\
            \textbf{Input}: $G$ grafo, e $u$ un suo vertice tale che per ogni $x \in V(G)$ esiste un cammino della forma $u \rightarrow x$; $w$ una funzione che associa pesi agli archi in $E(G)$, tale che ogni peso sia positivo.\\
            \textbf{Output}: le distanze pesate dei vertici di $G$ da $u$.
        }

        \begin{algorithmic}[1]
            \Function{Dijkstra$_1$}{$G$, $w$, $v$}
                \State $\texttt{distancesW} := \texttt{[}+ \infty \texttt{]} * n$ \Comment{valore sufficientemente grande}
                \State $\texttt{distancesW[}v\texttt{]} = 0$
                \State $R := \{v\}$
                \While{$\exists (\alpha, \beta) \in E(G) \mid \left \{ \begin{array}{l} \alpha \in R \\ \beta \in V(G) - R \end{array} \right.$}
                    \State $\displaystyle (x, u) \in \argmin_{\substack{(a, b) \in E(G) \\ a \in R \\ b \in V(G) - R}}{\left(\texttt{distancesW[}a\texttt{]} + w(a, b)\right)}$
                    \State $\texttt{distancesW[}u\texttt{]} = \texttt{distancesW[}x\texttt{]} + w(x, u)$
                    \State $R = R \cup \{u\}$
                \EndWhile
                \State \textbf{return} \texttt{distancesW}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia definendo, alla riga 2, l'array \texttt{distancesW}, interamente inizializzato a $+ \infty$, che dovrà essere riempito dall'algoritmo e restituito al termine dell'esecuzione; inoltre, alla riga 3 viene definito un insieme di vertici $R \subseteq V(G)$.

        Alla riga 5 viene istanziato un ciclo \texttt{while}, il quale procede fintanto che è presente un arco uscente da $R$, dunque composto da un vertice in $R$ ed uno in $V(G) - R$; al suo interno, alla riga 6, viene preso un arco $(x, u)$, uscente da $R$, tale da minimizzare la somma $$\texttt{distancesW[}x\texttt{]} + w(x, u) = \mathrm{dist}_w(v, x) + w(x, u)$$ e si noti che, per il \cref{dijkstra theorem}, si ha che $$\mathrm{dist}_w(v, u) = \mathrm{dist}_w(v, x) + w(x, u)$$ allora, viene aggiornato il valore di $\texttt{distancesW[}u\texttt{]}$ coerentemente, alla riga 7; infine, alla riga 8, viene aggiunto $u$ ad $R$. Infine, si noti inoltre che il \texttt{while} della riga 5 prosegue, fintanto che esistono ancora archi uscenti da $R$, e dunque l'algoritmo termina quando $R$ contiene ogni vertice del grafo.

        Si noti che l'algoritmo, ad ogni iterazione, sceglie dunque un arco $(x, u)$ tale da minimizzare il peso del cammino $\{v \rightarrow x\} \cup (x, u)$, e al termine di ogni iterazione del ciclo \texttt{while} viene inserito $u$ all'interno di $R$, estendendo l'insieme di vertici visitati; di conseguenza, il comportamento dell'algoritmo è greedy. Si noti che, per trovare un tale arco è necessario controllare ogni $(x, u)$ uscente da $R$.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Per trovare l'arco cercato, alla riga 7, è necessario controllare ogni arco uscente da $R$, e trovare quello tale da minimizzare la somma richiesta; si noti che, per il \cref{Somma dei gradi}, si ha che $$\displaystyle \sum_{v \in R_i}{\deg(v)} \le \sum_{v \in V(G)}{\deg(v)} = 2m$$ e dunque la ricerca da effettuare per l'$i$-esima iterazione ha costo $O(m)$; si noti inoltre che il ciclo \texttt{while} prosegue fino all'esaurimento degli archi uscenti da $R$, e dunque fino al termine della visita completa del grafo.

        Allora il costo del ciclo, e dunque dell'algoritmo, è pari a $O(n \cdot m)$.
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo pesato $G$ attraverso $w$, e $u \in V(G)$ un suo vertice, l'algoritmo restituisce le distanze pesate dei vertici di $G$ da $u$; l'algoritmo assume che ogni vertice di $G$ sia raggiungibile da $u$, e che ogni peso definito da $w$ sia positivo.\\
            \textbf{Input}: $G$ grafo, e $u$ un suo vertice tale che per ogni $x \in V(G)$ esiste un cammino della forma $u \rightarrow x$; $w$ una funzione che associa pesi agli archi in $E(G)$, tale che ogni peso sia positivo.\\
            \textbf{Output}: le distanze pesate dei vertici di $G$ da $u$.
        }

        \begin{algorithmic}[1]
            \Function{Dijkstra$_2$}{$G$, $w$, $u$}
                \State $\texttt{distancesW} := \texttt{[}+ \infty \texttt{]} * n$ \Comment{valore sufficientemente grande}
                \State $\texttt{distancesW[}u\texttt{]} = 0$
                \State $\texttt{MinHeap H} := \texttt{[]}$
                \For{$v \in V(G)$}
                    \State $\texttt{H.insert(}v, \texttt{distancesW[}v\texttt{]}\texttt{)}$ \Comment{le chiavi sono le distanze pesate da $u$}
                \EndFor
                \While{!\texttt{H.isEmpty()}}
                    \State $v := \texttt{H.extract\_min()}$ \Comment{il minore viene \underline{rimosso} da \texttt{H}}
                    \State $\texttt{distancesW[}v\texttt{]} = \texttt{H.get\_key(}v\texttt{)}$
                    \For{$x \in V(G) : \left \{ \begin{array}{l} (v, x) \in E(G) \\ \texttt{H.contains(}x\texttt{)} \end{array} \right.$}
                        \State $d := \texttt{distancesW[}v\texttt{]} + w(v, x)$
                        \If{$\texttt{H.get\_key(}x\texttt{)} > d$}
                            \State $\texttt{H.set\_key(}x, d\texttt{)}$
                        \EndIf
                    \EndFor
                \EndWhile
                \State \textbf{return} \texttt{distancesW}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia alla riga 2, ponendo a $+ \infty$ l'array che, al termine dell'algoritmo, conterrà le distanze, pesate su $w$, tra $u$ ed ogni altro vertice di $G$; inoltre, alla riga 3 viene posta la distanza pesata $\mathrm{dist}_w(u, u) = 0$, ed alla riga 4 viene definito \texttt{H}, un min-heap. Successivamente, alla riga 5, viene istanziato un ciclo \texttt{for}, utilizzato per inizializzare il min-heap, ponendo in esso ogni vertice $v \in V(G)$, associando loro i valori $$\texttt{distancesW[}v\texttt{]} = \mathrm{dist}_w(u, v)$$

        Alla riga 8, l'algoritmo inizia un ciclo \texttt{while}, che terminerà solamente quando il min-heap diventerà vuoto, ovvero quando ogni elemento al suo interno sarà stato analizzato; infatti, alla riga 9, con l'operazione $\texttt{H.extract\_min()}$, il vertice $v$ che attualmente minimizza la distanza pesata da $u$, viene \tbf{rimosso} dal min-heap. Dopo averlo estratto, alla riga 10 ne viene aggiornato il valore all'interno di $\texttt{distancesW[}v\texttt{]}$ (si noti che il valore \tbf{correntemente} noto della distanza pesata tra $u$ e $v$ è contenuto all'interno della chiave del min-heap, mentre l'array conterrà il valore finale). Alla riga 11, viene istanziato un ciclo \texttt{for}, il quale per ogni vertice $x$ uscente da $v$, ancora contenuto nel min-heap (e dunque ancora non esaminato), ne aggiorna la chiave all'interno di \texttt{H} (riga 14), se quest'ultima è maggiore di $\texttt{distancesW[}v \texttt{]} + w(v, x)$, ovvero $\mathrm{dist}_w(u, v)$ sommata al peso dell'arco $(v, x)$ stesso (righe 12 e 13).

        L'algoritmo termina restituendo $\texttt{distancesW}$ alla riga 18.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Il primo ciclo \texttt{for}, della riga 5, ha costo $O(n \log n)$, poiché contiene la riga 6, all'interno della quale viene utilizzata la funzione \texttt{H.insert()}, che ha costo $O(\log n)$.

        Il ciclo \texttt{for} della riga 11, contiene le righe 11, 13, 14, all'interno delle quale vengono utilizzate rispettivamente le funzioni \texttt{H.contains()}, \texttt{H.get\_key()} e \texttt{H.set\_key()}, che hanno tutte costo $O(\log n)$; inoltre, tale ciclo viene effettuato per ognuno dei vertici adiacenti uscenti da $x$, e dunque il suo costo è pari a $\displaystyle O \left( \sum_{x \in N(v)} 3 \log n \right) = \deg(v) \cdot O(\log n)$, dove con $N(v)$ si sta indicando l'insieme $$N (v) := \{x \in V(G) : (v, x) \in E(G)\}$$ degli archi adiacenti uscenti da $v$, e dunque si ha che $\left|N(v)\right| = \deg(v)$.

        Infine, il ciclo \texttt{while} della riga 8, poiché contiene le rige 9 e 10, che utilizzano rispettivamente le funzioni \texttt{H.extract\_min()} ed \texttt{H.get\_key()}, le quali hanno entrambe costo $O(\log n)$, ha costo pari a $\displaystyle O \left( \sum_{v \in V(G)} 2 \log n + \deg(v) \cdot \log n \right) = O \left( \sum_{v \in V(G)} \log n \right) + O \left ( \sum_{v \in V(G)} \deg(v) \cdot \log n \right) = O(n \log n) + \log n \cdot O (m )$.

        Allora, il costo dell'algoritmo è pari a $O (\log n (n + m))$.
    \end{proof}

    \section{Intervalli}

    \subsection{Trovare intervalli disgiunti}

    \begin{algorithm}[H]
        \caption{
            Data una lista di intervalli, l'algoritmo restituisce il sottoinsieme di intervalli disgiunti, di cardinalità massima.\\
            \textbf{Input}: $I$ lista di intervalli di numeri reali della forma $[a, b]$, con $a, b \in \mathbb{R}$.\\
            \textbf{Output}: il sottoinsieme di $I$ di intervalli disgiunti di cardinalità massima.
        }

        \begin{algorithmic}[1]
            \label{intervalli1}
            \Function{findIntervals}{$I$}
                \State $I\texttt{.sort(key=}\lambda (i): i \rightarrow i\texttt{.right())}$ \Comment{$I$ ordinato sugli estremi destri}
                \State $\texttt{Sol}:= \texttt{[]}$
                \For{$i \in I$}
                    \State $b_f := \texttt{Sol.last().right()}$ \Comment{estremo destro di \texttt{Sol.last()}}
                    \If{$i\texttt{.left()} > b_f$} \Comment{equivalente a $\texttt{Sol} \cap i = \varnothing$}
                        \State $\texttt{Sol.append(}i\texttt{)}$
                    \EndIf
                \EndFor
                \State \textbf{return} \texttt{Sol}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}
        Siano $\texttt{Sol}_0, \ldots, \texttt{Sol}_n$ gli stati di $\texttt{Sol}_k$, ad ogni iterazione $k \in [1, n]$ dell'algoritmo; allora, l'algoritmo funziona correttamente se e solo se $\exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}^* = \texttt{Sol}_n$.
        \begin{itemize}
            \item[] \tit{Prima implicazione.} $\exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}_n \subseteq \texttt{Sol}^*$
            \item \tit{caso base}
                \begin{itemize}
                    \item $k = 0 \implies \texttt{Sol}_0 := \varnothing \implies \forall \texttt{Sol}^*$ soluzione ottimale $\quad \texttt{Sol}_0 \subseteq \texttt{Sol}^*$
                \end{itemize}
            \item \tit{ipotesi induttiva}
                \begin{itemize}
                    \item $\exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}_k \subseteq \texttt{Sol}^*$
                \end{itemize}
            \item \tit{passo induttivo}
                \begin{itemize}
                    \item è necessario dimostrare che $\exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}_{k + 1} \subseteq \texttt{Sol}^*$
                    \item si noti che, per via della riga 7, per ogni $k \in [1, n)$ si verifica che
                    \begin{equation*}
                        \resizebox{.86\hsize}{!}{
                            $\texttt{Sol}_{k + 1} = \left \{ \begin{array}{ll} \texttt{Sol}_k & \exists [a_i, b_i] \in \texttt{Sol}_k \mid [a_i, b_i] \cap [a_{k + 1}, b_{k + 1}] \neq \varnothing \\ \texttt{Sol}_k \cup \{[a_{k + 1}, b_{k + 1}]\} & \forall [a_i, b_i] \in \texttt{Sol}_k \quad [a_i, b_i] \cap [a_{k + 1}, b_{k + 1}] = \varnothing \end{array} \right.$
                        }
                    \end{equation*}
                    
                    \item allora, nel primo caso, se esiste un intervallo $[a_i, b_i] \in \texttt{Sol}_k$ tale per cui $[a_i, b_i] \cap [a_{k + 1}, b_{k + 1}] \neq \varnothing$, si ha che l'intervallo $[a_{k + 1}, b_{k + 1}]$ non può essere inserito in $\texttt{Sol}_{k + 1}$, e dunque $\texttt{Sol}_{k + 1} = \texttt{Sol}_k \implies \exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}_{k + 1} = \texttt{Sol}_k \subseteq \texttt{Sol}^*$ per ipotesi induttiva
                    \item se invece non esiste alcun intervallo $[a_i, b_i] \in \texttt{Sol}_k$, tale che $[a_i, b_i] \cap [a_{k + 1}, b_{k + 1}] \neq \varnothing$, allora $[a_{k + 1}, b_{k + 1}]$ deve essere inserito in $\texttt{Sol}_{k + 1} = \texttt{Sol}_k \cup \{[a_{k + 1}, b_{k + 1}]\}$; si noti che, per definizione di $\texttt{Sol}_{k + 1}$, si ha che $$\forall k \in [1, n) \quad \texttt{Sol}_k \subseteq \texttt{Sol}_{k + 1}$$ allora, per dimostrare la tesi del passo induttivo è sufficiente considerare $[a_{k + 1}, b_{k + 1}]$; infatti $[a_{k + 1}, b_{k + 1}] \in \texttt{Sol}^* \implies \texttt{Sol}_{k +1} \subseteq \texttt{Sol}^*$ immediatamente
                    \item sia allora $[a_{k +1}, b_{k +1}] \notin \texttt{Sol}^*$; poiché $\texttt{Sol}^*$ è ottimale, tale condizione può verificarsi se e solo se $$\exists [a_j, b_j] \in \texttt{Sol}^* \mid [a_j, b_j] \cap [a_{k +1}, b_{k +1}] \neq \varnothing$$
                    dove $[a_j, b_j] \neq [a_{k +1},b_{k +1}]$
                    \item per assurdo sia $b_j < b_{k + 1}$; si noti che $[a_j, b_j] \in \texttt{Sol}^* \implies [a_j, b_j]$ è disgiunto con ogni altro intervallo in $\texttt{Sol}^*$ per definizione; inoltre, poiché gli intervalli in $I$ sono stati ordinati, in ordine crescente, attraverso il loro estremo destro, alla riga 2, allora $b_j < b_{k + 1}$ implica che $[a_j, b_j]$ doveva essere stato esaminato prima della $(k + 1)$-esima iterazione, e poiché $[a_j, b_j] \in \texttt{Sol}^*$, allora sicuramente $[a_j, b_j] \in \texttt{Sol}_k$, poiché $\texttt{Sol}_k \subseteq \texttt{Sol}^*$ per ipotesi induttiva, e dunque nessun intervallo in $\texttt{Sol}_k$ avrà intersezione con $[a_j, b_j]$
                    \item allora $\left. \begin{array}{l} [a_j, b_j] \in \texttt{Sol}_k \subseteq \texttt{Sol}^* \\ {[}a_j, b_j] \cap [a_{k +1}, b_{k + 1}] \neq \varnothing \end{array}\right \} \implies [a_{k +1}, b_{k +1}] \notin \texttt{Sol}_{k +1} \ \lightning$ 
                    \item allora, segue necessariamente che $b_j > b_{k + 1}$, e dunque $[a_j, b_j] \in \texttt{Sol}^* - \texttt{Sol}_k$; inoltre, poiché $[a_j, b_j] \cap [a_{k +1} , b_{k +1}] \neq \varnothing$ in ipotesi, si ha che $a_j < b_{k +1} < b_j$
                    \item sia $[a_h, b_h] \in \texttt{Sol}^* - \texttt{Sol}_k \mid [a_h, b_h] \neq [a_j, b_j]$; poiché $I$ è ordinato come precedentemente discusso, segue che $[a_h, b_h] \notin \texttt{Sol}_k \implies b_h > b_{k +1}$
                    \item inoltre, per definizione $[a_h, b_h] \in \texttt{Sol}^* \iff \forall [a_i, b_i] \in \texttt{Sol}^* \quad [a_h, b_h] \cap [a_i, b_i] = \varnothing$, e poiché $[a_j,b_j] \in \texttt{Sol}^*$, in particolare si ha che $[a_h, b_h] \cap [a_j, b_j] = \varnothing$; allora, poiché $b_h > b_{k + 1} \in (a_j, b_j)$, necessariamente $$a_j < b_{k +1} < b_j < a_h < b_h$$ e in particolare, $b_{k + 1} < a_h \implies [a_{k +1}, b_{k +1}] \cap [a_h, b_h] = \varnothing$
                    \item questo dimostra che ogni intervallo $[a_h, b_h] \in \texttt{Sol}^* - \texttt{Sol}_k$, che non sia proprio $[a_j, b_j]$, è disgiunto con $[a_{k +1}, b_{k +1}]$; allora, per trovare un insieme $\texttt{Sol}^*$ soluzione ottimale, tale per cui $\texttt{Sol}_{k + 1} \subseteq \texttt{Sol}^*$, è sufficiente considerare l'insieme $$\left(\texttt{Sol}^* - \{[a_j, b_j]\}\right) \cup \{[a_{k +1}, b_{k +1}]\}$$ in quanto $\texttt{Sol}_k \subseteq \texttt{Sol}^*$ per ipotesi induttiva, e $\texttt{Sol}_{k + 1} = \texttt{Sol}_k \cup \{[a_{k +1}, b_{k + 1}]\}$ per osservazione precedente.
                \end{itemize}
                \item[] \tit{Seconda implicazione.} $\exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}^* \subseteq \texttt{Sol}_n$. Per assurdo, si assuma che $\nexists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}^* \subseteq \texttt{Sol}_n \iff \forall \texttt{Sol}^*$ soluzione ottimale $\quad \texttt{Sol}^* \not \subseteq \texttt{Sol}_n \iff$ per ogni soluzione ottimale $\texttt{Sol}^*$ si verifica una delle seguenti:
                \begin{itemize}
                    \item $\texttt{Sol}^* \cap \texttt{Sol}_n = \varnothing$
                    \item $\texttt{Sol}^* \cap \texttt{Sol}_n \neq \varnothing \land \texttt{Sol}^* \not \subseteq \texttt{Sol}_n \land \texttt{Sol}_n \not \subseteq \texttt{Sol}^*$
                    \item $\texttt{Sol}^* \cap \texttt{Sol}_n \neq \varnothing \land \texttt{Sol}_n \subsetneq \texttt{Sol}^*$
                \end{itemize}

                si noti che per dimostrazione precedente, deve necessariamente verificarsi il terzo caso; allora $\texttt{Sol}_n \subsetneq \texttt{Sol}^* \iff \exists[a_t, b_t] \in \texttt{Sol}^* - \texttt{Sol}_n$, dove $[a_t, b_t]$ è disgiunto con ogni altro intervallo di $\texttt{Sol}_n$, poiché $\texttt{Sol}_n \subset \texttt{Sol}^*$ per dimostrazione precedente, e $\texttt{Sol}^*$ è ottimale. Allora, poiché l'$n$-esima è l'ultima iterazione dell'algoritmo, si ha che $t \in [1, n]$, e dunque $[a_t, b_t] \notin \texttt{Sol}_n$ implicherebbe che l'algoritmo avrebbe sbagliato a non inserire tale intervallo in $\texttt{Sol}_n$, poiché sarebbe stato analizzato e scartato alla riga 6 $\lightning$.
        \end{itemize}
    \end{proof}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia ordinando l'insieme di intervalli $I$, in ordine crescente del loro estremo destro, alla riga 2; successivamente, all'interno di \texttt{Sol} vengono inseriti tutti gli intervalli $i \in I$, tali che il loro estremo sinistro sia maggiore dell'estremo destro dell'ultimo elemento attualmente in \texttt{Sol}.

        Infatti, è possibile controllare esclusivamente l'ultimo elemento in \texttt{Sol}, poiché l'ordinamento iniziale garantisce che gli unici intervalli inseriti al suo interno saranno disgiunti; inoltre, grazie al controllo della riga 6, l'estremo sinistro dell'intervallo corrente deve essere superiore dell'estremo destro di \texttt{Sol.last()}, affinchè questo possa essere inserito in \texttt{Sol}.

        Infine, l'ordinamento della riga 2 garantisce di ottenere in output il sottoinsieme di intervalli disgiunti di cardinalità massima, poiché come primo intervallo in $I$ verrà posto l'intervallo con l'estremo destro inferiore.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Si noti che, all'interno dell'algoritmo si assume \texttt{Sol.last()} abbia costo $O(1)$, andando ad indicizzare correttamente l'ultimo elemento della lista.

        Poiché l'ordinamento di $I$, alla riga 2, indipendentemente dall'algoritmo di ordinamento (per confronti) scelto, non può avere costo inferiore a $O(n \log n)$, e il ciclo \texttt{for} della riga 4 ha costo $O(n)$, si ha che l'algoritmo ha costo $O(n \log n) + O(n) = O(n \log n)$.
    \end{proof}

    \subsection{Trovare insieme non disgiunto}

    \begin{algorithm}[H]
        \caption{
            Data una lista di intervalli, l'algoritmo restituisce l'insieme, di cardinalità minima, di interi $x_1, \ldots, x_k$, tali da intersecarsi con ogni intervallo.\\
            \textbf{Input}: $I$ lista di intervalli di numeri reali della forma $[a, b]$, con $a, b \in \mathbb{R}$.\\
            \textbf{Output}: l'insieme minimo di $x_1, \ldots, x_n$ tali che $\forall i \in I \quad i \cap \{x_1, \ldots, x_n\} \neq \varnothing$.
        }

        \begin{algorithmic}[1]
            \Function{findIntersectionPoints}{$I$}
                \State $I\texttt{.sort(key=}\lambda (i) : i \rightarrow i\texttt{.right())}$ \Comment{$I$ ordinato sugli estremi destri}
                \State $\texttt{Sol}:= \varnothing$
                \For{$i \in I$}
                    \State $b_f := \texttt{Sol.last().right()}$ \Comment{estremo destro di \texttt{Sol.last()}}
                    \If{$i\texttt{.left()} > b_f$} \Comment{equivalente a $\texttt{Sol} \cap i = \varnothing$}
                        \State $\texttt{Sol} = \texttt{Sol} \cup \{i\texttt{.right()}\}$
                    \EndIf
                \EndFor
                \State \textbf{return} \texttt{Sol}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}
        La struttura della dimostrazione di questo algoritmo è analoga a quella dell'\cref{intervalli1}; è dunque omessa la premessa iniziale, e la dimostrazione procede assumendo le stesse condizioni di partenza per l'induzione.

        \begin{itemize}
            \item[] \tit{Prima implicazione.} È riportata di seguito la dimostrazione a partire dal passo induttivo. 
            \item \tit{passo induttivo}
                \begin{itemize}
                    \item è necessario dimostrare che $\exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}_{k + 1} \subseteq \texttt{Sol}^*$
                    \item si noti che, per via della riga 6, si ha che $$\forall k \in [1, n) \quad \texttt{Sol}_{k +1} = \left \{ \begin{array}{ll} \texttt{Sol}_k & \texttt{Sol}_k \cap [a_{k +1}, b_{k+1}] \neq \varnothing \\ \texttt{Sol}_k \cup \{b_{k +1}\} & \texttt{Sol}_k \cap [a_{k +1}, b_{k +1}] = \varnothing \end{array} \right.$$
                    \item si noti che, nel primo caso, si ha che $\texttt{Sol}_{k +1} = \texttt{Sol}_k$, e dunque $\exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}_{k +1} = \texttt{Sol}_k \subseteq \texttt{Sol}^*$ per ipotesi induttiva
                    \item allora sia $[a_{k+1}, b_{k+1}]$ tale che $\texttt{Sol}_k \cap [a_{k+1}, b_{k +1}] = \varnothing \implies \texttt{Sol}_{k +1} = \texttt{Sol}_k \cup \{b_{k +1}\}$
                    \item sia $\texttt{Sol}^*$ una soluzione ottimale tale da soddisfare l'ipotesi induttiva; si noti che $b_{k +1} \in \texttt{Sol}^* \implies \exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}_{k +1} = \texttt{Sol}_k \cup \{ b_{k +1}\} \subseteq \texttt{Sol}^*$ per ipotesi induttiva
                    \item sia allora $b_{k +1} \notin \texttt{Sol}^*$; si noti che, poiché $\texttt{Sol}^*$ è una soluzione ottimale, necessariamente $$b_{k +1} \notin \texttt{Sol}^* \iff \exists x \in \texttt{Sol}^*  \mid x \in [a_{k + 1}, b_{k +1}]$$ inoltre, si noti che $x \notin \texttt{Sol}_k$, poiché $$\left . \begin{array}{r} b_{k +1} \in \texttt{Sol}_{k +1} \\ x \in \texttt{Sol}_k \\ x \in [a_{k +1}, b_{ k+1}] \end{array} \right \} \implies \texttt{Sol}_k \cap [a_{k +1}, b_{k +1}] \neq \varnothing \ \lightning$$ allora, poiché per ipotesi induttiva si ha che $\texttt{Sol}_k \subseteq \texttt{Sol}^*$, necessariamente $x \in \texttt{Sol}^* - \texttt{Sol}_k$
                    \item sia $j \in [1, n) \mid x \in [a_j, b_j]$; allora, si verifica uno dei seguenti casi:
                        \begin{itemize}
                            \item $j \le k \implies \exists \hat x \in \texttt{Sol}_k \mid \hat x \in [a_j, b_j]$ poiché doveva essere già stato analizzato dall'algoritmo
                            \item $j > k + 1 \implies b_j > b_{k +1}$; si noti inoltre che l'intervallo $[a_j, b_j]$ è stato scelto tale che $x \in [a_j, b_j]$, e poiché $x \in [a_{k +1}, b_{k +1}]$ per sua definizione, si verifica necessariamente che $$\left .\begin{array}{r} x \in [a_j , b_j] \cap [a_{k + 1}, b_{k +1}] \neq \varnothing \\ b_j > b_{k +1} \end{array} \right \} \implies a_j \le x \le b_{k +1} \le b_j$$ in particolare, si noti che $b_{k +1} \in [a_j, b_j]$, e poiché la dimostrazione non dipende dalla scelta di $x \in \texttt{Sol}^* - \texttt{Sol}_k$, né da $j \in [1, n)$, si ha che $b_{k +1}$ copre ogni intervallo coperto da $x$; allora, per far si che esista soluzione ottimale $\texttt{Sol}^*$ tale da contenere $\texttt{Sol}_{k +1} = \texttt{Sol}_k \cup \{b_{k +1}\}$, è sufficiente considerare $$(\texttt{Sol}^* - \{x\}) \cup \{b_{k +1}\}$$
                        \end{itemize}
                \end{itemize}
            \item[] \tit{Seconda implicazione.} $\exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}^* \subseteq \texttt{Sol}_n$. La dimostrazione è analoga a quella dell'\cref{intervalli1}, ed è dunque omessa.
        \end{itemize}
    \end{proof}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia ordinando gli intervalli in $I$, attraverso il loro estremo destro, e definendo successivamente un insieme \texttt{Sol} che conterrà la soluzione cercata.

        Alla riga 4, viene istanziato un ciclo \texttt{for}, che per ogni intervallo $i \in I$, ne inserisce l'estremo destro all'interno di \texttt{Sol}, se e solo tale $i$ non si interca con l'ultimo intervallo contenuto attualmente in \texttt{Sol} stesso.

        Si noti che la scelta dell'estremo destro garantisce di trovare una soluzione ottimale, poiché gli intervalli sono ordinati attraverso il loro estremo destro, ed è dunque necessario utilizzare gli estremi destri degli intervalli anche per trovare i valori da inserire in \texttt{Sol}, altrimenti non sarebbe garantito che quest'ultimo abbia cardinalità minima.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        L'ordinamento della riga 2 non può essere eseguito con costo minore di $O(n \log n)$; inoltre, il ciclo \texttt{for} della riga 4 esegue linearmente una sola iterazione, per ogni intervallo in $I$, e dunque il suo costo è pari a $O(n)$.

        Allora, il costo dell'algoritmo è pari a $O(n \log n) + O(n) = O(n \log n)$.
    \end{proof}

    \section{Minimum Spanning Tree (MST)}

    \subsection{Definizioni}

    \begin{frameddefn}{MST}
        Sia $G$ un grafo indiretto pesato; allora, si definisce \tbf{MST} (\tit{Minimum Spanning Tree}) un sottografo di $G$, connesso, di peso minimo, contenente ogni vertice di $G$.
    \end{frameddefn}

    \begin{example}[MST]
        Ad esempio, si consideri il seguente grafo indiretto pesato:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [above of=1] {2};
                \node[main node] (5) [below right of=2] {5};
                \node[main node] (3) [above right of=5] {3};
                \node[main node] (4) [below of=3] {4};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge node {3} (2)
                    (1) edge node {1} (5)
                    (2) edge node {3} (3)
                    (3) edge node {5} (4)
                    (3) edge node [left]{2} (5)
                    (4) edge node {2} (1)
                    (5) edge node {5} (4)
                    ;
            \end{tikzpicture}
            \caption{Un grafo indiretto pesato.}
        \end{figure}

        di esso, un possibile MST è il seguente:

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [above of=1] {2};
                \node[main node] (5) [below right of=2] {5};
                \node[main node] (3) [above right of=5] {3};
                \node[main node] (4) [below of=3] {4};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge node {1} (5)
                    (2) edge node {3} (3)
                    (3) edge node [left]{2} (5)
                    (4) edge node {2} (1)
                    ;
            \end{tikzpicture}
            \caption{Un MST del grafo pesato.}
        \end{figure}
    \end{example}

    \begin{framedlem}[label={lemma mst}]{MST aciclici}
        Sia $G$ un grafo indiretto pesato, con pesi strettamente positivi, e sia $H$ un suo MST; allora $H$ è aciclico.
    \end{framedlem}

    \begin{proof}
        Per assurdo, sia $H$ ciclico; allora, $H$ contiene almeno un ciclo, e per definizione ogni vertice di tale ciclo avrà grado almeno pari a 2; allora, rimuovendo un arco $(u, v)$ dal ciclo di $H$, si ottiene un grafo $H'$ con le seguenti caratteristiche:

        \begin{itemize}
            \item $\deg(u)$ e $\deg(v)$ saranno decrementati di 1, e poiché $\deg(u), \deg(v) \ge 2$, allora in $H'$ si ha $\deg(u), \deg(v) \ge 1$; allora, segue che $H'$ è ancora connesso, e che $V(H') = V(H) = V(G)$;
            \item sia $w_H$ il peso di $H$, e $w_{H'}$ il peso di $H'$; allora si ha che $$w_{H'} = w_H - w(u, v) \implies w_{H'} < w_H$$ poiché $w(u, v) > 0$ in ipotesi;
        \end{itemize}

        allora, segue che $H'$ è un MST, ma poiché $w_H \neq w_{H'}$, allora $H$ scelto in ipotesi non aveva peso minimo $\lightning$.
    \end{proof}

    \begin{framedlem}{Unicità dell'MST}
        Sia $G$ un grafo indiretto connesso, pesato attraverso una funzione $w$ che associa un peso ad ogni arco, tale che $w$ sia iniettiva; allora, esiste un unico MST di $G$.
    \end{framedlem}

    \begin{proof}
        Omessa.
    \end{proof}

    \subsection{Algoritmo di Kruskal}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo indiretto connesso $G$, pesato attraverso $w$ con pesi strettamente positivi, l'algoritmo ne restituisce un MST.\\
            \textbf{Input}: $G$ grafo indiretto connesso; $w$ una funzione che associa pesi, strettamente positivi, agli archi in $E(G)$.\\
            \textbf{Output}: un MST di $G$.
        }

        \begin{algorithmic}[1]
            \Function{Kruskal}{$G$, $w$}
                \State $E(G)\texttt{.sort(key=}\lambda (e): e \rightarrow w(e)\texttt{)}$ \Comment{$E(G)$ ordinato sui pesi di $w$}
                \State $\texttt{Sol}:= \varnothing$
                \For{$e \in E(G)$}
                \If{$\texttt{findCycle(}\texttt{Sol} \cup \{e\}\texttt{)} == \texttt{None}$} \Comment{se non contiene cicli}
                        \State $\texttt{Sol} = \texttt{Sol} \cup \{e\}$
                    \EndIf
                \EndFor
                \State \textbf{return} \texttt{Sol}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}
        Per dimostrare la correttezza dell'algoritmo, è necessario dimostrare che l'output sia un MST; allora, $\texttt{Sol}_m$ (si noti che l'algoritmo ha $m$ iterazioni) deve essere un albero, deve essere di copertura, e deve essere minimale rispetto al peso degli archi dato da $w$; si ha dunque che:

        \begin{itemize}
            \item $\texttt{Sol}_m$ è un albero
                \begin{itemize}
                    \item $\texttt{Sol}_m$ è aciclico, in quanto alla riga 5 vengono aggiunti esclusivamente archi che non formano un ciclo con l'insieme \texttt{Sol} di archi già aggiunti
                    \item per assurdo, sia $\texttt{Sol}_m$ un grafo non connesso, e dunque in esso esiste una componente $C$, non contenente tutti i vertici in $V(G)$; si noti che, poiché $G$ in input è connesso, allora necessariamente deve esistere un arco $$(x, y) \in E(G) \mid \left \{ \begin{array}{l} x \in V(C) \\ y \in V(G - C) \end{array} \right.$$ si noti inoltre che tale arco non può formare un ciclo in $C$, poiché $\texttt{Sol}_m$ è aciclico per il punto precedente; allora, per via della riga 5, l'algoritmo avrebbe sbagliato a non selezionare l'arco $(x, y)$
                    \item allora, per definizione, \texttt{Sol} è un albero
                \end{itemize}
            \item $\texttt{Sol}_m$ è un albero di copertura
                \begin{itemize}
                    \item poiché $\texttt{Sol}_m$ è connesso, e l'algoritmo visita ogni arco di $G$, anch'esso connesso, non può esistere un vertice non contenuto in $\texttt{Sol}_m$, e dunque l'output è necessariamente un albero di copertura
                \end{itemize}
        \end{itemize}

        La struttura della dimostrazione per garantire che $\texttt{Sol}_m$ sia un albero di copertura minimale, è analoga a quella dell'\cref{intervalli1}, con la sola differenza che le iterazioni del ciclo sono $m$ e non $n$ (si noti il ciclo \texttt{for} della riga 4); è dunque omessa la premessa iniziale, e la dimostrazione procede assumendo le stesse condizioni di partenza per l'induzione.

        \begin{itemize}
            \item[] \tit{Prima implicazione.} È riportata di seguito la dimostrazione a partire dal passo induttivo. 
            \item \tit{passo induttivo}
                \begin{itemize}
                    \item è necessario dimostrare che $\exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}_{k + 1} \subseteq \texttt{Sol}^*$
                    \item si noti che, per via della riga 5, si ha che per ogni $k \in [1, m)$ si ha che $$\texttt{Sol}_{k +1} = \left \{ \begin{array}{ll} \texttt{Sol}_k & \texttt{findCycle(Sol} \cup \{e_{k +1}\} \texttt{)} \neq \texttt{None} \\ \texttt{Sol}_k \cup \{e_{k +1}\} & \texttt{findCycle(Sol} \cup \{e_{k +1}\} \texttt{)} = \texttt{None} \end{array} \right.$$
                    \item si noti che, nel primo caso, si ha che $\texttt{Sol}_{k +1} = \texttt{Sol}_k$, e dunque $\exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}_{k +1} = \texttt{Sol}_k \subseteq \texttt{Sol}^*$ per ipotesi induttiva
                    \item allora sia $e_{k + 1}$ tale che $\texttt{findCycle(Sol} \cup \{e_{k +1}\} \texttt{)} = \texttt{None} \implies \texttt{Sol}_{k +1} = \texttt{Sol}_k \cup \{e_{k +1}\}$
                    \item sia $\texttt{Sol}^*$ una soluzione ottimale tale da soddisfare l'ipotesi induttiva; si noti che $e_{k +1} \in \texttt{Sol}^* \implies \exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}_k \cup \{ e_{k +1}\} \subseteq \texttt{Sol}^*$ per ipotesi induttiva
                    \item sia allora $e_{k +1} \notin \texttt{Sol}^*$, e sia $e_{k+1}:=(x, y) \in E(G)$ per certi $x, y \in V(G)$; si noti che, poichè $\texttt{Sol}^*$ è una soluzione ottimale, e dunque un MST, per il \cref{lemma mst} è aciclica, e dunque $(x, y) \notin \texttt{Sol}^*$ se e solo se esiste un cammino della forma $x \rightarrow y$ in $\texttt{Sol}^*$; allora, necessariamente $\{x \rightarrow y\} \cup \{e_{k +1}\}$ è un ciclo in $\texttt{Sol}^* \cup \{e_{k +1}\}$
                    \item si noti che, essendo $\{x \rightarrow y\} \cup \{e_{k +1}\}$ un ciclo parzialmente contenuto in $\texttt{Sol}_k$, allora deve esistere un arco $e_j := (z, y) \in \{x \rightarrow y\} - \texttt{Sol}_{k +1}$ per un certo $z \in V(G)$ (di fatto, l'ultimo arco del cammino $x \rightarrow y$)
                    \item si noti che, considerando l'insieme di archi $(\texttt{Sol}^* \cup \{e_{k +1}\}) - \{e_j\}$, il vertice $y$ resta coperto
                    \item si noti che l'arco $e_j$ non è in $\texttt{Sol}_{k +1}$, e dunque $k + 1 < j$; allora, per via dell'ordinamento degli archi alla riga $2$, necessariamente $w(e_{k +1}) \le w(e_j)$; dunque, si ha che $$\displaystyle \sum_{e \in ((\texttt{Sol}^* \cup \{e_{k +1}\}) - \{e_j\})}{w(e)} \le \sum_{e \in \texttt{Sol}^*}{w(e)}$$ ma poiché $\texttt{Sol}^*$, per definizione, è soluzione ottimale, non è possibile trovare una soluzione con peso inferiore, e dunque necessariamente $$(\texttt{Sol}^* \cup \{e_{k +1}\}) - \{e_j\}$$ ha lo stesso peso di $\texttt{Sol}^*$, che costituisce un MST di $G$; allora, tale insieme di archi è una soluzione ottimale, contenente $\texttt{Sol}_{k +1}$ per definizione.
                \end{itemize}
            \item[] \tit{Seconda implicazione.} $\exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}^* \subseteq \texttt{Sol}_m$. Si noti che, per implicazione precedente, esiste una soluzione ottimale $\texttt{Sol}^\#$ tale che $\texttt{Sol}_m \subseteq \texttt{Sol}^\#$; allora, poichè $\texttt{Sol}_m$ è stato dimostrato essere un MST, e $\texttt{Sol}^\#$ è un MST per definizione, si ha che $$\texttt{Sol}_m \subseteq \texttt{Sol}^\# \implies \texttt{Sol}_m = \texttt{Sol}^*$$
        \end{itemize}
    \end{proof}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia ordinando gli archi di $G$, in ordine crescente, attraverso i loro pesi, calcolati con la funzione $w$ in input, alla riga 2; questo ordinamento garantisce che l'algoritmo prenderà in considerazione esclusivamente gli archi col peso minore.

        Successivamente, alla riga 3 viene istanziato l'insieme \texttt{Sol}, che conterrà gli archi che comporranno la soluzione dell'algoritmo (ovvero, un MST di $G$), e all'interno del ciclo \texttt{for} della riga 4, l'algoritmo esegue un controllo (riga 5) per ogni singolo arco $e \in E(G)$, inserendolo all'interno di \texttt{Sol} (riga 6), esclusivamente se $\texttt{Sol} \cup \{e\}$ non costituisce un ciclo; questo controllo infatti garantisce che non si vengano a creare cicli all'interno della soluzione, e dunque che l'output sia sicuramente un albero.

        L'algoritmo termina restituendo, alla riga 9, l'insieme $\texttt{Sol} \subseteq E(G)$ degli archi che costituiscono un MST di $G$.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Alla riga 2, gli archi di $G$ vengono ordinati attraverso i pesi forniti dalla funzione $w$, e dunque il costo dell'operazione è pari a $O(m \log m)$. Si noti però che, nel caso peggiore, $G$ ha ogni vertice connesso con ogni altro vertice, e dunque $m = n\cdot ( n - 1) = n^2 - n$, ed è dunque possibile riscrivere il costo dell'operazione nel caso peggiore come segue: $$O (m \log m) = O(m \log (n^2 - n)) = O(m \log n^2) - O(m \log n) =$$ $$= O(m \log n^2) = O(m \cdot 2 \log n) = O(m \log n)$$

        Alla riga 4, viene istanziato un ciclo \texttt{for}, che per ognuno degli archi, effettua operazioni in tempo $O(n)$ (si veda l'\cref{findCycleNonDir} per la funzione \texttt{findCycle}), e dunque ha costo $m \cdot O(n) = O(n \cdot m)$.

        Allora, l'algoritmo ha costo $O(m \log n) + O(n \cdot m) = O(n \cdot m)$.
    \end{proof}

    \subsection{Algoritmo di Prim}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo indiretto connesso $G$, pesato attraverso $w$ con pesi positivi, l'algoritmo ne restituisce un MST.\\
            \textbf{Input}: $G$ grafo indiretto connesso; $w$ una funzione che associa pesi positivi agli archi in $E(G)$.\\
            \textbf{Output}: un MST di $G$.
        }

        \begin{algorithmic}[1]
            \Function{Prim$_1$}{$G$, $w$}
                \State $v \in V(G)$
                \State $\texttt{Sol} := \varnothing$
                \State $R := \{v\}$
                \While{$R \neq V(G)$}
                    \State $\displaystyle (x, y) \in \argmin_{a \in R, b \notin R} {w(a, b)}$
                    \State $\texttt{Sol} = \texttt{Sol} \cup \{ (x, y) \}$
                    \State $R = R \cup \{y\}$
                \EndWhile
                \State \textbf{return} \texttt{Sol}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}
        Per dimostrare la correttezza dell'algoritmo, è necessario dimostrare che l'output sia un MST; allora, $\texttt{Sol}_n$ deve essere un albero, deve essere di copertura, e deve essere minimale rispetto al peso degli archi dato da $w$; si ha dunque che:

        \begin{itemize}
            \item $\texttt{Sol}_n$ è un albero
                \begin{itemize}
                    \item $\texttt{Sol}_n$ è aciclico, in quanto alla riga 7 vengono aggiunti esclusivamente archi con un estremo in $R$ (insieme dei vertici visitati), e l'altro non in $R$ (si noti la riga 6), e dunque è garantito che l'algoritmo non ripercorra gli stessi vertici più di una volta
                    \item inoltre, per lo stesso motivo, l'algoritmo non crea componenti non connesse all'interno di $\texttt{Sol}_n$, poiché vengono presi sempre archi che congiungono $R$ con $V(G) - R$, e il grafo in input è connesso, e dunque necessariamente deve esserlo anche $\texttt{Sol}_n$
                    \item allora, per definizione, $\texttt{Sol}_n$ è un albero
                \end{itemize}
            \item $\texttt{Sol}_n$ è un albero di copertura
                \begin{itemize}
                    \item poiché il ciclo \texttt{while} della riga 5 non termina fino a quando $R = V(G)$, l'algoritmo visita tutti i vertici del grafo, e dunque $\texttt{Sol}_n$ è necessariamente un albero di copertura
                \end{itemize}
        \end{itemize}

        La struttura della dimostrazione per garantire che $\texttt{Sol}_n$ sia un albero di copertura minimale, è analoga a quella dell'\cref{intervalli1}; è dunque omessa la premessa iniziale, e la dimostrazione procede assumendo le stesse condizioni di partenza per l'induzione.

        \begin{itemize}
            \item[] \tit{Prima implicazione.} È riportata di seguito la dimostrazione a partire dal passo induttivo. 
            \item \tit{passo induttivo}
                \begin{itemize}
                    \item è necessario dimostrare che $\exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}_{k + 1} \subseteq \texttt{Sol}^*$
                    \item si noti che, per via della riga 7, si ha che $$\forall k \in [1, n) \quad \texttt{Sol}_{k +1} = \texttt{Sol}_k \cup \{e_{k +1}\}$$
                    \item sia $\texttt{Sol}^*$ una soluzione ottimale tale da soddisfare l'ipotesi induttiva; si noti che $e_{k +1} \in \texttt{Sol}^* \implies \exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}_{k +1} = \texttt{Sol}_k \cup \{e_{k +1}\} \subseteq \texttt{Sol}^*$ per ipotesi induttiva
                    \item sia allora $e_{k +1} \notin \texttt{Sol}^*$; si noti che, per il \cref{lemma mst}, si ha che qualsiasi soluzione ottimale $\texttt{Sol}^*$, che è un MST, deve essere aciclica, e dunque necessariamente $e_{k +1} \notin \texttt{Sol}^* \implies \texttt{Sol}^* \cup \{e_{k +1}\}$ è un ciclo $C$ di $\texttt{Sol}^*$; inoltre, poiché $\texttt{Sol}^*$ è una soluzione ottimale, ed è dunque un MST, il suo insieme di archi deve necessariamente avere il peso minimo possibile, e dunque per trovare una soluzione ottimale tale da includere $\texttt{Sol}_{k + 1}$, è necessario garantire che il peso di quest'ultima sia pari a quello di $\texttt{Sol}^*$
                    \item inoltre, necessariamente $C \not \subset R_k$, poiché l'arco $e_{k + 1}$, per via della riga 6, deve essere stato scelto con un estremo in $R_k$ e un estremo in $\texttt{Sol}^* - R_k$; allora, $C$ è solo parzialmente contenuto in $R_k$, ma poiché $C$ è un ciclo, affinché questo si chiuda deve necessariamente esistere un secondo arco $\hat e \in C$, tale da congiungere $R_k$ e $\texttt{Sol}^* - R_k$
                    \item si noti che deve necessariamente verificarsi che $$w(e_{k +1}) \le w(\hat e) \iff w(e_{k +1}) - w(\hat e) \le 0$$ poiché se $\hat e$ avesse avuto peso inferiore a $e_{k +1}$, l'algoritmo, alla riga 6, lo avrebbe scelto al posto di $e_{k +1}$
                    \item allora, dalla formula precedente segue che $$\displaystyle \sum_{e \in ((\texttt{Sol}^* \cup \{e_{k +1}\} )- \{\hat e\} )}{w(e)}  = \sum_{e \in \texttt{Sol}^*}{w(e)} + w(e_{k +1}) - w(\hat e) \le \sum_{e \in \texttt{Sol}^*}{w(e)}$$ e dunque, e possibile considerare l'insieme di archi $$(\texttt{Sol}^* \cup \{e_{k+1}\}) - \{\hat e\}$$ per ottenere una soluzione ottimale tale da contenere $\texttt{Sol}_{k + 1}$.
                \end{itemize}

            \item[] \tit{Seconda implicazione.} $\exists \texttt{Sol}^*$ soluzione ottimale $\mid \texttt{Sol}^* \subseteq \texttt{Sol}_m$. Si noti che, per implicazione precedente, esiste una soluzione ottimale $\texttt{Sol}^\#$ tale che $\texttt{Sol}_m \subseteq \texttt{Sol}^\#$; allora, poichè $\texttt{Sol}_m$ è stato dimostrato essere un MST, e $\texttt{Sol}^\#$ è un MST per definizione, si ha che $$\texttt{Sol}_m \subseteq \texttt{Sol}^\# \implies \texttt{Sol}_m = \texttt{Sol}^*$$
        \end{itemize}
    \end{proof}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia scegliendo un verice $v \in V(G)$ qualsiasi, e lo inserisce in $R$ alla riga 4, dove quest'ultimo rappresenterà, nelle successive iterazioni l'insieme di vertici correntemente visitati dall'algoritmo; viene inoltre definto \texttt{Sol} alla riga 3, che sarà l'insieme di archi costituenti un MST di $G$.

        All'interno del ciclo \texttt{while} della riga 5, fintanto che $R$ non è pari a $V(G)$ (si noti dunque che ogni vertice del grafo viene visitato, garantendo che \texttt{Sol} copra $G$, poiché quest'ultimo è un grafo indiretto connesso), viene scelto un arco $(x, y)$, avente peso minore, con un estremo in $R$, e l'altro in $V(G) - R$ (dunque, un arco che colleghi la regione di vertici già visitati, con quelli ancora da visitare); ciò garantisce che il grafo in output sia connesso, e poiché viene scelto con peso minimo, è anche garantito che \texttt{Sol} sia minimale rispetto ai pesi di $w$; allora, l'output dell'algoritmo è proprio un MST.

        L'algoritmo termina restituendo, alla riga 10, l'insieme di archi trovato.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        L'algoritmo effettua un singolo ciclo \texttt{while}, alla riga 5, che termina nel momento in cui $R$ contiene ogni vertice di $V(G)$, e poiché ad ogni iterazione viene inserito esattamente un vertice in $R$ (riga 8), allora il ciclo viene ripetuto esattamente $n$ volte.

        Inoltre, alla riga 6 viene cercato l'arco, uscente da $R$, con peso minore, e il costo di tale operazione, nel caso peggiore, è $O(m)$, poiché implica il controllo del peso di ogni singolo arco uscente da $R$.

        Allora, complessivamente l'algoritmo ha costo $n \cdot O(m) = O(n \cdot m)$.
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo indiretto connesso $G$, pesato attraverso $w$ con pesi positivi, l'algoritmo ne restituisce un MST.\\
            \textbf{Input}: $G$ grafo indiretto connesso; $w$ una funzione che associa pesi positivi agli archi in $E(G)$.\\
            \textbf{Output}: un MST di $G$.
        }

        \begin{algorithmic}[1]
            \Function{Prim$_2$}{$G$, $w$}
                \State $v \in V(G)$
                \State $\texttt{Sol} := \varnothing$
                \State $R := \{v\}$
                \State $\texttt{MinHeap H} := \texttt{[]}$
                \For{$u \in V(G) - \{v\}$}
                    \State $\texttt{H.insert(}u, +\infty\texttt{)}$
                \EndFor
                \State $\texttt{parents} := \texttt{[}-1\texttt{]} * n$
                \State $\texttt{parents[}v\texttt{]} = v$
                \For{$y \in V(G) : y \sim v$}
                    \State $\texttt{parents[}y\texttt{]} = v$
                    \State $\texttt{H.set\_key(}y, w(v, y)\texttt{)}$
                \EndFor
                \While{$R \neq V(G)$}
                    \State $y := \texttt{H.extract\_min()}$ \Comment{l'elemento viene \underline{rimosso}}
                        \State $\texttt{Sol} = \texttt{Sol} \cup \{ (\texttt{parents[}y\texttt{]}, y) \}$
                    \State $R = R \cup \{y\}$
                    \For{$x \in V(G) - R : x \sim y$}
                        \If{$\texttt{H.get\_key(}x\texttt{)} > w(x, y)$}
                            \State $\texttt{parents[}x\texttt{]} = y$
                            \State $\texttt{H.set\_key(}x, w(x, y)\texttt{)}$
                        \EndIf
                    \EndFor
                \EndWhile
                \State \textbf{return} \texttt{Sol}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia alla riga 2, scegliendo un vertice qualunque $v \in V(G)$, dal quale partire; alla riga 3, viene definito \texttt{Sol}, che contterrà l'insieme di archi dell'MST di $G$, che verrà restituito alla riga 26 al termine della procedura; alla riga 4, viene inoltre definito un insieme di vertici $R$, che costituirà la regione di vertici correntemente coperta dall'algoritmo, coerentemente con gli archi presenti \texttt{Sol}; infine, alla riga 5, viene inizializzato un min-heap, che viene riempito con le righe 6 e 7, utilizzando tutti i vertici in $V(G)$, eccetto $v$ scelto in partenza; si noti che ad ogni nodo viene associato inizialmente $+ \infty$, poiché per ogni $u \in V(G) - \{v\}$ il criterio di ordinamento del min-heap dovrà essere il peso minimo dell'arco tale da inserire $u$ in $R$, ed è dunque necessario inizializzare le loro chiavi con un valore sufficientemente alto, tale da non essere mai scelto inizialmente dall'algoritmo.

        Alla riga 9 viene definito un array di padri \texttt{parents}, il quale non costituirà propriamente un albero di padri della visita del grafo in input, ma un albero i cui padri dei nodi sono sempre i vertici tali da minimizzare l'arco tra padre e figlio; alla riga 10 viene marcata la radice dell'albero ponendo $\texttt{parents[}v\texttt{]} = v$.

        Il ciclo \texttt{for} della riga 11, per ogni vertice $y$ adiacente a $v$ di partenza, inizializza $\texttt{parents[}y\texttt{]} = v$ alla riga 12, ed alla riga 13 viene aggiornato il valore della chiave di $y$ all'interno del min-heap, attraverso proprio il peso dell'arco $(v, y)$.

        La procedura dell'algoritmo inizia dunque alla riga 15, definendo un ciclo \texttt{while} che termina se $R$ contiene tutti i vertici in $V(G)$, garantendo dunque che l'output \texttt{Sol} sarà un grafo di copertura; alla riga 16 viene salvato $y$, definito come l'elemento minore del min-heap (\tbf{si noti che l'operazione} \texttt{H.extract\_min()} \tbf{rimuove il minimo dal min-heap}); inoltre, per definizione di $y$, per le chiavi usate nel min-heap, e per come \texttt{parents} è stato definito, è garantito che l'arco $(\texttt{parents[}y\texttt{]}, y)$ minimizza il peso, e dunque nella riga 17 quest'ultimo arco viene inserito in \texttt{Sol}, ed $y$ viene aggiunto ad $R$ alla riga seguente (si noti che questo assicura che l'algoritmo scelga un insieme di archi minimale rispetto al peso di $w$).

        Successivamente, alla riga 19 viene istanziato un ulteriore ciclo \texttt{for}, il quale per ogni vertice $x$ non contenuto in $R$ (dunque ancora non visitato dall'algoritmo), adiacente all'attuale $y$, effettua un aggiornamento simile a quello eseguito dal ciclo \texttt{for} della riga 11: infatti, la riga 20 controlla che il peso dell'arco $(x, y)$ sia minore della chiave di $x$, attualmente in \texttt{H}, poiché in tal caso è necessario aggiornare quest'ultima con $w(x, y)$, dunque minimizzando sempre il peso associato ad $x$ nel min-heap; inoltre, in tal caso va anche aggiornato il padre di $x$ con $y$, salvando dunque all'interno di $\texttt{parents[}x\texttt{]}$ il padre di $x$ tale da minimizzare il peso dell'arco tra padre e figlio.

        L'algoritmo termina alla riga 26, restituendo l'insieme di archi \texttt{Sol} ottenuto.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Si noti che il costo di inserimento di un nodo all'interno di un heap è pari a $O(\log n)$, e dunque il costo del ciclo \texttt{for} alla riga 6 è pari a $O (n \log n)$, poichè viene inserito ogni nodo in $V(G) - \{v\}$ all'interno di \texttt{H}, e $O(|V(G) - \{v\}|) = O(n - 1) = O(n)$. Inoltre, la creazione dell'array \texttt{parents}, alla riga 9, ha costo $O(n)$.

        Il costo di aggiornamento di una chiave all'interno di un heap è pari anch'esso a $O(\log n)$, e dunque la complessità del ciclo \texttt{for} della riga 11 è pari a $\deg(v) \cdot O(\log n)$, per via della riga 13.

        L'operazione di estrazione del minimo dal min-heap, alla riga 16, ha anch'essa costo pari a $O(\log n)$, esattamente come il controllo della riga 20, e l'operazione di aggiornamento della chiave di $x$ nella riga 22; allora, il ciclo \texttt{for} della riga 19 ha costo $\deg(y) \cdot O(\log n)$, mentre il costo del ciclo \texttt{while} della riga 15 è pari a $$\displaystyle O\left(\sum_{y \in V(G) - \{v\}}{\log n + \deg(y) \cdot \log n}\right)$$

        Allora, accorpando il costo del ciclo \texttt{for} della riga 11, all'interno del costo del ciclo \texttt{while} della riga 15, è possibile riscrivere l'equazione precedente come segue: $$\displaystyle O\left(\sum_{y \in V(G)} {\log n}\right) + O \left(\sum_{y \in V(G)}{\deg(y) \cdot \log n}\right) = $$ $$= O(n \log n ) + O(m \log n) = O(\log n \cdot( n + m))$$

        Dunque, il costo dell'algoritmo è pari a $O(n \log n) + O(n) + O(\log n \cdot (n + m)) = O(\log n \cdot (n + m))$.
    \end{proof}

    \chapter{Algoritmi Divide et Impera}

    \section{Algoritmi Divide et Impera}

    \subsection{Definizioni}

    \begin{frameddefn}{Divide et Impera}
        Con la locuzione latina \tbf{divide et impera}, ci si riferisce ad una tecnica di risoluzione dei problemi, basata sull'induzione, attraverso la quale:

        \begin{itemize} 
            \item il problema di partenza viene inizialmente scomposto in sotto-problemi di dimensione inferiore, ricorsivamente (la fase del \tit{divide});
            \item successivamente, una volta raggiunto un caso base della ricorsione, per il quale è nota la soluzione, l'algoritmo procede a combinare le soluzioni dei sotto-problemi, per trovare la soluzione del problema di partenza (la fase dell'\tit{impera}).
        \end{itemize}
    \end{frameddefn}

    \begin{framedthm}[label={master theorem},breakable]{Master theorem - Teorema principale}
        Siano $\alpha, \beta \ge 1$ tali da descrivere la seguente equazione di ricorrenza: $$\left \{ \begin{array}{l} T(n) = \alpha \cdot T\left(\dfrac{n}{\beta}\right) + f(n) \\ T(1) = \Theta(1) \end{array} \right.$$ allora, si ha che
        \begin{itemize}
            \item se $f(n) = \Theta(n^c)$, con $c < \log_{\beta}{\alpha}$, allora $$T(n) = \Theta(n^{\log_{\beta}{\alpha}})$$
            \item se $f(n) = \Theta(n^c \log^k n)$, con $\left \{ \begin{array}{l} c = \log_{\beta}{\alpha} \\ k \ge 0 \end{array} \right.$, allora $$T(n) = \Theta(n^{\log_{\beta}{\alpha}} \log^{k+1} {n})$$
            \item se $f(n) = \Theta(n^c)$, con $c > \log_{\beta}{\alpha}$, allora $$T(n) = f(n)$$
        \end{itemize}
    \end{framedthm}

    \subsection{Trovare la somma massima dei sotto-array}

    \begin{algorithm}[H]
        \caption{
            Dato un array \texttt{A} di $n$ interi, l'algoritmo restituisce la somma massima tra i suoi sotto-array contigui.\\
            \textbf{Input}: \texttt{A} un array di $n$ interi.\\
            \textbf{Output}: la somma massima tra i sotto-array contigui di \texttt{A}.
        }

        \begin{algorithmic}[1]
            \Function{FMSSInternals}{\texttt{A}, $a$, $b$}
                \If{$a == b$}
                    \If{$\texttt{A[}a\texttt{]} \ge 0$}
                        \State \textbf{return} $\texttt{A[}a\texttt{]}$
                    \Else
                        \State \textbf{return} $0$
                    \EndIf
                \Else
                    \State $m := \left \lfloor \dfrac{a + b}{2} \right \rfloor$
                    \State $M_s := \texttt{FMSSInternals(A}, a, m\texttt{)}$
                    \State $M_d := \texttt{FMSSInternals(A}, m +1, b\texttt{)}$
                    \State $t, s := 0$
                    \For{$i \in [m, a]$} \Comment{dal centro verso sinistra}
                        \State $t \ += \texttt{A[}i\texttt{]}$
                        \If{$t > s$}
                            \State $s = t$
                        \EndIf
                    \EndFor
                    \State $t, d := 0$
                    \For{$i \in [m + 1, b]$} \Comment{dal centro verso destra}
                        \State $t \ += \texttt{A[}i\texttt{]}$
                        \If{$t > d$}
                            \State $d = t$
                        \EndIf
                    \EndFor
                    \State \textbf{return} $\max(M_s, M_d, s + d)$
                \EndIf
            \EndFunction
            \\
            \Function{findMaxSubarraySum}{\texttt{A}}
                \State \textbf{return} $\texttt{FMSSInternals(A}, 0, \texttt{A.length()} - 1 \texttt{)}$ \Comment{$-1$ per indicizzazione}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia alla riga 31, chiamando una funzione ricorsiva interna, ovvero \texttt{FMSSInternals}, poiché è necessario inizializzarne correttamente i parametri; infatti, ad essa vengono forniti, oltre all'array di interi \texttt{A}, anche gli estremi di quest'ultimo, in modo da effettuare la ricorsione sull'intero array.

        La funzione ricorsiva inizia controllando se $a$ e $b$ in input sono uguali: questo infatti è il caso base della ricorsione, poiché per tale condizione l'array della chiamata ricorsiva corrente è lungo 1, e dunque ha esattamente 1 elemento; allora, in tal caso è sufficiente restituire $\texttt{A[}a\texttt{]}$ stesso, se quest'ultimo fosse positivo, mentre se dovesse essere negativo, è necessario restituire 0. Questo, in quanto, nel caso peggiore, \texttt{A} è costituito esclusivamente da interi negativi, e dunque il sotto-array contiguo di \texttt{A} con somma maggiore risulta essere l'array nullo, che ha sempre somma 0, e dunque la riga 6 garantisce tale risultato in questo caso particolare (per la precisione, si noti che le righe 15 e 22, discusse nella sezione successiva, già garantiscono che il risultato sia 0 nel caso in cui \texttt{A} contenesse eslcusivamente interi negativi, con la sola eccezione in cui \texttt{A} è costituito da un solo intero, negativo, nel qual caso la ricosione non verrebbe effettuata, e dunque la riga 6 garantisce di restituire 0 ugualmente).

        Se invece $a \neq b$, allora non si entra nel caso base, poiché gli estremi del sotto-array da controllare non coincidono; alla riga 9 viene dunque calcolato il punto medio $m$ tra $a$ e $b$, arrotondato per eccesso, per poter effettuare 2 chiamate ricorsive, nelle righe 10 e 11, grazie alle quali vengono rispettivamente restituiti $M_s$ ed $M_d$, che costituiscono il primo la somma massima tra i sotto-array compresi in $\texttt{A[}a\texttt{:}m\texttt{]}$, ed il secondo la somma massima tra i sotto-array compresi in $\texttt{A[}m + 1\texttt{:}b\texttt{]}$ (considerando la notazione degli \tbf{slice} inclusiva su entrambe gli estremi).

        Successivamente, dalla riga 12 alla riga 18, viene effettuato un ciclo \texttt{for}, che calcola la somma massima della prima metà del sotto-array del livello di ricorsione corrente, dunque partendo dal centro muovendosi verso sinistra; viceversa, il ciclo \texttt{for} delle righe successive (dalla 19 fino alla 25), effettua il calcolo della somma massima della seconda metà del sotto-array del livello di ricosione corrente, dunque partendo dal centro muovendosi verso destra. Questo, poiché se entrambe i cicli si muovessero con indici crescenti (dunque da sinistra verso destra), di fatto i due \texttt{for} starebbero controllando 2 sotto-array diversi nel livello di ricorsione corrente, e non lo stesso; per ovviare a questo problema, è sufficiente realizzare i cicli come appena descritto. Si noti che all'interno di questi, viene semplicemente istanziato un accumulatore $t$, che viene aggiornato ad ogni iterazione dei cicli (righe 14 e 21) con l'$i$-esimo elemento di \texttt{A}, e se $t$ diventa maggiore del totale massimo raggiunto correntemente (in un caso rappresentato da $s$, nell'altro rappresentato da $d$), lo si aggiorna col valore attuale di $t$. Si noti che questo garantisce che venga propagato 0 nel caso di array totalmente negativo, poiché $s$ e $d$ vengono aggiornati soltanto se $t$ diventa maggiore del loro valore corrente, e sono inizializzati entrambi a 0 nelle righe 12 e 19. Infine, si noti che $t$ alla riga 19 deve essere nuovamente inizializzato a 0 per funzionare correttamente.

        Concludendo, il caso ricorsivo, alla riga 26, restituisce il massimo tra $M_s$, $M_d$ e $s + d$, dove quest'ultima costituisce proprio la somma massima del sotto-array corrente.

    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Il caso base dell'algoritmo è costituito da un controllo, alla riga 2, con costo $\Theta(1)$, e da un secondo controllo, alla riga 3, con costo $\Theta(1)$ anch'esso; allora, il caso base ha costo $\Theta(1)$.

        Nel caso ricorsivo, incontrato dall'algoritmo nella riga 8, vengono effettuate 2 chiamate ricorsive, alle righe 10 e 11, che hanno entrambe costo $T\left(\dfrac{n}{2}\right)$, e successivamente vengono eseguiti due cicli \texttt{for}, nelle righe 13 e 19, i quali hanno entrambi costo $\Theta\left(\dfrac{n}{2}\right) = \Theta(n)$; allora, il costo del caso ricorsivo è pari a $2 \cdot T\left(\dfrac{n}{2}\right) + \Theta(n)$.

        Allora, l'equazione di ricorrenza che rappresenta il costo dell'algoritmo è la seguente $$\left \{ \begin{array}{l} T(n) = T\left(\dfrac{n}{2}\right) + \Theta(n) \\ T(1) = \Theta(1) \end{array} \right .$$

        Si noti che questa è un'equazione che è possibile risolvere utilizzando il \cref{master theorem}, in quanto $$\left . \begin{array}{l} \alpha = 2 \\ \beta = 2 \end{array} \right \} \implies n ^ {\log_{2}{2}} = n^1 = n$$ e poiché $f(n) = \Theta(n)$, allora ci si trova nel secondo caso del Master theorem, ed infatti ponendo $k = 0$ si ha $$f(n) = \Theta(n) = \Theta(n \log^0 n)$$ e dunque si ha che $$T(n) = \Theta(n ^{\log_{2}{2}} \log ^ {0 + 1}n) = \Theta(n \log n)$$
    \end{proof}

    \chapter{Programmazione dinamica}

    \section{Programmazione dinamica}

    \subsection{Definizioni}

    \begin{frameddefn}{Programmazione dinamica}
        Con \tbf{programmazione dinamica}, si intende un processo di risoluzione dei problemi, in cui questi vengono risolti partendo da soluzioni delgli stessi, ma di dimensioni inferiori; si noti che tali sottoproblemi sono sovrapponibili, e questo rende l'approccio fondamentalmente differente rispetto al \tit{divide et impera}.
    \end{frameddefn}

    \begin{frameddefn}{Tempo polinomiale}
        Un algoritmo è detto essere in \tbf{tempo polinomiale} se il suo tempo di esecuzione è limitato superiormente da un'espressione polinomiale della dimensione del suo input.
    \end{frameddefn}

    \begin{frameddefn}{Insieme P}
        L'\tbf{insieme P} è costituito dalla categoria di problemi per i quali esiste un algoritmo deterministico in tempo polinomiale, rispetto alla dimensione del proprio input.
    \end{frameddefn}

    \begin{frameddefn}{NP-completezza}
        Un problema è detto \tbf{NP-completo}, se non è possibile risolverlo deterministicamente in tempo polinomiale, rispetto alla dimensione del suo input.
    \end{frameddefn}

    \begin{example}[Problema NP-completo]
        Un esempio di problema attualmente NP-completo è rappresentato dal sudoku, in quanto non sono noti algoritmi di risoluzione in tempo polinomiale, rispetto alla dimensione dell'input, per risolverne uno.
    \end{example}

    \section{Memoizzazione}

    \subsection{Definizioni}

    \begin{frameddefn}{Memoizzazione}
        La \tbf{memoizzazione}, nota anche con il termine inglese \tit{memoization}, è una tecnica di programmazione che consiste nel salvare in memoria i valori restituiti da una funzione, in modo da averli nuovamente a disposizione per un uso successivo, senza aver bisogno di ricalcolarli; tale tecnica viene spesso utilizzata per risolvere problemi di programmazione dinamica.
    \end{frameddefn}

    \subsection{Trovare il massimo spazio allocabile}

    \begin{algorithm}[H]
        \caption{
            Data una lista \texttt{S} di dimensioni di $n$ file, e una memoria di capacità $C$, l'algoritmo restituisce il massimo spazio che può essere allocato dai file in \texttt{S} sulla memoria; inoltre, i file hanno tutti dimensione inferiore a $C$.\\
            \textbf{Input}: \texttt{S} lista di dimensioni di file; $C$ capacità della memoria, tale che $\forall s_i \in \texttt{S} \quad s_i \le C$.\\
            \textbf{Output}: quantità di spazio massima che è possibile allocare.
        }

        \begin{algorithmic}[1]
            \label{fileallocation}
            \Function{maxAllocation}{\texttt{S}, $C$}
                \State $\texttt{T} := \texttt{[[}-1\texttt{]} * (C + 1) \texttt{]} * (n + 1)$
                \For{$k \in [0, n]$} \Comment{$[0, n]$ ha $n + 1$ elementi}
                    \State $\texttt{T[} k\texttt{][}0\texttt{]} = 0$ \Comment{prima colonna a $0$}
                \EndFor
                \For{$j \in [0, C]$} \Comment{$[0, C]$ ha $C + 1$ elementi}
                    \State $\texttt{T[} 0\texttt{][}j\texttt{]} = 0$ \Comment{prima riga a $0$}
                \EndFor
                \For{$k \in [1, n]$}
                    \For{$j \in [1, C]$}
                        \State $\texttt{T[}k\texttt{][}j\texttt{]} = \texttt{T[}k -1 \texttt{][}j\texttt{]}$ \Comment{lo stesso della riga precedente}

                        \State $v := \texttt{T[}k-1\texttt{][}j-s_k\texttt{]} + s_k$
                        \If{$\left \{ \begin{array}{l} j \ge s_k \\ v > \texttt{T[}k - 1\texttt{][}j\texttt{]} \end{array} \right.$}
                            \State $\texttt{T[}k\texttt{][}j\texttt{]} = v$
                        \EndIf
                    \EndFor
                \EndFor
                \State \textbf{return} $\texttt{T[}n\texttt{][}C\texttt{]}$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia definendo una matrice \texttt{T} di dimensioni $(n + 1) \times (C + 1)$, inizializzandone ogni cella a $-1$; successivamente, con il ciclo \texttt{for} della riga 3, viene posta ogni cella della prima colonna a 0, e con il ciclo \texttt{for} della riga 6 ogni cella della prima riga a 0.

        Lo scopo della matrice costruita dall'algoritmo, è di rappresentare, per ogni cella $\texttt{T[}k\texttt{][}j\texttt{]}$, dati $k$ file aventi dimensione $s_1, \ldots, s_k$, lo spazio massimo che è possibile allocare con un sottoinsieme di tali file, su un disco di capacità $j$; di conseguenza, il numero di file che è possibile allocare con una memoria di capacità nulla, è sicuramente 0 (dunque la prima colonna è sempre costituita interamente da 0), e la memoria che è possibile allocare con 0 file è sicuramente 0 (dunque la prima riga vede solamente il valore 0). In simboli, si ha che $$\forall k \in [0, n], j \in [0, C] \quad \texttt{T[}k\texttt{][}0\texttt{]} = \texttt{T[}0\texttt{][}j\texttt{]} = 0$$

        Procedendo per gradi, si può notare che, avendo ad esempio $k = 1$, indipendentemente dalla memoria disponibile, non sarà possibile allorare uno spazio superiore a $s_k = s_1$, e dunque segue che $$\texttt{T[}1\texttt{][}j\texttt{]}= \left \{ \begin{array}{ll} 0 & j < s_1 \\ s_1 & j \ge s_1 \end{array} \right.$$ ovvero, la seconda riga della tabella sarà completamente nulla, fino a quando $j = s_1$, valore dal quale la riga sarà interamente pari a $s_1$.

        Successivamente, analizzando il caso più generale, per descrivere la funzione della matrice, è necessario osservare la legge che ne definisce una generica cella: $$\texttt{T[}k\texttt{][}j\texttt{]}=\left \{ \begin{array}{ll}\texttt{T[}k - 1\texttt{][}j\texttt{]} & j < s_k \\ \max\{\texttt{T[}k - 1\texttt{][}j\texttt{]}, \texttt{T[}k-1\texttt{][}j - s_k\texttt{]} + s_k\} & j \ge s_k \end{array} \right.$$

        Partendo dal primo caso, se si verifica che $j < s_k$, e dunque la capacità del disco è inferiore della dimensione del $k$-esimo file, non sarà certo possibile inserire quest'ultimo all'interno della memoria, e dunque in tal caso è sufficiente riportare il valore che la tabella aveva nella cella sovrastante, ovvero $\texttt{T[}k-1\texttt{][}j\texttt{]}$, indicando che il file con dimensione $s_k$ non è stato scelto. Viceversa, nel caso in cui $j \ge s_k$, e dunque vi è spazio all'interno della memoria per il $k$-esimo file, vengono prese in considerazione 2 possibili scelte, e di queste viene selezionata quella con valore maggiore: in particolare, viene comparata la quantità di memoria allocata senza l'inserimento del $k$-esimo file (primo argomento di $\max$), con la memoria allocata nel caso in cui il $k$-esimo file è stato inserito. Quest'ultimo caso è descritto dal secondo argomento di $\max$, controllando all'interno di \texttt{T} quanto spazio è possibile allocare con una memoria di dimensione $j - s_k$, potendo scegliere tra un sottoinsieme dei primi $(k - 1)$-file; infatti, poiché la dimensione della memoria attuale è $j$, ed è noto che $j \ge s_k$, allora all'interno del disco di dimensione $j$ esisteranno certamente 2 regioni di dimensioni, rispettivamente, $s_k$ e $j - s_k$ (nel caso peggiore $j = s_k \implies j - s_k = 0$). Allora, per sapere quanto spazio è possibile allocare nella regione di memoria avente $j - s_k$ unità di spazio, è sufficiente controllare proprio la matrice stessa, che sicuramente avrà raccolto l'informazione in qualche iterazione precedente; infine, è necessario aggiungere $s_k$ dopo aver preso tale quantità di spazio massimo allocabile, per coprire la regione di memoria di dimensione $s_k$ discussa precedentemente.

        Tornando alla discussione dell'algoritmo, nelle righe 9 e 10 vengono istanziati due cicli \texttt{for}, con i quali verranno attraversate e riempite tutte le celle della matrice \texttt{T}; in particolare, alla riga 11 il valore della cella corrrente viene aggiornato con quello della sua sovrastante (il caso in cui $j < s_k$, di fatto scelto come default), e nel caso in cui, alla riga 13, si verificasse invece che $j \ge s_k$, e $v$ (definito alla riga 12, pari al valore discusso precedentemente), è maggiore del valore contenuto nella cella sovrastante, allora la cella corrente viene rimpiazzata con $v$ stesso; si noti che quest'ultimo controllo fonde la definizione per casi della cella generica, ed il $\max$ presente nella formula.

        Infine, l'algoritmo termina alla riga 18, restituendo $\texttt{T[}n\texttt{][}C\texttt{]}$, poiché lo scopo iniziale dell'algoritmo era proprio di trovare la dimensione massima che fosse possibile allocare in una memoria di dimensione $C$, avendo $n$ file aventi dimensioni $s_1, \ldots, s_n$.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        L'algoritmo inizializza una matrice \texttt{T} di dimensioni $(n + 1) \times (C + 1)$ alla riga 2, e il costo della sua creazione è dunque $O((n + 1) \cdot (C + 1)) = O(n \cdot C)$; i cicli \texttt{for} delle righe 3 e 6 hanno rispettivamente costo $O(n)$ e $O(C)$.

        Il ciclo \texttt{for} della riga 9, poiché contiene un ulteriore ciclo \texttt{for} annidato alla riga 10, ha costo $O(n \cdot C)$; inoltre all'interno di questi vengono svolte solamente operazioni elementari.

        Allora il costo dell'algoritmo è pari a $2 \cdot O(n \cdot C) + O(n) + O(C) = O(n \cdot C)$.

        Si noti che l'input contiene $n$ interi $s_1, \ldots, s_n$, e il numero di bit totale per salvarli in memoria è pari a $$\displaystyle \sum_{i = 1}^n{O(\log s_i)} = O \left ( \sum_{i = 1}^n{\log s_i} \right )$$ ma poiché nel caso peggiore i file hanno tutti dimensione pari a $C$, allora $$O \left ( \sum_{i = 1}^n{\log s_i} \right ) = O(n \cdot \log C)$$ poiché $C$ necessita di $O(\log C)$ bit per essere rappresentato; infine, contando anche la dimensione in bit di $C$ in input, si ha che la dimensione dell'input dell'algoritmo è pari a $O((n + 1) \cdot \log C) = O(n \cdot \log C)$. Allora, l'algoritmo non ha tempo polinomiale, poiché il suo costo è pari a $O(n \cdot C)$; infatti, tale problema è NP-completo.
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Data una lista \texttt{S} di dimensioni di $n$ file, una memoria di capacità $C$, e la matrice costruita attraverso la funzione \texttt{fileAllocation} dell'\cref{fileallocation}, l'algoritmo restituisce un insieme di file che massimizza la memoria allocata; inoltre, i file hanno tutti dimensione inferiore a $C$.\\
            \textbf{Input}: \texttt{S} lista di dimensioni di file; $C$ capacità della memoria, tale che $\forall s_i \in \texttt{S} \quad s_i \le C$; \texttt{T} matrice prodotta precedentemente.\\
            \textbf{Output}: un insieme di file che massimizza la memoria allocata.
        }

        \begin{algorithmic}[1]
            \label{fileallocation2}
            \Function{maxAllocationFiles}{\texttt{S}, $C$, \texttt{T}}
                \State $\texttt{Sol} := \varnothing$
                \State $j:= C$
                \For{$k \in [n, 1]$} \Comment{il ciclo decrementa}
                    \If{$\texttt{T[}k\texttt{][}j\texttt{]} > \texttt{T[}k -1\texttt{][}j\texttt{]}$}
                        \State $\texttt{Sol} = \texttt{Sol} \cup \{s_k\}$
                        \State $j \ -= s_k$
                    \EndIf
                \EndFor
                \State \textbf{return} \texttt{Sol}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia definendo un insieme vuoto \texttt{Sol} alla riga 2, che conterrà l'insieme di file al termine della procedura; inoltre, alla riga 3 viene definito $j$, pari alla capacità del disco $C$.

        Il ciclo \texttt{for} della riga 4 scorre la matrice \texttt{T} in input seguendo un preciso criterio: partendo dall'ultima riga della matrice (si noti che con $k \in [n, 1]$ si sta indicando un ciclo che parte da $n$ e termina ad $1$), il ciclo controlla che la cella corrente, ovvero $\texttt{T[}k\texttt{][}j\texttt{]}$, sia maggiore della cella direttamente sopra ad essa, ovvero $\texttt{T[}k - 1\texttt{][}j\texttt{]}$; in tal caso, l'algoritmo inserisce $s_k$ all'interno della soluzione \texttt{Sol} alla riga 6, e alla riga 7 $j$ viene decrementato di $s_k$;

        L'idea alla base di questo ciclo è di partire dall'ultima cella della matrice (ultima riga, ultima colonna, ovvero $\texttt{T[}n\texttt{][}C\texttt{]}$), salendo le righe in ogni iterazione, spostanosi orizzontalmente ogni volta che la cella sovrastante a quella corrente aveva valore inferiore ad essa; questo, poiché per via di come è stata costruita la matrice, necessariamente $$\texttt{T[}k\texttt{][}j\texttt{]} \neq \texttt{T[}k -1\texttt{][}j\texttt{]} \iff \texttt{T[}k\texttt{][}j\texttt{]} > \texttt{T[}k -1\texttt{][}j\texttt{]}$$ e dunque se la cella corrente ha valore maggiore della cella ad essa sovrastante, allora il $k$-esimo file era stato scelto dall'algoritmo, e va dunque inserito all'interno della soluzione; si noti dunque che poiché è stato preso il $k$-esimo file, la dimensione della memoria all'iterazione successiva dovrà essere pari a $j - s_k$, altrimenti si lascerebbe lo spazio del $k$-esimo file.

        L'algoritmo termina restituendo la soluzione accumulata, alla riga 10.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        L'algoritmo effettua un solo ciclo \texttt{for} alla riga 4, che scorre la matrice in input parentdo da $k = n$ fino a $k = 1$, ma si noti che non viene controllata ogni cella di \texttt{T}, poiché viene controllata al massimo 1 cella per riga.

        Allora, il costo dell'algoritmo è pari a $O(n)$.
    \end{proof}

    \subsection{Knapsack problem}

    \begin{algorithm}[H]
        \caption{
            Date due liste \texttt{V} e \texttt{P}, rispettivamente rappresentanti il valore e il peso di $n$ oggetti, ed uno zaino di capienza $C$, l'algoritmo restituisce il massimo valore che può essere totalizzato dagli oggetti in input, all'interno dello zaino, senza sforare la sua capienza massima in termini di peso; inoltre, gli oggetti hanno tutti peso inferiore a $C$.\\
            \textbf{Input}: \texttt{V} lista di valori di oggetti; \texttt{P} lista di pesi di oggetti; $C$ capienza dello zaino, tale che $\forall p_i \in \texttt{P} \quad p_i \le C$.\\
            \textbf{Output}: valore massimo che è possibile totalizzare.
        }

        \begin{algorithmic}[1]
            \Function{Knapsack}{\texttt{V}, \texttt{P}, $C$}
                \State $\texttt{T} := \texttt{[[}-1\texttt{]} * (C + 1) \texttt{]} * (n + 1)$
                \For{$k \in [0, n]$} \Comment{$[0, n]$ ha $n + 1$ elementi}
                    \State $\texttt{T[} k\texttt{][}0\texttt{]} = 0$ \Comment{prima colonna a $0$}
                \EndFor
                \For{$j \in [0, C]$} \Comment{$[0, C]$ ha $C + 1$ elementi}
                    \State $\texttt{T[} 0\texttt{][}j\texttt{]} = 0$ \Comment{prima riga a $0$}
                \EndFor
                \For{$k \in [1, n]$}
                    \For{$j \in [1, C]$}
                        \State $\texttt{T[}k\texttt{][}j\texttt{]} = \texttt{T[}k -1 \texttt{][}j\texttt{]}$ \Comment{lo stesso della riga precedente}

                        \State $v := \texttt{T[}k-1\texttt{][}j-p_k\texttt{]} + v_k$
                        \If{$\left \{ \begin{array}{l} j \ge p_k \\ v > \texttt{T[}k - 1\texttt{][}j\texttt{]} \end{array} \right.$}
                            \State $\texttt{T[}k\texttt{][}j\texttt{]} = v$
                        \EndIf
                    \EndFor
                \EndFor
                \State \textbf{return} $\texttt{T[}n\texttt{][}C\texttt{]}$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        Si noti che l'algoritmo presenta eslcusivamente qualche variazione rispetto all'\cref{fileallocation}, poiché quello presentato è una sola generalizzazione del problema dell'allocazione dei file, precedentemente discusso; in particolare, variano le seguenti righe:

        \begin{itemize}
            \item riga 12, dove bisogna controllare la $(j - p_k)$-esima colonna della $(k -1)$-riga, ma aggiungere $v_k$, poiché \texttt{T} contiene i valori massimi possibili, e non i pesi (si noti che nel problema precedente pesi e valori erano coincidenti);
            \item riga 13, in cui varia semplicemente il controllo $j \ge p_k$, per ragione analoga.
        \end{itemize}

        Infine, si noti che anche in questa generalizzazione è possibile descrivere un algoritmo in grado di restituire gli oggetti che hanno permesso di raggiungere tale valore massimo, ma il codice sarebbe uguale in tutto e per tutto all'\cref{fileallocation2}, dove l'input dell'algoritmo sarebbero semplicemente i pesi degli oggetti, e viene dunque omesso.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Poiché l'algoritmo è di fatto una sola generalizzazione dell'\cref{fileallocation}, e il codice è pressochè invariato, il costo è esattamente lo stesso, ovvero $O(n \cdot C)$, ed infatti il problema resta NP-completo.
    \end{proof}

    \subsection{Trovare il peso massimo di un cammino}

    \begin{framedlem}[label={lemma grafi max}]{Pesi di cammini}
        Sia $G$ un grafo diretto aciclico, pesato attraverso $w$, siano $u, v \in V(G)$ due suoi vertici, e sia $z \in V(G) - \{u, v\}$ un suo vertice, tale che $(z, v) \in E(G)$; se esiste un cammino della forma $u \rightarrow z$, avente peso $\alpha$, allora esiste un cammino della forma $u \rightarrow v$, avente peso $\alpha + w(z, v)$.
    \end{framedlem}

    \begin{proof}
        Sia $u \rightarrow z$ un cammino da $u$ a $z$, avente peso $\alpha$; allora, per trovare un cammino della forma $u \rightarrow v$, avente peso $\alpha + w(z, v)$, è sufficiente considerare il cammino $\{u \rightarrow z\} \cup (z, v)$, poiché $(z, v) \in E(G)$ in ipotesi.

        Si noti che questa dimostrazione non esclude la possibilità che il cammino $u \rightarrow z$ sia passante per $v$; ma, se così fosse, poiché in ipotesi si ha che $(z, v) \in E(G)$, allora $(\{u \rightarrow z\} - \{u \rightarrow v\}) \cup (z, v)$ è un ciclo di $G$, e questo non è possibile perché $G$ è stato scelto aciclico in ipotesi $\lightning$. Inoltre, se si rimuovesse l'ipotesi per cui $G$ deve essere aciclico, allora la tesi non sarebbe necessariamente vera, poiché nel caso appena descritto, il percorso $u \rightarrow z$ passante per $v$ in realtà non è un cammino ma una passeggiata, e sarebbe dunque falsa l'implicazione.
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo $G$ diretto aciclico, pesato attraverso $w$ con pesi sia positivi che negativi, e due suoi nodi $u, v \in V(G)$, l'algoritmo restituisce il peso massimo che un cammino della forma $u \rightarrow y$ può avere.\\
            \textbf{Input}: $G$ grafo diretto aciclico; $w$ funzione dei pesi degli archi; $u, y \in V(G)$ due vertici di $G$.\\
            \textbf{Output}: peso massimo di un cammino $u \rightarrow y$.
        }

        \begin{algorithmic}[1]
            \label{maxWeightPaths}
            \Function{maxWeightPaths}{$G$, $w$, $u$, $y$}
                \State $\texttt{T} := \texttt{[[}-1\texttt{]} * n \texttt{]} * n$
                \For{$x \in V(G)$}
                    \If{$x == u$}
                        \State $\texttt{T[}0\texttt{][}x\texttt{]} = 0$
                    \Else
                        \State $\texttt{T[}0\texttt{][}x\texttt{]} = - \infty$
                    \EndIf
                \EndFor
                \For{$k \in [1, n - 1]$}
                    \For{$x \in V(G)$}
                        \State $\texttt{T[}k\texttt{][}x\texttt{]} = \texttt{T[}k -1 \texttt{][}x\texttt{]}$ \Comment{lo stesso della riga precedente}

                        \For{$(z, x) \in E(G)$} \Comment{$z$ è \underline{entrante} in $x$ ($G$ è diretto)}
                            \State $v := \texttt{T[}k-1\texttt{][}z\texttt{]} + w(z, x)$
                            \If{$v > \texttt{T[}k\texttt{][}x\texttt{]}$}
                                \State $\texttt{T[}k\texttt{][}x\texttt{]} = v$
                            \EndIf
                        \EndFor
                    \EndFor
                \EndFor
                \State \textbf{return} $\texttt{T[}n- 1\texttt{][}y\texttt{]}$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        La discussione della correttezza qui presentata fa affidamento a quella dell'\cref{fileallocation}, dunque alcuni dettagli verrano sottointesi poiché l'idea alla base è la medesima.

        L'algoritmo inizia alla riga 2, definendo una matrice $n \times n$, inizializzata con ogni cella a $-1$, e successivamente, attraverso un ciclo \texttt{for} alla riga 3, vengono inizializzate tutte le celle della prima riga della matrice a $- \infty$, eccetto per $\texttt{T[}0\texttt{][}u\texttt{]}=0$.

        In questo algoritmo, lo scopo della matrice è di rappresentare, per ogni generica cella $\texttt{T[}k\texttt{][}x\texttt{]}$, il peso massimo di un cammino della forma $u \rightarrow x$, avente al massimo $k$ archi; allora, naturalmente il peso di un cammino $u \rightarrow u$ è sempre 0, mentre per quanto riguarda il resto della prima riga, ad ogni sua cella viene assegnato il valore $- \infty$, poiché l'algoritmo deve trovare il peso massimo di cammini, e dunque tale valore ricopre il ruolo di invariante nei controlli, oltre al fatto che non esiste altro cammino, che inizi da $u$, avente 0 archi, oltre a $u \rightarrow u$ già discusso; inoltre, si noti che il grafo $G$ in input non è necessariamente fortemente connesso, e poiché è diretto, è possibile non vi siano cammini tra due dati nodi di $G$, dunque in tal caso l'algoritmo propagherà il valore $- \infty$, ad indicare che il valore iniziale non è stato modificato, in quanto non vi era valore con cui sostituire quello di partenza, perché non esiste collegamento tra i due vertici. In simboli $$\texttt{T[}0\texttt{][}x\texttt{]} = \left \{ \begin{array}{ll} - \infty & x \neq u \\ 0 & x = u \end{array} \right.$$

        Proseguendo gradualmente, se il numero di archi massimo aumenta a $k =1$, allora si ha che $$\texttt{T[}1\texttt{][}x\texttt{]} = \left \{ \begin{array}{ll} \texttt{T[}0\texttt{][}x\texttt{]} & (u, x) \notin E(G) \\ w(u, x) & (u, x) \in E(G) \end{array} \right .$$ poiché naturlamente, avendo a disposizione al massimo 1 arco, se l'arco $(u, x)$ stesso è presente in $E(G)$ (si noti che il grafo è diretto), allora il valore della cella sarà assegnato al peso di quest'ultimo, ovvero $w(u, x)$, altrimenti viene lasciato il peso che aveva nella cella sovrastante (si noti che non basta inserre $- \infty$, poiché andrebbe gestito comunque il caso in cui $x = u$, e dunque basta prendere il valore della cella sovrastante per evitare ulteriori controlli).

        Spostando l'attenzione al caso generale, la formula che descrive una generica cella della matrice dell'algoritmo è la seguente:
        \begin{equation*}
            \resizebox{.99\hsize}{!}{
                $\forall k \in [1, n-1] \quad \texttt{T[}k\texttt{][}x\texttt{]} = \max\{ \texttt{T[}k -1\texttt{][}x\texttt{]},\{\texttt{T[}k-1\texttt{][}z\texttt{]} + w(z, x) \mid \exists z \in V(G) : (z, x) \in E(G) \}\}$
            }
        \end{equation*}

        Partendo da $k$, si può notare che tale variabile è definita nell'intervallo $[1, n - 1]$, esattamente come procede l'algoritmo per il ciclo \texttt{for} della riga 10; infatti il numero di righe della matrice creata è $n$ (riga 2), e la prima è già stata riempita come discusso precedentemente, ma il motivo per cui sono sufficienti esattamente $n$ righe, è che $k$ rappresenta il numero massimo di archi di un cammino da $u$ ad ognuno dei nodi della $k$-esima riga, e poiché sono cammini, e dunque per definizione in essi non è possibile ripercorrere vertici o archi più di una volta, allora segue che il massimo numero di archi che ognuno dei cammini può avere è $n - 1$, e dunque è sufficiente avere $n$ righe, nell'intervallo $[0, n - 1]$.

        Inoltre, la formula definisce che la cella generica, che rappresenta il peso massimo che un cammino $u \rightarrow x$ avente al massimo $k$ archi può avere, è pari al valore massimo tra i seguenti:

        \begin{itemize}
            \item $\texttt{T[}k - 1\texttt{][}x\texttt{]}$, e se questo dovesse essere stato il valore preso dal $\max$, allora qualsiasi altro cammino $u \rightarrow x$ presente in $G$, con al massimo $k$ archi, doveva avere necessariamente peso inferiore al peso massimo tra i cammini, correntemente trovati, aventi al massimo $k - 1$ archi (si noti che questa eventualità si può verificare in quanto $G$ contiene archi anche negativi, altrimenti se i pesi degli archi fossero stati tutti positivi, aggiungere un arco avrebbe certamente restituito un valore maggiore, o uguale, al peso del cammino);
            \item $\{\texttt{T[}k-1\texttt{][}z\texttt{]} + w(z, x) \mid \exists z \in V(G) : (z, x) \in E(G) \}$, che rappresenta l'insieme dei pesi dei cammini $u \rightarrow z$, aventi al massimo $k - 1$ archi (dove $z$ è un nodo adiacente entrante in $x$), ad ognuno dei quali è stato aggiunto il peso dell'arco $(z, x)$; si noti che è garantito che $$\texttt{T[}k - 1\texttt{][}z\texttt{]} = \alpha \implies w_p(\{u \rightarrow x\}) = \alpha + w(z, x)$$ grazie al \cref{lemma grafi max} (il quale è applicabile, poiché $G$ in input è aciclico).
        \end{itemize}

        Allora, l'algoritmo effettua 2 cicli \texttt{for} annidati, nelle righe 10 e 11, per poter scorrere l'intera matrice, e come per l'\cref{fileallocation}, alla riga 12 viene posto come valore di default il primo argomento del $\max$ della formula discussa in precedenza; successivamente, viene effettuato un ulteriore ciclo \texttt{for}, alla riga 13, il quale per ogni arco $(z, x)$, con $z$ vertice adiacente entrante in $x$, aggiorna il valore della cella corrente con quello definito alla riga 14, se si verifica la condizione della riga 15, di fatto implementando i controlli per il secondo argomento del $\max$ della formula, assegnando a $\texttt{T[}k\texttt{][}x\texttt{]}$ sempre il valore maggiore corrente.

        Infine, l'algoritmo termina alla riga 21, restituendo $\texttt{T[}n - 1\texttt{][}y\texttt{]}$, poiché l'indicizzazione parte da $0$ e l'ultima riga della matrice è l'$(n - 1)$-esima, ritornando dunque il peso massimo di un cammino $u \rightarrow y$, avente al massimo $n - 1$ archi.

        Per concludere, si noti che invertendo la condizione della riga 15, si ottiene il noto algoritmo di Bellman-Ford, grazie al quale è possibile trovare il peso minimo che cammini tra due nodi di $G$ possono avere (di fatto, la loro distanza pesata).
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        L'algoritmo costruisce una matrice $n \times n$ alla riga 2, e la sua costruzione ha dunque costo $O(n^2)$; inoltre, il ciclo \texttt{for} della riga 3 ha costo $O(n)$.

        Il ciclo della riga 11 scorre ogni nodo del grafo, ma il \texttt{for} in esso annidato, della riga 13, cicla solamente sui nodi adiacenti entranti nel vertice correntemente visitato, dunque il costo di questi due \texttt{for} annidati è $\displaystyle O\left(\sum_{x \in V(G)}{\deg^{in}(x) + 1} \right) = O(n + m)$; allora, poiché questi due cicli si trovano in un ulteriore \texttt{for} (alla riga 10), che percorre ogni intero $k \in [1, n- 1]$, ed ha dunque costo $O(n - 1) = O(n)$, segue che il costo di questa sezione dell'algoritmo è $O(n \cdot (n + m))$

        Allora, il costo dell'algoritmo è pari a $O(n ^2) + O(n) + O(n \cdot(n + m)) = O(n \cdot (n + m))$.

        Il grafo $G$ in input, se rappresentato attraverso liste di adiacenza (salvando ad esempio i vertici adiacenti entranti per ogni nodo, facilitando le computazioni del ciclo \texttt{for} della riga 13), ha costo spaziale pari a $O(n + m)$, mentre il costo di $w$ è $O(m) = O(n - 1) = O(n)$, poiché è sufficiente ad esempio un array, che mappi ogni arco ad un peso. Allora, la dimensione dell'input è pari a $O(n + m) + O(n) = O(n + m)$, ma poiché il costo dell'algoritmo è $O(n \cdot (n + m))$, questo risulta dunque essere in P, poiché ha costo polinomiale rispetto alla dimensione del suo input.
    \end{proof}

    \begin{algorithm}[H]
        \caption{
            Dato un grafo $G$ diretto aciclico, pesato attraverso $w$ con pesi sia positivi che negativi, due suoi nodi $u, v \in V(G)$, e la matrice costruita attraverso la funzione \texttt{maxWeightPaths} dell'\cref{maxWeightPaths}, l'algoritmo restituisce un cammino, della forma $u \rightarrow y$, che realizza tale peso massimo.\\
            \textbf{Input}: $G$ grafo diretto aciclico; $w$ funzione dei pesi degli archi; $u, y \in V(G)$ due vertici di $G$; \texttt{T} matrice prodotta precedentemente.\\
            \textbf{Output}: cammino tale da massimizzare il peso di un cammino $u \rightarrow y$.
        }

        \begin{algorithmic}[1]
            \Function{maxWeightPath}{$G$, $w$, $u$, $y$, \texttt{T}}
                \State $\texttt{Sol} := \varnothing$
                \State $v := y$
                \State $k := n -1$
                \While{$v \neq u$}
                    \If{$\texttt{T[}k\texttt{][}v\texttt{]} == \texttt{T[} k - 1\texttt{][}v \texttt{]}$}
                        \State $k \ -= 1$
                    \Else
                        \For{$(z, v) \in E(G)$} \Comment{$z$ è \underline{entrante} in $v$ ($G$ è diretto)}
                            \If{$\texttt{T[}k - 1\texttt{][}z\texttt{]} + w(z, v) == \texttt{T[}k\texttt{][}v\texttt{]}$}
                                \State $\texttt{Sol} = \texttt{Sol} \cup \{(z, v)\}$
                                \State $v := z$
                                \State $k \ -= 1$
                                \State \textbf{break}
                            \EndIf
                        \EndFor
                    \EndIf
                \EndWhile
                \State \textbf{return} \texttt{Sol}
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    \begin{proof}[Correttezza dell'algoritmo.]
        L'algoritmo inizia definendo un insieme \texttt{Sol}, alla riga 2, che conterrà l'insieme degli archi che compongono uno dei cammini tali da realizzare il peso massimo di un cammino della forma $u \rightarrow y$; alla riga 3, viene inoltre definito un vertice $v$, posto inizialmente pari ad $y$, per poter iterare successivamente, ed alla riga 3 si ha $k$, posto pari ad $n - 1$, che servirà per puntare alla riga corrente della matrice \texttt{T} (partendo dunque dall'ultima riga).

        Nella riga 5, viene istanziato un ciclo \texttt{while}, il quale, fintanto che $v$ non diventa il vertice di partenza del cammino, ovvero $u$, visiterà la matrice \texttt{T}; si noti che la matrice viene attraversata dal basso verso l'alto, analogamente all'\cref{fileallocation2}.

        Se la condizione della riga 6 è verificata, allora aver aggiunto un arco ai $k - 1$ archi già presenti nel cammino $u \rightarrow v$ non ha contribuito al massimo peso possibile per un tale cammino; dunque, è sufficiente aggiornare $k$, per andare ad esaminare la riga superiore nella matrice. Al contrario, se il controllo della riga 6 è falso, allora viene istanziato, alla riga 9, un ciclo \texttt{for}, il quale per ogni vertice $z$ entrante in $v$ (si noti che $G$ è diretto), controlla se $\texttt{T[}k - 1\texttt{][}z\texttt{]} + w(z, v)$ è proprio pari a $\texttt{T[}k\texttt{][}v\texttt{]}$, ed in tal caso: l'arco $(z, v)$ viene aggiunto a \texttt{Sol} (riga 11), $v$ viene aggiornato con il nuovo vertice corrente, ovvero $z$ (si sta percorrendo il cammino da $y$ verso $u$), $k$ viene decrementato di $1$ (riga 13, per andare alla riga superiore di \texttt{T}), ed infine viene effettuato un \texttt{break} del ciclo \texttt{for}; di conseguenza, lo scopo del loop è quello di trovare, tra gli archi entranti in $v$, quello che realizzava il peso del cammino $u \rightarrow v$ avente al massimo $k$ archi.

        Al termine del ciclo \texttt{while}, l'algoritmo restituisce l'insieme di archi contenuto in \texttt{Sol}, che comporranno dunque un cammino della forma $u \rightarrow y$, avente il peso massimo cercato.
    \end{proof}

    \begin{proof}[Costo dell'algoritmo.]
        Partendo dal ciclo \texttt{for} della riga 9, esso controlla tutti i nodi adiacenti entranti in $v$, e nel caso peggiore vanno visitati tutti; inoltre, poiché al suo interno vi sono esclusivamente operazioni in tempo costante, il ciclo ha costo $O(\deg^{in}(v))$.

        Infine, tenendo in considerazione la riga 6, che ha costo $O(1)$, si ha che il costo del ciclo \texttt{while} della riga 5 è pari a $\displaystyle O\left(\sum_{v \in V(G)}{\deg^{in}(v) + 1} \right) = O(n + m)$.
    \end{proof}

\end{document}
